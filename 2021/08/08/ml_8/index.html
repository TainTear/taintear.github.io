<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="回归（Regression） 概述       我们前边提到的分类的目标变量是标称型数据，而回归则是对连续型的数据做出处理，回归的目的是预测数值型数据的目标值。                     回归 场景       回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。 假如你想要预测兰博基尼跑车的功率大小，可能会这">
<meta property="og:type" content="article">
<meta property="og:title" content="第8章 预测数值型数据-回归">
<meta property="og:url" content="http://example.com/2021/08/08/ml_8/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:description" content="回归（Regression） 概述       我们前边提到的分类的目标变量是标称型数据，而回归则是对连续型的数据做出处理，回归的目的是预测数值型数据的目标值。                     回归 场景       回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。 假如你想要预测兰博基尼跑车的功率大小，可能会这">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9B%9E%E5%BD%92%E9%A6%96%E9%A1%B5.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_18.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_19.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_20.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_1.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_1.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_2.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_3.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_21.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_4.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_23.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_6.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_2.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_7.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_8.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_9.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_10.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_11.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_22.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_12.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_13.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_14.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_15.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_16.png">
<meta property="og:image" content="http://example.com/2021/08/08/ml_8/img/LinearR_17.png">
<meta property="article:published_time" content="2021-08-07T19:07:57.000Z">
<meta property="article:modified_time" content="2021-08-28T19:36:07.334Z">
<meta property="article:author" content="TainTear">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/08/08/ml_8/img/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9B%9E%E5%BD%92%E9%A6%96%E9%A1%B5.png"><title>第8章 预测数值型数据-回归 | TainTear's Blog</title><link ref="canonical" href="http://example.com/2021/08/08/ml_8/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第8章 预测数值型数据-回归</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-08</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9B%9E%E5%BD%92%E9%A6%96%E9%A1%B5.png" alt="预测数值型数据回归首页" title="回归Regression首页"></p>

        <h2 id="回归（Regression）-概述"   >
          <a href="#回归（Regression）-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归（Regression）-概述" class="headerlink" title="回归（Regression） 概述"></a>回归（Regression） 概述</h2>
      <p><code>我们前边提到的分类的目标变量是标称型数据，而回归则是对连续型的数据做出处理，回归的目的是预测数值型数据的目标值。</code></p>

        <h2 id="回归-场景"   >
          <a href="#回归-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归-场景" class="headerlink" title="回归 场景"></a>回归 场景</h2>
      <p>回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。</p>
<p>假如你想要预测兰博基尼跑车的功率大小，可能会这样计算:</p>
<p>HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio</p>
<p>这就是所谓的 <code>回归方程(regression equation)</code>，其中的 0.0015 和 -0.99 称作 <code>回归系数（regression weights）</code>，求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。我们这里所说的，回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。</p>
<p>说到回归，一般都是指 <code>线性回归(linear regression)</code>。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。</p>
<p>补充:<br>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。</p>

        <h2 id="回归-原理"   >
          <a href="#回归-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归-原理" class="headerlink" title="回归 原理"></a>回归 原理</h2>
      
        <h3 id="1、线性回归"   >
          <a href="#1、线性回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、线性回归" class="headerlink" title="1、线性回归"></a>1、线性回归</h3>
      <p>我们应该怎样从一大堆数据里求出回归方程呢？ 假定输入数据存放在矩阵 x 中，而回归系数存放在向量 w 中。那么对于给定的数据 X1，预测结果将会通过 Y = X1^T w 给出。现在的问题是，手里有一些 X 和对应的 y，怎样才能找到 w 呢？一个常用的方法就是找出使误差最小的 w 。这里的误差是指预测 y 值和真实 y 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用平方误差（实际上就是我们通常所说的最小二乘法）。</p>
<p>平方误差可以写做（其实我们是使用这个函数作为 loss function）: </p>
<p><img src="img/LinearR_18.png" alt="平方误差"></p>
<p>用矩阵表示还可以写做 <img src="img/LinearR_19.png" alt="平方误差_2"> 。如果对 w 求导，得到 <img src="img/LinearR_20.png" alt="平方误差_3"> ，令其等于零，解出 w 如下（具体求导过程为: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://blog.csdn.net/nomadlx53/article/details/50849941" >http://blog.csdn.net/nomadlx53/article/details/50849941</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> ）:</p>
<p><img src="img/LinearR_1.png" alt="回归系数的最佳估计计算公式"></p>

        <h4 id="1-1、线性回归-须知概念"   >
          <a href="#1-1、线性回归-须知概念" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1、线性回归-须知概念" class="headerlink" title="1.1、线性回归 须知概念"></a>1.1、线性回归 须知概念</h4>
      
        <h5 id="1-1-1、矩阵求逆"   >
          <a href="#1-1-1、矩阵求逆" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1、矩阵求逆" class="headerlink" title="1.1.1、矩阵求逆"></a>1.1.1、矩阵求逆</h5>
      <p>因为我们在计算回归方程的回归系数时，用到的计算公式如下: </p>
<p><img src="img/LinearR_1.png" alt="回归系数的最佳估计计算公式"></p>
<p>需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用，我们在程序代码中对此作出判断。<br>判断矩阵是否可逆的一个可选方案是: </p>
<p>判断矩阵的行列式是否为 0，若为 0 ，矩阵就不存在逆矩阵，不为 0 的话，矩阵才存在逆矩阵。</p>

        <h5 id="1-1-2、最小二乘法"   >
          <a href="#1-1-2、最小二乘法" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-2、最小二乘法" class="headerlink" title="1.1.2、最小二乘法"></a>1.1.2、最小二乘法</h5>
      <p>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。</p>

        <h4 id="1-2、线性回归-工作原理"   >
          <a href="#1-2、线性回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2、线性回归-工作原理" class="headerlink" title="1.2、线性回归 工作原理"></a>1.2、线性回归 工作原理</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">读入数据，将数据特征x、特征标签y存储在矩阵x、y中</span><br><span class="line">验证 x^Tx 矩阵是否可逆</span><br><span class="line">使用最小二乘法求得 回归系数 w 的最佳估计</span><br></pre></td></tr></table></div></figure>


        <h4 id="1-3、线性回归-开发流程"   >
          <a href="#1-3、线性回归-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3、线性回归-开发流程" class="headerlink" title="1.3、线性回归 开发流程"></a>1.3、线性回归 开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</span><br><span class="line">分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</span><br><span class="line">训练算法: 找到回归系数</span><br><span class="line">测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果</span><br><span class="line">使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</span><br></pre></td></tr></table></div></figure>


        <h4 id="1-4、线性回归-算法特点"   >
          <a href="#1-4、线性回归-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-4、线性回归-算法特点" class="headerlink" title="1.4、线性回归 算法特点"></a>1.4、线性回归 算法特点</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 结果易于理解，计算上不复杂。</span><br><span class="line">缺点: 对非线性的数据拟合不好。</span><br><span class="line">适用于数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h4 id="1-5、线性回归-项目案例"   >
          <a href="#1-5、线性回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5、线性回归-项目案例" class="headerlink" title="1.5、线性回归 项目案例"></a>1.5、线性回归 项目案例</h4>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="1-5-1、线性回归-项目概述"   >
          <a href="#1-5-1、线性回归-项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5-1、线性回归-项目概述" class="headerlink" title="1.5.1、线性回归 项目概述"></a>1.5.1、线性回归 项目概述</h5>
      <p>根据下图中的点，找出该数据的最佳拟合直线。</p>
<p><img src="img/LinearR_2.png" alt="线性回归数据示例图" title="线性回归数据示例图"></p>
<p>数据格式为: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x0          x1          y </span><br><span class="line">1.000000	0.067732	3.176513</span><br><span class="line">1.000000	0.427810	3.816464</span><br><span class="line">1.000000	0.995731	4.550095</span><br><span class="line">1.000000	0.738336	4.256571</span><br></pre></td></tr></table></div></figure>


        <h5 id="1-5-2、线性回归-编写代码"   >
          <a href="#1-5-2、线性回归-编写代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5-2、线性回归-编写代码" class="headerlink" title="1.5.2、线性回归 编写代码"></a>1.5.2、线性回归 编写代码</h5>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span>                 </span><br><span class="line">    <span class="string">&quot;&quot;&quot; 加载数据</span></span><br><span class="line"><span class="string">        解析以tab键分隔的文件中的浮点数</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        dataMat :   feature 对应的数据集</span></span><br><span class="line"><span class="string">        labelMat :  feature 对应的分类标签，即类别标签</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取样本特征的总数，不算最后的目标变量 </span></span><br><span class="line">    numFeat = <span class="built_in">len</span>(<span class="built_in">open</span>(fileName).readline().split(<span class="string">&#x27;\t&#x27;</span>)) - <span class="number">1</span> </span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        lineArr =[]</span><br><span class="line">        <span class="comment"># 删除一行中以tab分隔的数据前后的空白符号</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># i 从0到2，不包括2 </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeat):</span><br><span class="line">            <span class="comment"># 将数据添加到lineArr List中，每一行数据测试数据组成一个行向量           </span></span><br><span class="line">            lineArr.append(<span class="built_in">float</span>(curLine[i]))</span><br><span class="line">            <span class="comment"># 将测试数据的输入数据部分存储到dataMat 的List中</span></span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        <span class="comment"># 将每一行的最后一个数据，即类别，或者叫目标变量存储到labelMat List中</span></span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(curLine[-<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegres</span>(<span class="params">xArr,yArr</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Description: </span></span><br><span class="line"><span class="string">        线性回归</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        xArr : 输入的样本数据，包含每个样本数据的 feature</span></span><br><span class="line"><span class="string">        yArr : 对应于输入数据的类别标签，也就是每个样本对应的目标变量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ws: 回归系数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># mat()函数将xArr，yArr转换为矩阵 mat().T 代表的是对矩阵进行转置操作</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 矩阵乘法的条件是左矩阵的列数等于右矩阵的行数</span></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="comment"># 因为要用到xTx的逆矩阵，所以事先需要确定计算得到的xTx是否可逆，条件是矩阵的行列式不为0</span></span><br><span class="line">    <span class="comment"># linalg.det() 函数是用来求得矩阵的行列式的，如果矩阵的行列式为0，则这个矩阵是不可逆的，就无法进行接下来的运算                   </span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span> </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 最小二乘法</span></span><br><span class="line">    <span class="comment"># http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133</span></span><br><span class="line">    <span class="comment"># 书中的公式，求得w的最优解</span></span><br><span class="line">    ws = xTx.I * (xMat.T*yMat)            </span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression1</span>():</span></span><br><span class="line">    xArr, yArr = loadDataSet(<span class="string">&quot;data/8.Regression/data.txt&quot;</span>)</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr)</span><br><span class="line">    ws = standRegres(xArr, yArr)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)               <span class="comment">#add_subplot(349)函数的参数的意思是，将画布分成3行4列图像画在从左到右从上到下第9块</span></span><br><span class="line">    ax.scatter(xMat[:, <span class="number">1</span>].flatten(), yMat.T[:, <span class="number">0</span>].flatten().A[<span class="number">0</span>]) <span class="comment">#scatter 的x是xMat中的第二列，y是yMat的第一列</span></span><br><span class="line">    xCopy = xMat.copy() </span><br><span class="line">    xCopy.sort(<span class="number">0</span>)</span><br><span class="line">    yHat = xCopy * ws</span><br><span class="line">    ax.plot(xCopy[:, <span class="number">1</span>], yHat)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>



        <h5 id="1-5-3、线性回归-拟合效果"   >
          <a href="#1-5-3、线性回归-拟合效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5-3、线性回归-拟合效果" class="headerlink" title="1.5.3、线性回归 拟合效果"></a>1.5.3、线性回归 拟合效果</h5>
      <p><img src="img/LinearR_3.png" alt="线性回归数据效果图" title="线性回归数据效果图"></p>

        <h3 id="2、局部加权线性回归"   >
          <a href="#2、局部加权线性回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、局部加权线性回归" class="headerlink" title="2、局部加权线性回归"></a>2、局部加权线性回归</h3>
      <p>线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。</p>
<p>一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数大致为:</p>
<p><img src="img/LinearR_21.png" alt="局部加权线性回归回归系数公式"></p>
<p>目标函数中 w 为权重，不是回归系数。与 kNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数 w 的形式如下: </p>
<p><img src="img/LinearR_4.png" alt="局部加权线性回归回归系数公式"></p>
<p>其中 W 是一个矩阵，用来给每个数据点赋予权重。$\hat{w}$ 则为回归系数。 这两个是不同的概念，请勿混用。</p>
<p>LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下: </p>
<p><img src="img/LinearR_23.png" alt="局部加权线性回归高斯核"></p>
<p>这样就构建了一个只含对角元素的权重矩阵 <strong>w</strong>，并且点 x 与 x(i) 越近，w(i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系。</p>
<p><img src="img/LinearR_6.png" alt="参数k与权重的关系"></p>
<p>上面的图是 每个点的权重图（假定我们正预测的点是 x = 0.5），最上面的图是原始数据集，第二个图显示了当 k = 0.5 时，大部分的数据都用于训练回归模型；而最下面的图显示当 k=0.01 时，仅有很少的局部点被用于训练回归模型。</p>

        <h4 id="2-1、局部加权线性回归-工作原理"   >
          <a href="#2-1、局部加权线性回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1、局部加权线性回归-工作原理" class="headerlink" title="2.1、局部加权线性回归 工作原理"></a>2.1、局部加权线性回归 工作原理</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">读入数据，将数据特征x、特征标签y存储在矩阵x、y中</span><br><span class="line">利用高斯核构造一个权重矩阵 W，对预测点附近的点施加权重</span><br><span class="line">验证 X^TWX 矩阵是否可逆</span><br><span class="line">使用最小二乘法求得 回归系数 w 的最佳估计</span><br></pre></td></tr></table></div></figure>


        <h4 id="2-2、局部加权线性回归-项目案例"   >
          <a href="#2-2、局部加权线性回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2、局部加权线性回归-项目案例" class="headerlink" title="2.2、局部加权线性回归 项目案例"></a>2.2、局部加权线性回归 项目案例</h4>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="2-2-1、局部加权线性回归-项目概述"   >
          <a href="#2-2-1、局部加权线性回归-项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-1、局部加权线性回归-项目概述" class="headerlink" title="2.2.1、局部加权线性回归 项目概述"></a>2.2.1、局部加权线性回归 项目概述</h5>
      <p>我们仍然使用上面 线性回归 的数据集，对这些点进行一个 局部加权线性回归 的拟合。</p>
<p><img src="img/LinearR_2.png" alt="局部加权线性回归数据示例图"></p>
<p>数据格式为: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.000000	0.067732	3.176513</span><br><span class="line">1.000000	0.427810	3.816464</span><br><span class="line">1.000000	0.995731	4.550095</span><br><span class="line">1.000000	0.738336	4.256571</span><br></pre></td></tr></table></div></figure>


        <h5 id="2-2-2、局部加权线性回归-编写代码"   >
          <a href="#2-2-2、局部加权线性回归-编写代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-2、局部加权线性回归-编写代码" class="headerlink" title="2.2.2、局部加权线性回归 编写代码"></a>2.2.2、局部加权线性回归 编写代码</h5>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 局部加权线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span>(<span class="params">testPoint,xArr,yArr,k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Description: </span></span><br><span class="line"><span class="string">            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            testPoint: 样本点</span></span><br><span class="line"><span class="string">            xArr: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr: 每个样本对应的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            testPoint * ws: 数据点与具有权重的系数相乘得到的预测点</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)</span></span><br><span class="line"><span class="string">            理解: x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。</span></span><br><span class="line"><span class="string">            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。</span></span><br><span class="line"><span class="string">            算法思路: 假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，</span></span><br><span class="line"><span class="string">            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># mat() 函数是将array转换为矩阵的函数， mat().T 是转换为矩阵之后，再进行转置操作</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 获得xMat矩阵的行数</span></span><br><span class="line">    m = shape(xMat)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重                   </span></span><br><span class="line">    weights = mat(eye((m)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="comment"># testPoint 的形式是 一个行向量的形式</span></span><br><span class="line">        <span class="comment"># 计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值</span></span><br><span class="line">        diffMat = testPoint - xMat[j,:]</span><br><span class="line">        <span class="comment"># k控制衰减的速度</span></span><br><span class="line">        weights[j,j] = exp(diffMat*diffMat.T/(-<span class="number">2.0</span>*k**<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 根据矩阵乘法计算 xTx ，其中的 weights 矩阵是样本点对应的权重矩阵</span></span><br><span class="line">    xTx = xMat.T * (weights * xMat)</span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 计算出回归系数的一个估计</span></span><br><span class="line">    ws = xTx.I * (xMat.T * (weights * yMat))</span><br><span class="line">    <span class="keyword">return</span> testPoint * ws</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTest</span>(<span class="params">testArr,xArr,yArr,k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Description: </span></span><br><span class="line"><span class="string">            测试局部加权线性回归，对数据集中每个点调用 lwlr() 函数</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            testArr: 测试所用的所有样本点</span></span><br><span class="line"><span class="string">            xArr: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr: 每个样本对应的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            k: 控制核函数的衰减速率</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            yHat: 预测点的估计值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 得到样本点的总数</span></span><br><span class="line">    m = shape(testArr)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 构建一个全部都是 0 的 1 * m 的矩阵</span></span><br><span class="line">    yHat = zeros(m)</span><br><span class="line">    <span class="comment"># 循环所有的数据点，并将lwlr运用于所有的数据点 </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        yHat[i] = lwlr(testArr[i],xArr,yArr,k)</span><br><span class="line">    <span class="comment"># 返回估计值</span></span><br><span class="line">    <span class="keyword">return</span> yHat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTestPlot</span>(<span class="params">xArr,yArr,k=<span class="number">1.0</span></span>):</span>  </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Description:</span></span><br><span class="line"><span class="string">            首先将 X 排序，其余的都与lwlrTest相同，这样更容易绘图</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            xArr: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr: 每个样本对应的类别标签，即目标变量，实际值</span></span><br><span class="line"><span class="string">            k: 控制核函数的衰减速率的有关参数，这里设定的是常量值 1</span></span><br><span class="line"><span class="string">        Return: </span></span><br><span class="line"><span class="string">            yHat: 样本点的估计值</span></span><br><span class="line"><span class="string">            xCopy: xArr的复制</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 生成一个与目标变量数目相同的 0 向量</span></span><br><span class="line">    yHat = zeros(shape(yArr))</span><br><span class="line">    <span class="comment"># 将 xArr 转换为 矩阵形式</span></span><br><span class="line">    xCopy = mat(xArr)</span><br><span class="line">    <span class="comment"># 排序</span></span><br><span class="line">    xCopy.sort(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 开始循环，为每个样本点进行局部加权线性回归，得到最终的目标变量估计值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(shape(xArr)[<span class="number">0</span>]):</span><br><span class="line">        yHat[i] = lwlr(xCopy[i],xArr,yArr,k)</span><br><span class="line">    <span class="keyword">return</span> yHat,xCopy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for LWLR</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression2</span>():</span></span><br><span class="line">    xArr, yArr = loadDataSet(<span class="string">&quot;data/8.Regression/data.txt&quot;</span>)</span><br><span class="line">    yHat = lwlrTest(xArr, xArr, yArr, <span class="number">0.003</span>)</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    srtInd = xMat[:,<span class="number">1</span>].argsort(<span class="number">0</span>)           <span class="comment"># argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出</span></span><br><span class="line">    xSort=xMat[srtInd][:,<span class="number">0</span>,:]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(xSort[:,<span class="number">1</span>], yHat[srtInd])</span><br><span class="line">    ax.scatter(xMat[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], mat(yArr).T.flatten().A[<span class="number">0</span>] , s=<span class="number">2</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>



        <h5 id="2-2-3、局部加权线性回归-拟合效果"   >
          <a href="#2-2-3、局部加权线性回归-拟合效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-3、局部加权线性回归-拟合效果" class="headerlink" title="2.2.3、局部加权线性回归 拟合效果"></a>2.2.3、局部加权线性回归 拟合效果</h5>
      <p><img src="img/LinearR_7.png" alt="局部加权线性回归数据效果图"></p>
<p>上图使用了 3 种不同平滑值绘出的局部加权线性回归的结果。上图中的平滑系数 k =1.0，中图 k = 0.01，下图 k = 0.003 。可以看到，k = 1.0 时的使所有数据等比重，其模型效果与基本的线性回归相同，k=0.01时该模型可以挖出数据的潜在规律，而 k=0.003时则考虑了太多的噪声，进而导致了过拟合现象。</p>

        <h4 id="2-3、局部加权线性回归-注意事项"   >
          <a href="#2-3、局部加权线性回归-注意事项" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3、局部加权线性回归-注意事项" class="headerlink" title="2.3、局部加权线性回归 注意事项"></a>2.3、局部加权线性回归 注意事项</h4>
      <p>局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集。</p>

        <h3 id="3、线性回归-amp-局部加权线性回归-项目案例"   >
          <a href="#3、线性回归-amp-局部加权线性回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、线性回归-amp-局部加权线性回归-项目案例" class="headerlink" title="3、线性回归 &amp; 局部加权线性回归 项目案例"></a>3、线性回归 &amp; 局部加权线性回归 项目案例</h3>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>到此为止，我们已经介绍了找出最佳拟合直线的两种方法，下面我们用这些技术来预测鲍鱼的年龄。</p>

        <h4 id="3-1、项目概述"   >
          <a href="#3-1、项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1、项目概述" class="headerlink" title="3.1、项目概述"></a>3.1、项目概述</h4>
      <p>我们有一份来自 UCI 的数据集合的数据，记录了鲍鱼（一种介壳类水生动物）的年龄。鲍鱼年龄可以从鲍鱼壳的层数推算得到。</p>

        <h4 id="3-2、开发流程"   >
          <a href="#3-2、开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2、开发流程" class="headerlink" title="3.2、开发流程"></a>3.2、开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</span><br><span class="line">分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</span><br><span class="line">训练算法: 找到回归系数</span><br><span class="line">测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果</span><br><span class="line">使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<blockquote>
<p>准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</p>
</blockquote>
<p>数据存储格式:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1	0.455	0.365	0.095	0.514	0.2245	0.101	0.15	15</span><br><span class="line">1	0.35	0.265	0.09	0.2255	0.0995	0.0485	0.07	7</span><br><span class="line">-1	0.53	0.42	0.135	0.677	0.2565	0.1415	0.21	9</span><br><span class="line">1	0.44	0.365	0.125	0.516	0.2155	0.114	0.155	10</span><br><span class="line">0	0.33	0.255	0.08	0.205	0.0895	0.0395	0.055	7</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</p>
</blockquote>
<blockquote>
<p>训练算法: 找到回归系数</p>
</blockquote>
<p>使用上面我们讲到的 局部加权线性回归 训练算法，求出回归系数</p>
<blockquote>
<p>测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssError</span>(<span class="params">yArr,yHatArr</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        返回真实值与预测值误差大小</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        yArr: 样本的真实值</span></span><br><span class="line"><span class="string">        yHatArr: 样本的预测值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        一个数字，代表误差</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">   <span class="keyword">return</span> ((yArr-yHatArr)**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test for abloneDataSet</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abaloneTest</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        预测鲍鱼的年龄</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    abX, abY = loadDataSet(<span class="string">&quot;data/8.Regression/abalone.txt&quot;</span>)</span><br><span class="line">    <span class="comment"># 使用不同的核进行预测</span></span><br><span class="line">    oldyHat01 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">    oldyHat1 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">    oldyHat10 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">10</span>)   </span><br><span class="line">    <span class="comment"># 打印出不同的核预测值与训练数据集上的真实值之间的误差大小</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;old yHat01 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat01.T)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;old yHat1 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat1.T)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;old yHat10 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat10.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印出 不同的核预测值 与 新数据集（测试数据集）上的真实值之间的误差大小</span></span><br><span class="line">    newyHat01 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;new yHat01 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat01.T)</span><br><span class="line">    newyHat1 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;new yHat1 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat1.T)</span><br><span class="line">    newyHat10 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;new yHat10 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat10.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用简单的 线性回归 进行预测，与上面的计算进行比较</span></span><br><span class="line">    standWs = standRegres(abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>])</span><br><span class="line">    standyHat = mat(abX[<span class="number">100</span>:<span class="number">199</span>]) * standWs</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;standRegress error Size is:&quot;</span>, rssError(abY[<span class="number">100</span>:<span class="number">199</span>], standyHat.T.A)</span><br></pre></td></tr></table></div></figure>


<p>根据我们上边的测试，可以看出: </p>
<p>简单线性回归达到了与局部加权现行回归类似的效果。这也说明了一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是 10 吗？或许是，但如果想得到更好的效果，可以尝试用 10 个不同的样本集做 10 次测试来比较结果。</p>
<blockquote>
<p>使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</p>
</blockquote>

        <h3 id="4、缩减系数来-“理解”-数据"   >
          <a href="#4、缩减系数来-“理解”-数据" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、缩减系数来-“理解”-数据" class="headerlink" title="4、缩减系数来 “理解” 数据"></a>4、缩减系数来 “理解” 数据</h3>
      <p>如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即我们不能再使用前面介绍的方法。这是因为在计算 <img src="img/LinearR_8.png" alt="矩阵求逆"> 的时候会出错。</p>
<p>如果特征比样本点还多(n &gt; m)，也就是说输入数据的矩阵 x 不是满秩矩阵。非满秩矩阵求逆时会出现问题。</p>
<p>为了解决这个问题，我们引入了 <code>岭回归（ridge regression）</code> 这种缩减方法。接着是 <code>lasso法</code>，最后介绍 <code>前向逐步回归</code>。</p>

        <h4 id="4-1、岭回归"   >
          <a href="#4-1、岭回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1、岭回归" class="headerlink" title="4.1、岭回归"></a>4.1、岭回归</h4>
      <p>简单来说，岭回归就是在矩阵 <img src="img/LinearR_9.png" alt="矩阵_1"> 上加一个 λI 从而使得矩阵非奇异，进而能对 <img src="img/LinearR_10.png" alt="矩阵_2"> 求逆。其中矩阵I是一个 n * n （等于列数） 的单位矩阵，<br>对角线上元素全为1，其他元素全为0。而λ是一个用户定义的数值，后面会做介绍。在这种情况下，回归系数的计算公式将变成: </p>
<p><img src="img/LinearR_11.png" alt="岭回归的回归系数计算"></p>
<p>岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入 λ 来限制了所有 w 之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作 <code>缩减(shrinkage)</code>。</p>
<p><img src="img/LinearR_22.png" alt="岭回归"></p>
<p>缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。</p>
<p>这里通过预测误差最小化得到 λ: 数据获取之后，首先抽一部分数据用于测试，剩余的作为训练集用于训练参数 w。训练完毕后在测试集上测试预测性能。通过选取不同的 λ 来重复上述测试过程，最终得到一个使预测误差最小的 λ 。</p>

        <h5 id="4-1-1、岭回归-原始代码"   >
          <a href="#4-1-1、岭回归-原始代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-1、岭回归-原始代码" class="headerlink" title="4.1.1、岭回归 原始代码"></a>4.1.1、岭回归 原始代码</h5>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeRegres</span>(<span class="params">xMat,yMat,lam=<span class="number">0.2</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Desc: </span></span><br><span class="line"><span class="string">            这个函数实现了给定 lambda 下的岭回归求解。</span></span><br><span class="line"><span class="string">            如果数据的特征比样本点还多，就不能再使用上面介绍的的线性回归和局部线性回归了，因为计算 (xTx)^(-1)会出现错误。</span></span><br><span class="line"><span class="string">            如果特征比样本点还多（n &gt; m），也就是说，输入数据的矩阵x不是满秩矩阵。非满秩矩阵在求逆时会出现问题。</span></span><br><span class="line"><span class="string">            为了解决这个问题，我们下边讲一下: 岭回归，这是我们要讲的第一种缩减方法。</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            xMat: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yMat: 每个样本对应的类别标签，即目标变量，实际值</span></span><br><span class="line"><span class="string">            lam: 引入的一个λ值，使得矩阵非奇异</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            经过岭回归公式计算得到的回归系数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="comment"># 岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆</span></span><br><span class="line">    denom = xTx + eye(shape(xMat)[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="comment"># 检查行列式是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆。</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(denom) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = denom.I * (xMat.T*yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span>(<span class="params">xArr,yArr</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Desc: </span></span><br><span class="line"><span class="string">            函数 ridgeTest() 用于在一组 λ 上测试结果</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            xArr: 样本数据的特征，即 feature</span></span><br><span class="line"><span class="string">            yArr: 样本数据的类别标签，即真实数据</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            wMat: 将所有的回归系数输出到一个矩阵并返回</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat=mat(yArr).T</span><br><span class="line">    <span class="comment"># 计算Y的均值</span></span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Y的所有的特征减去均值</span></span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    <span class="comment"># 标准化 x，计算 xMat 平均值</span></span><br><span class="line">    xMeans = mean(xMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 然后计算 X的方差</span></span><br><span class="line">    xVar = var(xMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 所有特征都减去各自的均值并除以方差</span></span><br><span class="line">    xMat = (xMat - xMeans)/xVar</span><br><span class="line">    <span class="comment"># 可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数。</span></span><br><span class="line">    numTestPts = <span class="number">30</span></span><br><span class="line">    <span class="comment"># 创建30 * m 的全部数据为0 的矩阵</span></span><br><span class="line">    wMat = zeros((numTestPts,shape(xMat)[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTestPts):</span><br><span class="line">        <span class="comment"># exp() 返回 e^x </span></span><br><span class="line">        ws = ridgeRegres(xMat,yMat,exp(i-<span class="number">10</span>))</span><br><span class="line">        wMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> wMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for ridgeRegression</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression3</span>():</span></span><br><span class="line">    abX,abY = loadDataSet(<span class="string">&quot;data/8.Regression/abalone.txt&quot;</span>)</span><br><span class="line">    ridgeWeights = ridgeTest(abX, abY)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(ridgeWeights)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>



        <h5 id="4-1-2、岭回归在鲍鱼数据集上的运行效果"   >
          <a href="#4-1-2、岭回归在鲍鱼数据集上的运行效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-2、岭回归在鲍鱼数据集上的运行效果" class="headerlink" title="4.1.2、岭回归在鲍鱼数据集上的运行效果"></a>4.1.2、岭回归在鲍鱼数据集上的运行效果</h5>
      <p><img src="img/LinearR_12.png" alt="岭回归的运行效果"></p>
<p>上图绘制出了回归系数与 log(λ) 的关系。在最左边，即 λ 最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减为0；在中间部分的某值将可以取得最好的预测效果。为了定量地找到最佳参数值，还需要进行交叉验证。另外，要判断哪些变量对结果预测最具有影响力，在上图中观察它们对应的系数大小就可以了。</p>

        <h4 id="4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator"   >
          <a href="#4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator" class="headerlink" title="4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)"></a>4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)</h4>
      <p>在增加如下约束时，普通的最小二乘法回归会得到与岭回归一样的公式: </p>
<p><img src="img/LinearR_13.png" alt="lasso_1"></p>
<p>上式限定了所有回归系数的平方和不能大于 λ 。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得到一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。</p>
<p>与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下: </p>
<p><img src="img/LinearR_14.png" alt="lasso_2"></p>
<p>唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭: 在 λ 足够小的时候，一些系数会因此被迫缩减到 0.这个特性可以帮助我们更好地理解数据。</p>

        <h4 id="4-3、前向逐步回归"   >
          <a href="#4-3、前向逐步回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-3、前向逐步回归" class="headerlink" title="4.3、前向逐步回归"></a>4.3、前向逐步回归</h4>
      <p>前向逐步回归算法可以得到与 lasso 差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有权重都设置为 0，然后每一步所做的决策是对某个权重增加或减少一个很小的值。</p>
<p>伪代码如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">数据标准化，使其分布满足 0 均值 和单位方差</span><br><span class="line">在每轮迭代过程中: </span><br><span class="line">    设置当前最小误差 lowestError 为正无穷</span><br><span class="line">    对每个特征:</span><br><span class="line">        增大或缩小:</span><br><span class="line">            改变一个系数得到一个新的 w</span><br><span class="line">            计算新 w 下的误差</span><br><span class="line">            如果误差 Error 小于当前最小误差 lowestError: 设置 Wbest 等于当前的 W</span><br><span class="line">        将 W 设置为新的 Wbest</span><br></pre></td></tr></table></div></figure>


        <h5 id="4-3-1、前向逐步回归-原始代码"   >
          <a href="#4-3-1、前向逐步回归-原始代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-3-1、前向逐步回归-原始代码" class="headerlink" title="4.3.1、前向逐步回归 原始代码"></a>4.3.1、前向逐步回归 原始代码</h5>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stageWise</span>(<span class="params">xArr,yArr,eps=<span class="number">0.01</span>,numIt=<span class="number">100</span></span>):</span></span><br><span class="line">    xMat = mat(xArr); yMat=mat(yArr).T</span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean     <span class="comment"># 也可以规则化ys但会得到更小的coef</span></span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    m,n=shape(xMat)</span><br><span class="line">    <span class="comment">#returnMat = zeros((numIt,n)) # 测试代码删除</span></span><br><span class="line">    ws = zeros((n,<span class="number">1</span>)); wsTest = ws.copy(); wsMax = ws.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numIt):</span><br><span class="line">        <span class="built_in">print</span> (ws.T)</span><br><span class="line">        lowestError = inf; </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> sign <span class="keyword">in</span> [-<span class="number">1</span>,<span class="number">1</span>]:</span><br><span class="line">                wsTest = ws.copy()</span><br><span class="line">                wsTest[j] += eps*sign</span><br><span class="line">                yTest = xMat*wsTest</span><br><span class="line">                rssE = rssError(yMat.A,yTest.A)</span><br><span class="line">                <span class="keyword">if</span> rssE &lt; lowestError:</span><br><span class="line">                    lowestError = rssE</span><br><span class="line">                    wsMax = wsTest</span><br><span class="line">        ws = wsMax.copy()</span><br><span class="line">        returnMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for stageWise</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression4</span>():</span></span><br><span class="line">    xArr,yArr=loadDataSet(<span class="string">&quot;data/8.Regression/abalone.txt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(stageWise(xArr,yArr,<span class="number">0.01</span>,<span class="number">200</span>))</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    yM = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yM</span><br><span class="line">    weights = standRegres(xMat, yMat.T)</span><br><span class="line">    <span class="built_in">print</span> (weights.T)</span><br></pre></td></tr></table></div></figure>



        <h5 id="4-3-2、逐步线性回归在鲍鱼数据集上的运行效果"   >
          <a href="#4-3-2、逐步线性回归在鲍鱼数据集上的运行效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-3-2、逐步线性回归在鲍鱼数据集上的运行效果" class="headerlink" title="4.3.2、逐步线性回归在鲍鱼数据集上的运行效果"></a>4.3.2、逐步线性回归在鲍鱼数据集上的运行效果</h5>
      <p><img src="img/LinearR_15.png" alt="逐步线性回归运行效果"></p>
<p>逐步线性回归算法的主要优点在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。</p>

        <h4 id="4-4、小结"   >
          <a href="#4-4、小结" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-4、小结" class="headerlink" title="4.4、小结"></a>4.4、小结</h4>
      <p>当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias），与此同时却减小了模型的方差。</p>

        <h3 id="5、权衡偏差和方差"   >
          <a href="#5、权衡偏差和方差" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、权衡偏差和方差" class="headerlink" title="5、权衡偏差和方差"></a>5、权衡偏差和方差</h3>
      <p>任何时候，一旦发现模型和测量值之间存在差异，就说出现了误差。当考虑模型中的 “噪声” 或者说误差时，必须考虑其来源。你可能会对复杂的过程进行简化，这将导致在模型和测量值之间出现 “噪声” 或误差，若无法理解数据的真实生成过程，也会导致差异的产生。另外，测量过程本身也可能产生 “噪声” 或者问题。下面我们举一个例子，我们使用 <code>线性回归</code> 和 <code>局部加权线性回归</code> 处理过一个从文件导入的二维数据。</p>
<p><img src="img/LinearR_16.png" alt="生成公式"></p>
<p>其中的 N(0, 1) 是一个均值为 0、方差为 1 的正态分布。我们尝试过仅用一条直线来拟合上述数据。不难想到，直线所能得到的最佳拟合应该是 3.0+1.7x 这一部分。这样的话，误差部分就是 0.1sin(30x)+0.06N(0, 1) 。在上面，我们使用了局部加权线性回归来试图捕捉数据背后的结构。该结构拟合起来有一定的难度，因此我们测试了多组不同的局部权重来找到具有最小测试误差的解。</p>
<p>下图给出了训练误差和测试误差的曲线图，上面的曲面就是测试误差，下面的曲线是训练误差。我们根据 预测鲍鱼年龄 的实验知道: 如果降低核的大小，那么训练误差将变小。从下图开看，从左到右就表示了核逐渐减小的过程。</p>
<p><img src="img/LinearR_17.png" alt="偏差方差图"></p>
<p>一般认为，上述两种误差由三个部分组成: 偏差、测量误差和随机噪声。局部加权线性回归 和 预测鲍鱼年龄 中，我们通过引入了三个越来越小的核来不断增大模型的方差。</p>
<p>在缩减系数来“理解”数据这一节中，我们介绍了缩减法，可以将一些系数缩减成很小的值或直接缩减为 0 ，这是一个增大模型偏差的例子。通过把一些特征的回归系数缩减到 0 ，同时也就减小了模型的复杂度。例子中有 8 个特征，消除其中两个后不仅使模型更易理解，同时还降低了预测误差。对照上图，左侧是参数缩减过于严厉的结果，而右侧是无缩减的效果。</p>
<p>方差是可以度量的。如果从鲍鱼数据中取一个随机样本集（例如取其中 100 个数据）并用线性模型拟合，将会得到一组回归系数。同理，再取出另一组随机样本集并拟合，将会得到另一组回归系数。这些系数间的差异大小也就是模型方差的反映。</p>

        <h3 id="6、回归-项目案例"   >
          <a href="#6、回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#6、回归-项目案例" class="headerlink" title="6、回归 项目案例"></a>6、回归 项目案例</h3>
      
        <h4 id="项目案例1-预测乐高玩具套装的价格"   >
          <a href="#项目案例1-预测乐高玩具套装的价格" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-预测乐高玩具套装的价格" class="headerlink" title="项目案例1: 预测乐高玩具套装的价格"></a>项目案例1: 预测乐高玩具套装的价格</h4>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h5>
      <p>Dangler 喜欢为乐高套装估价，我们用回归技术来帮助他建立一个预测模型。</p>

        <h5 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h5>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(1) 收集数据: 用 Google Shopping 的API收集数据。</span><br><span class="line">(2) 准备数据: 从返回的JSON数据中抽取价格。</span><br><span class="line">(3) 分析数据: 可视化并观察数据。</span><br><span class="line">(4) 训练算法: 构建不同的模型，采用逐步线性回归和直接的线性回归模型。</span><br><span class="line">(5) 测试算法: 使用交叉验证来测试不同的模型，分析哪个效果最好。</span><br><span class="line">(6) 使用算法: 这次练习的目标就是生成数据模型。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 使用 Google 购物的 API </p>
</blockquote>
<p>由于 Google 提供的 api 失效，我们只能自己下载咯，将数据存储在了 input 文件夹下的 setHtml 文件夹下</p>
<blockquote>
<p>准备数据: 从返回的 JSON 数据中抽取价格</p>
</blockquote>
<p>因为我们这里不是在线的，就不再是 JSON 了，我们直接解析线下的网页，得到我们想要的数据。</p>
<blockquote>
<p>分析数据: 可视化并观察数据</p>
</blockquote>
<p>这里我们将解析得到的数据打印出来，然后观察数据。</p>
<blockquote>
<p>训练算法: 构建不同的模型</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从页面读取数据，生成retX和retY列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrapePage</span>(<span class="params">retX, retY, inFile, yr, numPce, origPrc</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打开并读取HTML文件</span></span><br><span class="line">    fr = <span class="built_in">open</span>(inFile)</span><br><span class="line">    soup = BeautifulSoup(fr.read())</span><br><span class="line">    i=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据HTML页面结构进行解析</span></span><br><span class="line">    currentRow = soup.findAll(<span class="string">&#x27;table&#x27;</span>, r=<span class="string">&quot;%d&quot;</span> % i)</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">len</span>(currentRow)!=<span class="number">0</span>):</span><br><span class="line">        currentRow = soup.findAll(<span class="string">&#x27;table&#x27;</span>, r=<span class="string">&quot;%d&quot;</span> % i)</span><br><span class="line">        title = currentRow[<span class="number">0</span>].findAll(<span class="string">&#x27;a&#x27;</span>)[<span class="number">1</span>].text</span><br><span class="line">        lwrTitle = title.lower()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找是否有全新标签</span></span><br><span class="line">        <span class="keyword">if</span> (lwrTitle.find(<span class="string">&#x27;new&#x27;</span>) &gt; -<span class="number">1</span>) <span class="keyword">or</span> (lwrTitle.find(<span class="string">&#x27;nisb&#x27;</span>) &gt; -<span class="number">1</span>):</span><br><span class="line">            newFlag = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newFlag = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找是否已经标志出售，我们只收集已出售的数据</span></span><br><span class="line">        soldUnicde = currentRow[<span class="number">0</span>].findAll(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].findAll(<span class="string">&#x27;span&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(soldUnicde)==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;item #%d did not sell&quot;</span> % i</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 解析页面获取当前价格</span></span><br><span class="line">            soldPrice = currentRow[<span class="number">0</span>].findAll(<span class="string">&#x27;td&#x27;</span>)[<span class="number">4</span>]</span><br><span class="line">            priceStr = soldPrice.text</span><br><span class="line">            priceStr = priceStr.replace(<span class="string">&#x27;$&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="comment">#strips out $</span></span><br><span class="line">            priceStr = priceStr.replace(<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="comment">#strips out ,</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(soldPrice)&gt;<span class="number">1</span>:</span><br><span class="line">                priceStr = priceStr.replace(<span class="string">&#x27;Free shipping&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            sellingPrice = <span class="built_in">float</span>(priceStr)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 去掉不完整的套装价格</span></span><br><span class="line">            <span class="keyword">if</span>  sellingPrice &gt; origPrc * <span class="number">0.5</span>:</span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&quot;%d\t%d\t%d\t%f\t%f&quot;</span> % (yr,numPce,newFlag,origPrc, sellingPrice)</span><br><span class="line">                    retX.append([yr, numPce, newFlag, origPrc])</span><br><span class="line">                    retY.append(sellingPrice)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        currentRow = soup.findAll(<span class="string">&#x27;table&#x27;</span>, r=<span class="string">&quot;%d&quot;</span> % i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 依次读取六种乐高套装的数据，并生成数据矩阵        </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setDataCollect</span>(<span class="params">retX, retY</span>):</span></span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego8288.html&#x27;</span>, <span class="number">2006</span>, <span class="number">800</span>, <span class="number">49.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10030.html&#x27;</span>, <span class="number">2002</span>, <span class="number">3096</span>, <span class="number">269.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10179.html&#x27;</span>, <span class="number">2007</span>, <span class="number">5195</span>, <span class="number">499.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10181.html&#x27;</span>, <span class="number">2007</span>, <span class="number">3428</span>, <span class="number">199.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10189.html&#x27;</span>, <span class="number">2008</span>, <span class="number">5922</span>, <span class="number">299.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10196.html&#x27;</span>, <span class="number">2009</span>, <span class="number">3263</span>, <span class="number">249.99</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用交叉验证来测试不同的模型，分析哪个效果最好</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉验证测试岭回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossValidation</span>(<span class="params">xArr,yArr,numVal=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="comment"># 获得数据点个数，xArr和yArr具有相同长度</span></span><br><span class="line">    m = <span class="built_in">len</span>(yArr)</span><br><span class="line">    indexList = <span class="built_in">range</span>(m)</span><br><span class="line">    errorMat = zeros((numVal,<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主循环 交叉验证循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numVal):</span><br><span class="line">        <span class="comment"># 随机拆分数据，将数据分为训练集（90%）和测试集（10%）</span></span><br><span class="line">        trainX=[]; trainY=[]</span><br><span class="line">        testX = []; testY = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对数据进行混洗操作</span></span><br><span class="line">        random.shuffle(indexList)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 切分训练集和测试集</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">if</span> j &lt; m*<span class="number">0.9</span>: </span><br><span class="line">                trainX.append(xArr[indexList[j]])</span><br><span class="line">                trainY.append(yArr[indexList[j]])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                testX.append(xArr[indexList[j]])</span><br><span class="line">                testY.append(yArr[indexList[j]])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得回归系数矩阵</span></span><br><span class="line">        wMat = ridgeTest(trainX,trainY)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 循环遍历矩阵中的30组回归系数</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>):</span><br><span class="line">            <span class="comment"># 读取训练集和数据集</span></span><br><span class="line">            matTestX = mat(testX); matTrainX=mat(trainX)</span><br><span class="line">            <span class="comment"># 对数据进行标准化</span></span><br><span class="line">            meanTrain = mean(matTrainX,<span class="number">0</span>)</span><br><span class="line">            varTrain = var(matTrainX,<span class="number">0</span>)</span><br><span class="line">            matTestX = (matTestX-meanTrain)/varTrain</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试回归效果并存储</span></span><br><span class="line">            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算误差</span></span><br><span class="line">            errorMat[i,k] = ((yEst.T.A-array(testY))**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算误差估计值的均值</span></span><br><span class="line">    meanErrors = mean(errorMat,<span class="number">0</span>)</span><br><span class="line">    minMean = <span class="built_in">float</span>(<span class="built_in">min</span>(meanErrors))</span><br><span class="line">    bestWeights = wMat[nonzero(meanErrors==minMean)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不要使用标准化的数据，需要对数据进行还原来得到输出结果</span></span><br><span class="line">    xMat = mat(xArr); yMat=mat(yArr).T</span><br><span class="line">    meanX = mean(xMat,<span class="number">0</span>); varX = var(xMat,<span class="number">0</span>)</span><br><span class="line">    unReg = bestWeights/varX</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出构建的模型</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;the best model from Ridge Regression is:\n&quot;</span>,unReg</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;with constant term: &quot;</span>,-<span class="number">1</span>*<span class="built_in">sum</span>(multiply(meanX,unReg)) + mean(yMat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># predict for lego&#x27;s price</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression5</span>():</span></span><br><span class="line">    lgX = []</span><br><span class="line">    lgY = []</span><br><span class="line"></span><br><span class="line">    setDataCollect(lgX, lgY)</span><br><span class="line">    crossValidation(lgX, lgY, <span class="number">10</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 这次练习的目标就是生成数据模型</p>
</blockquote>

        <h2 id="7、选读内容"   >
          <a href="#7、选读内容" class="heading-link"><i class="fas fa-link"></i></a><a href="#7、选读内容" class="headerlink" title="7、选读内容"></a>7、选读内容</h2>
      <p>求解线性回归可以有很多种方式，除了上述的方法（正规方程 normal equation）解决之外，还有可以对Cost function 求导，其中最简单的方法就是梯度下降法。</p>
<p> 那么正规方程就可以直接得出真实值。而梯度下降法只能给出近似值。</p>
<p>以下是梯度下降法和正规方程的比较:</p>
<div class="table-container"><table>
<thead>
<tr>
<th>梯度下降法</th>
<th align="center">正规方程</th>
</tr>
</thead>
<tbody><tr>
<td>结果为真实值的近似值</td>
<td align="center">结果为真实值</td>
</tr>
<tr>
<td>需要循环多次</td>
<td align="center">无需循环</td>
</tr>
<tr>
<td>样本数量大的时候也ok</td>
<td align="center">样本数量特别大的时候会很慢（n&gt;10000）</td>
</tr>
</tbody></table></div>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="http://example.com">TainTear</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="http://example.com/2021/08/08/ml_8/">http://example.com/2021/08/08/ml_8/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/09/ml_9/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第9章 树回归</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/07/ml_7/"><span class="paginator-prev__text">第7章 集成方法 ensemble method</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">
          回归（Regression） 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92-%E5%9C%BA%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">
          回归 场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92-%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">
          回归 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.1.</span> <span class="toc-text">
          1、线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%A1%BB%E7%9F%A5%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.1.</span> <span class="toc-text">
          1.1、线性回归 须知概念</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-1%E3%80%81%E7%9F%A9%E9%98%B5%E6%B1%82%E9%80%86"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">
          1.1.1、矩阵求逆</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-2%E3%80%81%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">
          1.1.2、最小二乘法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.2.</span> <span class="toc-text">
          1.2、线性回归 工作原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.3.</span> <span class="toc-text">
          1.3、线性回归 开发流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%AE%97%E6%B3%95%E7%89%B9%E7%82%B9"><span class="toc-number">3.1.4.</span> <span class="toc-text">
          1.4、线性回归 算法特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">3.1.5.</span> <span class="toc-text">
          1.5、线性回归 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-1%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.5.1.</span> <span class="toc-text">
          1.5.1、线性回归 项目概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-2%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-number">3.1.5.2.</span> <span class="toc-text">
          1.5.2、线性回归 编写代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-3%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E6%8B%9F%E5%90%88%E6%95%88%E6%9E%9C"><span class="toc-number">3.1.5.3.</span> <span class="toc-text">
          1.5.3、线性回归 拟合效果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.2.</span> <span class="toc-text">
          2、局部加权线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">3.2.1.</span> <span class="toc-text">
          2.1、局部加权线性回归 工作原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">3.2.2.</span> <span class="toc-text">
          2.2、局部加权线性回归 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-1%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">
          2.2.1、局部加权线性回归 项目概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-2%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">
          2.2.2、局部加权线性回归 编写代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-3%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E6%8B%9F%E5%90%88%E6%95%88%E6%9E%9C"><span class="toc-number">3.2.2.3.</span> <span class="toc-text">
          2.2.3、局部加权线性回归 拟合效果</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3%E3%80%81%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">3.2.3.</span> <span class="toc-text">
          2.3、局部加权线性回归 注意事项</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-amp-%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">3.3.</span> <span class="toc-text">
          3、线性回归 &amp; 局部加权线性回归 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1%E3%80%81%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">3.3.1.</span> <span class="toc-text">
          3.1、项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2%E3%80%81%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.3.2.</span> <span class="toc-text">
          3.2、开发流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E7%BC%A9%E5%87%8F%E7%B3%BB%E6%95%B0%E6%9D%A5-%E2%80%9C%E7%90%86%E8%A7%A3%E2%80%9D-%E6%95%B0%E6%8D%AE"><span class="toc-number">3.4.</span> <span class="toc-text">
          4、缩减系数来 “理解” 数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1%E3%80%81%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">3.4.1.</span> <span class="toc-text">
          4.1、岭回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-1%E3%80%81%E5%B2%AD%E5%9B%9E%E5%BD%92-%E5%8E%9F%E5%A7%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">3.4.1.1.</span> <span class="toc-text">
          4.1.1、岭回归 原始代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-2%E3%80%81%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%9C%A8%E9%B2%8D%E9%B1%BC%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E8%BF%90%E8%A1%8C%E6%95%88%E6%9E%9C"><span class="toc-number">3.4.1.2.</span> <span class="toc-text">
          4.1.2、岭回归在鲍鱼数据集上的运行效果</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2%E3%80%81%E5%A5%97%E7%B4%A2%E6%96%B9%E6%B3%95-Lasso%EF%BC%8CThe-Least-Absolute-Shrinkage-and-Selection-Operator"><span class="toc-number">3.4.2.</span> <span class="toc-text">
          4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3%E3%80%81%E5%89%8D%E5%90%91%E9%80%90%E6%AD%A5%E5%9B%9E%E5%BD%92"><span class="toc-number">3.4.3.</span> <span class="toc-text">
          4.3、前向逐步回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-3-1%E3%80%81%E5%89%8D%E5%90%91%E9%80%90%E6%AD%A5%E5%9B%9E%E5%BD%92-%E5%8E%9F%E5%A7%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">3.4.3.1.</span> <span class="toc-text">
          4.3.1、前向逐步回归 原始代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-3-2%E3%80%81%E9%80%90%E6%AD%A5%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9C%A8%E9%B2%8D%E9%B1%BC%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E8%BF%90%E8%A1%8C%E6%95%88%E6%9E%9C"><span class="toc-number">3.4.3.2.</span> <span class="toc-text">
          4.3.2、逐步线性回归在鲍鱼数据集上的运行效果</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4%E3%80%81%E5%B0%8F%E7%BB%93"><span class="toc-number">3.4.4.</span> <span class="toc-text">
          4.4、小结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%9D%83%E8%A1%A1%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">3.5.</span> <span class="toc-text">
          5、权衡偏差和方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">3.6.</span> <span class="toc-text">
          6、回归 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B1-%E9%A2%84%E6%B5%8B%E4%B9%90%E9%AB%98%E7%8E%A9%E5%85%B7%E5%A5%97%E8%A3%85%E7%9A%84%E4%BB%B7%E6%A0%BC"><span class="toc-number">3.6.1.</span> <span class="toc-text">
          项目案例1: 预测乐高玩具套装的价格</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">3.6.1.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.6.1.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E9%80%89%E8%AF%BB%E5%86%85%E5%AE%B9"><span class="toc-number">4.</span> <span class="toc-text">
          7、选读内容</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>