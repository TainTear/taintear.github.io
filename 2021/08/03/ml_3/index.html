<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="决策树 概述       决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 决策树学习通常包括 3 个">
<meta property="og:type" content="article">
<meta property="og:title" content="第3章 决策树">
<meta property="og:url" content="http://example.com/2021/08/03/ml_3/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:description" content="决策树 概述       决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 决策树学习通常包括 3 个">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/08/03/ml_3/img/DecisionTree_headPage_xy.png">
<meta property="og:image" content="http://example.com/2021/08/03/ml_3/img/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg">
<meta property="og:image" content="http://example.com/2021/08/03/ml_3/img/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png">
<meta property="og:image" content="http://example.com/2021/08/03/ml_3/img/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg">
<meta property="article:published_time" content="2021-08-02T19:07:57.000Z">
<meta property="article:modified_time" content="2021-08-28T19:35:25.836Z">
<meta property="article:author" content="TainTear">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/08/03/ml_3/img/DecisionTree_headPage_xy.png"><title>第3章 决策树 | TainTear's Blog</title><link ref="canonical" href="http://example.com/2021/08/03/ml_3/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第3章 决策树</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-03</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/DecisionTree_headPage_xy.png" alt="决策树_首页" title="决策树首页"></p>

        <h2 id="决策树-概述"   >
          <a href="#决策树-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-概述" class="headerlink" title="决策树 概述"></a>决策树 概述</h2>
      <p><code>决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。</code></p>
<p><code>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</code></p>
<p><code>决策树学习通常包括 3 个步骤: 特征选择、决策树的生成和决策树的修剪。</code></p>

        <h2 id="决策树-场景"   >
          <a href="#决策树-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-场景" class="headerlink" title="决策树 场景"></a>决策树 场景</h2>
      <p>一个叫做 “二十个问题” 的游戏，游戏的规则很简单: 参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。</p>
<p>一个邮件分类系统，大致工作流程如下: </p>
<p><img src="img/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="决策树-流程图" title="决策树示例流程图"></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先检测发送邮件域名地址。如果地址为 myEmployer.com, 则将其放在分类 &quot;无聊时需要阅读的邮件&quot;中。</span><br><span class="line">如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 &quot;曲棍球&quot; , 如果包含则将邮件归类到 &quot;需要及时处理的朋友邮件&quot;, </span><br><span class="line">如果不包含则将邮件归类到 &quot;无需阅读的垃圾邮件&quot; 。</span><br></pre></td></tr></table></div></figure>

<p>决策树的定义: </p>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>
<p>用决策树对需要测试的实例进行分类: 从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。</p>

        <h2 id="决策树-原理"   >
          <a href="#决策树-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2>
      
        <h3 id="决策树-须知概念"   >
          <a href="#决策树-须知概念" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3>
      
        <h4 id="信息熵-amp-信息增益"   >
          <a href="#信息熵-amp-信息增益" class="heading-link"><i class="fas fa-link"></i></a><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4>
      <p>熵（entropy）:<br>熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p>
<p>信息论（information theory）中的熵（香农熵）:<br>是一种信息的度量方式，表示信息的混乱程度，也就是说: 信息越有序，信息熵越低。例如: 火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p>
<p>信息增益（information gain）:<br>在划分数据集前后信息发生的变化称为信息增益。</p>

        <h3 id="决策树-工作原理"   >
          <a href="#决策树-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3>
      <p>如何构造一个决策树?<br/><br>我们使用 createBranch() 方法，如下所示: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def createBranch():</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">    检测数据集中的所有数据的分类标签是否相同:</span><br><span class="line">        If so return 类标签</span><br><span class="line">        Else:</span><br><span class="line">            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）</span><br><span class="line">            划分数据集</span><br><span class="line">            创建分支节点</span><br><span class="line">                for 每个划分的子集</span><br><span class="line">                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中</span><br><span class="line">            return 分支节点</span><br></pre></td></tr></table></div></figure>


        <h3 id="决策树-开发流程"   >
          <a href="#决策树-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法。</span><br><span class="line">准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)</span><br><span class="line">分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。</span><br><span class="line">训练算法: 构造树的数据结构。</span><br><span class="line">测试算法: 使用训练好的树计算错误率。</span><br><span class="line">使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</span><br></pre></td></tr></table></div></figure>


        <h3 id="决策树-算法特点"   >
          <a href="#决策树-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-算法特点" class="headerlink" title="决策树 算法特点"></a>决策树 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。</span><br><span class="line">缺点: 容易过拟合。</span><br><span class="line">适用数据类型: 数值型和标称型。</span><br></pre></td></tr></table></div></figure>


        <h2 id="决策树-项目案例"   >
          <a href="#决策树-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-项目案例" class="headerlink" title="决策树 项目案例"></a>决策树 项目案例</h2>
      
        <h3 id="项目案例1-判定鱼类和非鱼类"   >
          <a href="#项目案例1-判定鱼类和非鱼类" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-判定鱼类和非鱼类" class="headerlink" title="项目案例1: 判定鱼类和非鱼类"></a>项目案例1: 判定鱼类和非鱼类</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>根据以下 2 个特征，将动物分成两类: 鱼类和非鱼类。</p>
<p>特征: </p>
<ol>
<li>不浮出水面是否可以生存</li>
<li>是否有脚蹼</li>
</ol>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法</span><br><span class="line">准备数据: 树构造算法（这里使用的是ID3算法，因此数值型数据必须离散化。）</span><br><span class="line">分析数据: 可以使用任何方法，构造树完成之后，我们可以将树画出来。</span><br><span class="line">训练算法: 构造树结构</span><br><span class="line">测试算法: 使用习得的决策树执行分类</span><br><span class="line">使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p><img src="img/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png" alt="海洋生物数据"></p>
<p>我们利用 createDataSet() 函数输入数据</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span>():</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;no surfacing&#x27;</span>, <span class="string">&#x27;flippers&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>准备数据: 树构造算法</p>
</blockquote>
<p>此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。</p>
<blockquote>
<p>分析数据: 可以使用任何方法，构造树完成之后，我们可以将树画出来。</p>
</blockquote>
<p><img src="img/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg" alt="熵的计算公式"></p>
<p>计算给定数据集的香农熵的函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="comment"># 求list的长度，表示计算参与训练的数据量</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)</span><br><span class="line">    <span class="comment"># 计算分类标签label出现的次数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># the the number of unique elements and their occurrence</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签</span></span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于 label 标签的占比，求出 label 标签的香农熵</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        <span class="comment"># 使用所有类标签的发生频率计算类别出现的概率。</span></span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key])/numEntries</span><br><span class="line">        <span class="comment"># 计算香农熵，以 2 为底求对数</span></span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></div></figure>

<p>按照给定特征划分数据集</p>
<p><code>将指定特征的特征值等于 value 的行剩下列作为子数据集。</code></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span>(<span class="params">dataSet, index, value</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)</span></span><br><span class="line"><span class="string">        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集                 待划分的数据集</span></span><br><span class="line"><span class="string">        index 表示每一行的index列        划分数据集的特征</span></span><br><span class="line"><span class="string">        value 表示index列对应的value值   需要返回的特征的值。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet: </span><br><span class="line">        <span class="comment"># index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line">        <span class="comment"># 判断index列的值是否为value</span></span><br><span class="line">        <span class="keyword">if</span> featVec[index] == value:</span><br><span class="line">            <span class="comment"># chop out index used for splitting</span></span><br><span class="line">            <span class="comment"># [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行</span></span><br><span class="line">            reducedFeatVec = featVec[:index]</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            请百度查询一下:  extend和append的区别</span></span><br><span class="line"><span class="string">            music_media.append(object) 向列表中添加一个对象object</span></span><br><span class="line"><span class="string">            music_media.extend(sequence) 把一个序列seq的内容添加到列表中 (跟 += 在list运用类似， music_media += sequence)</span></span><br><span class="line"><span class="string">            1、使用append的时候，是将object看作一个对象，整体打包添加到music_media对象中。</span></span><br><span class="line"><span class="string">            2、使用extend的时候，是将sequence看作一个序列，将这个序列和music_media序列合并，并放在其后面。</span></span><br><span class="line"><span class="string">            music_media = []</span></span><br><span class="line"><span class="string">            music_media.extend([1,2,3])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果: </span></span><br><span class="line"><span class="string">            #[1, 2, 3]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.append([4,5,6])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果: </span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6]]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.extend([7,8,9])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果: </span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6], 7, 8, 9]</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            reducedFeatVec.extend(featVec[index+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment"># [index+1:]表示从跳过 index 的 index+1行，取接下来的数据</span></span><br><span class="line">            <span class="comment"># 收集结果值 index列为value的行【该行需要排除index列】</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></div></figure>

<p>选择最好的数据集划分方式</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;chooseBestFeatureToSplit(选择最好的特征)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestFeature 最优的特征列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 求第一行有多少列的 Feature, 最后一列是label列嘛</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 数据集的原始信息熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 最优的信息增益值, 和最优的Featurn编号</span></span><br><span class="line">    bestInfoGain, bestFeature = <span class="number">0.0</span>, -<span class="number">1</span></span><br><span class="line">    <span class="comment"># iterate over all the features</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        <span class="comment"># create a list of all the examples of this feature</span></span><br><span class="line">        <span class="comment"># 获取对应的feature下的所有数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># get a set of unique values</span></span><br><span class="line">        <span class="comment"># 获取剔重后的集合，使用set对list数据进行去重</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        <span class="comment"># 创建一个临时的信息熵</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 遍历某一列的value集合，计算该列的信息熵 </span></span><br><span class="line">        <span class="comment"># 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算概率</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet)/<span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            <span class="comment"># 计算信息熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值</span></span><br><span class="line">        <span class="comment"># 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;infoGain=&#x27;</span>, infoGain, <span class="string">&#x27;bestFeature=&#x27;</span>, i, baseEntropy, newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问: 上面的 newEntropy 为什么是根据子集计算的呢？</span><br><span class="line">答: 因为我们在根据一个特征计算香农熵的时候，该特征的分类值是相同，这个特征这个分类的香农熵为 0；</span><br><span class="line">这就是为什么计算新的香农熵的时候使用的是子集。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 构造树的数据结构</p>
</blockquote>
<p>创建树的函数代码如下: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span>(<span class="params">dataSet, labels</span>):</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件: 所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件: 使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择最优的列，得到最优列对应的label含义</span></span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取label的名称</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    <span class="comment"># 初始化myTree</span></span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 注: labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改</span></span><br><span class="line">    <span class="comment"># 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示&#x27;no surfacing&#x27; is not in list</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    <span class="comment"># 取出最优列，然后它的branch做分类</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        <span class="comment"># 求出剩余的标签label</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">        <span class="comment"># print &#x27;myTree&#x27;, value, myTree</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用决策树执行分类</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">inputTree, featLabels, testVec</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;classify(给输入的节点，进行分类)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inputTree  决策树模型</span></span><br><span class="line"><span class="string">        featLabels Feature标签对应的名称</span></span><br><span class="line"><span class="string">        testVec    测试输入的数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        classLabel 分类的结果值，需要映射label才能知道名称</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取tree的根节点对于的key值</span></span><br><span class="line">    firstStr = <span class="built_in">list</span>(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 通过key得到根节点对应的value</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;+++&#x27;</span>, firstStr, <span class="string">&#x27;xxx&#x27;</span>, secondDict, <span class="string">&#x27;---&#x27;</span>, key, <span class="string">&#x27;&gt;&gt;&gt;&#x27;</span>, valueOfFeat</span><br><span class="line">    <span class="comment"># 判断分枝是否结束: 判断valueOfFeat是否是dict类型</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(valueOfFeat, <span class="built_in">dict</span>):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</p>
</blockquote>

        <h3 id="项目案例2-使用决策树预测隐形眼镜类型"   >
          <a href="#项目案例2-使用决策树预测隐形眼镜类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例2-使用决策树预测隐形眼镜类型" class="headerlink" title="项目案例2: 使用决策树预测隐形眼镜类型"></a>项目案例2: 使用决策树预测隐形眼镜类型</h3>
      <p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <ol>
<li>收集数据: 提供的文本文件。</li>
<li>解析数据: 解析 tab 键分隔的数据行</li>
<li>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</li>
<li>训练算法: 使用 createTree() 函数。</li>
<li>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</li>
<li>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</li>
</ol>
<blockquote>
<p>收集数据: 提供的文本文件</p>
</blockquote>
<p>文本文件数据格式如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">young	myope	no	reduced	no lenses</span><br><span class="line">pre	myope	no	reduced	no lenses</span><br><span class="line">presbyopic	myope	no	reduced	no lenses</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>解析数据: 解析 tab 键分隔的数据行</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lecses = [inst.strip().split(<span class="string">&#x27;\t&#x27;</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLabels = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;prescript&#x27;</span>, <span class="string">&#x27;astigmatic&#x27;</span>, <span class="string">&#x27;tearRate&#x27;</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>treePlotter.createPlot(lensesTree)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 使用 createTree() 函数</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree = trees.createTree(lenses, lensesLabels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree</span><br><span class="line">&#123;<span class="string">&#x27;tearRate&#x27;</span>: &#123;<span class="string">&#x27;reduced&#x27;</span>: <span class="string">&#x27;no lenses&#x27;</span>, <span class="string">&#x27;normal&#x27;</span>: &#123;<span class="string">&#x27;astigmatic&#x27;</span>:&#123;<span class="string">&#x27;yes&#x27;</span>:</span><br><span class="line">&#123;<span class="string">&#x27;prescript&#x27;</span>:&#123;<span class="string">&#x27;hyper&#x27;</span>:&#123;<span class="string">&#x27;age&#x27;</span>:&#123;<span class="string">&#x27;pre&#x27;</span>:<span class="string">&#x27;no lenses&#x27;</span>, <span class="string">&#x27;presbyopic&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;no lenses&#x27;</span>, <span class="string">&#x27;young&#x27;</span>:<span class="string">&#x27;hard&#x27;</span>&#125;&#125;, <span class="string">&#x27;myope&#x27;</span>:<span class="string">&#x27;hard&#x27;</span>&#125;&#125;, <span class="string">&#x27;no&#x27;</span>:&#123;<span class="string">&#x27;age&#x27;</span>:&#123;<span class="string">&#x27;pre&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;soft&#x27;</span>, <span class="string">&#x27;presbyopic&#x27;</span>:&#123;<span class="string">&#x27;prescript&#x27;</span>: &#123;<span class="string">&#x27;hyper&#x27;</span>:<span class="string">&#x27;soft&#x27;</span>, <span class="string">&#x27;myope&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;no lenses&#x27;</span>&#125;&#125;, <span class="string">&#x27;young&#x27;</span>:<span class="string">&#x27;soft&#x27;</span>&#125;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</p>
</blockquote>
<blockquote>
<p>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</p>
</blockquote>
<p>使用 pickle 模块存储决策树</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span>(<span class="params">inputTree, filename</span>):</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span>(<span class="params">filename</span>):</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></div></figure>

<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong>        </li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="http://example.com">TainTear</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="http://example.com/2021/08/03/ml_3/">http://example.com/2021/08/03/ml_3/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/04/ml_4/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第4章 基于概率论的分类方法-朴素贝叶斯</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/02/ml_2/"><span class="paginator-prev__text">第2章 k-近邻算法</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">
          决策树 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E5%9C%BA%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">
          决策树 场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">
          决策树 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E9%A1%BB%E7%9F%A5%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.</span> <span class="toc-text">
          决策树 须知概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5-amp-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">3.1.1.</span> <span class="toc-text">
          信息熵 &amp; 信息增益</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">
          决策树 工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text">
          决策树 开发流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E7%AE%97%E6%B3%95%E7%89%B9%E7%82%B9"><span class="toc-number">3.4.</span> <span class="toc-text">
          决策树 算法特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">4.</span> <span class="toc-text">
          决策树 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B1-%E5%88%A4%E5%AE%9A%E9%B1%BC%E7%B1%BB%E5%92%8C%E9%9D%9E%E9%B1%BC%E7%B1%BB"><span class="toc-number">4.1.</span> <span class="toc-text">
          项目案例1: 判定鱼类和非鱼类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">4.1.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B2-%E4%BD%BF%E7%94%A8%E5%86%B3%E7%AD%96%E6%A0%91%E9%A2%84%E6%B5%8B%E9%9A%90%E5%BD%A2%E7%9C%BC%E9%95%9C%E7%B1%BB%E5%9E%8B"><span class="toc-number">4.2.</span> <span class="toc-text">
          项目案例2: 使用决策树预测隐形眼镜类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0-1"><span class="toc-number">4.2.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B-1"><span class="toc-number">4.2.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>