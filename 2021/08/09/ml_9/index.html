<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="树回归 概述       我们本章介绍 CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。                     树回归 场景       我们在第 8 章中介绍了线性回归的一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加">
<meta property="og:type" content="article">
<meta property="og:title" content="第9章 树回归">
<meta property="og:url" content="http://example.com/2021/08/09/ml_9/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:description" content="树回归 概述       我们本章介绍 CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。                     树回归 场景       我们在第 8 章中介绍了线性回归的一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/TreeRegression_headPage_xy.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/RegTree_1.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/RegTree_2.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/RegTree_3.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/RegTree_4.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/RegTree_5.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/GUI%E7%A4%BA%E4%BE%8B%E5%9B%BE.png">
<meta property="og:image" content="http://example.com/2021/08/09/ml_9/img/GUI%E6%9B%B4%E5%A5%BD%E7%9A%84%E7%A4%BA%E4%BE%8B%E5%9B%BE.png">
<meta property="article:published_time" content="2021-08-08T19:07:57.000Z">
<meta property="article:modified_time" content="2021-08-28T19:36:12.736Z">
<meta property="article:author" content="TainTear">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/08/09/ml_9/img/TreeRegression_headPage_xy.png"><title>第9章 树回归 | TainTear's Blog</title><link ref="canonical" href="http://example.com/2021/08/09/ml_9/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第9章 树回归</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-09</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/TreeRegression_headPage_xy.png" alt="预测数值型数据回归首页" title="树回归首页"></p>

        <h2 id="树回归-概述"   >
          <a href="#树回归-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#树回归-概述" class="headerlink" title="树回归 概述"></a>树回归 概述</h2>
      <p><code>我们本章介绍 CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。</code></p>

        <h2 id="树回归-场景"   >
          <a href="#树回归-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#树回归-场景" class="headerlink" title="树回归 场景"></a>树回归 场景</h2>
      <p>我们在第 8 章中介绍了线性回归的一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。</p>
<p>一种可行的方法是将数据集切分成很多份易建模的数据，然后利用我们的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树回归和回归法就相当有用。</p>
<p>除了我们在 第3章 中介绍的 决策树算法，我们介绍一个新的叫做 CART(Classification And Regression Trees, 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。</p>

        <h2 id="1、树回归-原理"   >
          <a href="#1、树回归-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、树回归-原理" class="headerlink" title="1、树回归 原理"></a>1、树回归 原理</h2>
      
        <h3 id="1-1、树回归-原理概述"   >
          <a href="#1-1、树回归-原理概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1、树回归-原理概述" class="headerlink" title="1.1、树回归 原理概述"></a>1.1、树回归 原理概述</h3>
      <p>为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？</p>
<p>在这里，计算连续型数值的混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。</p>
<p>上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。总方差可以通过均方差乘以数据集中样本点的个数来得到。</p>

        <h3 id="1-2、树构建算法-比较"   >
          <a href="#1-2、树构建算法-比较" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2、树构建算法-比较" class="headerlink" title="1.2、树构建算法 比较"></a>1.2、树构建算法 比较</h3>
      <p>我们在 第3章 中使用的树构建算法是 ID3 。ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。</p>
<p>除了切分过于迅速外， ID3 算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。</p>
<p>CART 是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对 CART 稍作修改就可以处理回归问题。第 3 章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。</p>
<p>回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型。</p>

        <h4 id="1-2-1、附加-各常见树构造算法的划分分支方式"   >
          <a href="#1-2-1、附加-各常见树构造算法的划分分支方式" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-1、附加-各常见树构造算法的划分分支方式" class="headerlink" title="1.2.1、附加 各常见树构造算法的划分分支方式"></a>1.2.1、附加 各常见树构造算法的划分分支方式</h4>
      <p>还有一点要说明，构建决策树算法，常用到的是三个方法: ID3, C4.5, CART.<br>三种方法区别是划分树的分支的方式:</p>
<ol>
<li>ID3 是信息增益分支</li>
<li>C4.5 是信息增益率分支</li>
<li>CART 做分类工作时，采用 GINI 值作为节点分裂的依据；回归时，采用样本的最小方差作为节点的分裂依据。</li>
</ol>
<p>工程上总的来说: </p>
<p>CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；</p>
<p>以此拓展出以 CART 为基础的 “树群” Random forest ， 以 回归树 为基础的 “树群” GBDT 。</p>

        <h3 id="1-3、树回归-工作原理"   >
          <a href="#1-3、树回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3、树回归-工作原理" class="headerlink" title="1.3、树回归 工作原理"></a>1.3、树回归 工作原理</h3>
      <p>1、找到数据集切分的最佳位置，函数 chooseBestSplit() 伪代码大致如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对每个特征:</span><br><span class="line">    对每个特征值: </span><br><span class="line">        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）</span><br><span class="line">        计算切分的误差</span><br><span class="line">        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差</span><br><span class="line">返回最佳切分的特征和阈值</span><br></pre></td></tr></table></div></figure>
<p>2、树构建算法，函数 createTree() 伪代码大致如下:   </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">找到最佳的待切分特征:</span><br><span class="line">    如果该节点不能再分，将该节点存为叶节点</span><br><span class="line">    执行二元切分</span><br><span class="line">    在右子树调用 createTree() 方法</span><br><span class="line">    在左子树调用 createTree() 方法</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-4、树回归-开发流程"   >
          <a href="#1-4、树回归-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-4、树回归-开发流程" class="headerlink" title="1.4、树回归 开发流程"></a>1.4、树回归 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(1) 收集数据: 采用任意方法收集数据。</span><br><span class="line">(2) 准备数据: 需要数值型数据，标称型数据应该映射成二值型数据。</span><br><span class="line">(3) 分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树。</span><br><span class="line">(4) 训练算法: 大部分时间都花费在叶节点树模型的构建上。</span><br><span class="line">(5) 测试算法: 使用测试数据上的R^2值来分析模型的效果。</span><br><span class="line">(6) 使用算法: 使用训练处的树做预测，预测结果还可以用来做很多事情。</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-5、树回归-算法特点"   >
          <a href="#1-5、树回归-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5、树回归-算法特点" class="headerlink" title="1.5、树回归 算法特点"></a>1.5、树回归 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 可以对复杂和非线性的数据建模。</span><br><span class="line">缺点: 结果不易理解。</span><br><span class="line">适用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-6、回归树-项目案例"   >
          <a href="#1-6、回归树-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-6、回归树-项目案例" class="headerlink" title="1.6、回归树 项目案例"></a>1.6、回归树 项目案例</h3>
      
        <h4 id="1-6-1、项目概述"   >
          <a href="#1-6-1、项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-6-1、项目概述" class="headerlink" title="1.6.1、项目概述"></a>1.6.1、项目概述</h4>
      <p>在简单数据集上生成一棵回归树。</p>

        <h4 id="1-6-2、开发流程"   >
          <a href="#1-6-2、开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-6-2、开发流程" class="headerlink" title="1.6.2、开发流程"></a>1.6.2、开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</span><br><span class="line">分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</span><br><span class="line">训练算法: 大部分时间都花费在叶节点树模型的构建上</span><br><span class="line">测试算法: 使用测试数据上的R^2值来分析模型的效果</span><br><span class="line">使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<p>data1.txt 文件中存储的数据格式如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.036098	0.155096</span><br><span class="line">0.993349	1.077553</span><br><span class="line">0.530897	0.893462</span><br><span class="line">0.712386	0.564858</span><br><span class="line">0.343554	-0.371700</span><br><span class="line">0.098016	-0.332760</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</p>
</blockquote>
<blockquote>
<p>分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</p>
</blockquote>
<p>基于 CART 算法构建回归树的简单数据集<br><img src="img/RegTree_1.png" alt="基于 CART 算法构建回归树的简单数据集">  </p>
<p>用于测试回归树的分段常数数据集<br><img src="img/RegTree_2.png" alt="用于测试回归树的分段常数数据集">  </p>
<blockquote>
<p>训练算法: 构造树的数据结构</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binSplitDataSet</span>(<span class="params">dataSet, feature, value</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;binSplitDataSet(将数据集，按照feature列的value进行 二元切分)</span></span><br><span class="line"><span class="string">        Description: 在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat 数据集</span></span><br><span class="line"><span class="string">        feature 待切分的特征列</span></span><br><span class="line"><span class="string">        value 特征列要比较的值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        mat0 小于等于 value 的数据集在左边</span></span><br><span class="line"><span class="string">        mat1 大于 value 的数据集在右边</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># # 测试案例</span></span><br><span class="line">    <span class="comment"># print &#x27;dataSet[:, feature]=&#x27;, dataSet[:, feature]</span></span><br><span class="line">    <span class="comment"># print &#x27;nonzero(dataSet[:, feature] &gt; value)[0]=&#x27;, nonzero(dataSet[:, feature] &gt; value)[0]</span></span><br><span class="line">    <span class="comment"># print &#x27;nonzero(dataSet[:, feature] &lt;= value)[0]=&#x27;, nonzero(dataSet[:, feature] &lt;= value)[0]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)</span></span><br><span class="line">    <span class="comment"># nonzero(dataSet[:, feature] &gt; value)  返回结果为true行的index下标</span></span><br><span class="line">    mat0 = dataSet[nonzero(dataSet[:, feature] &lt;= value)[<span class="number">0</span>], :]</span><br><span class="line">    mat1 = dataSet[nonzero(dataSet[:, feature] &gt; value)[<span class="number">0</span>], :]</span><br><span class="line">    <span class="keyword">return</span> mat0, mat1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.用最佳方式切分数据集</span></span><br><span class="line"><span class="comment"># 2.生成相应的叶节点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestSplit</span>(<span class="params">dataSet, leafType=regLeaf, errType=regErr, ops=(<span class="params"><span class="number">1</span>, <span class="number">4</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet   加载的原始数据集</span></span><br><span class="line"><span class="string">        leafType  建立叶子点的函数</span></span><br><span class="line"><span class="string">        errType   误差计算函数(求总方差)</span></span><br><span class="line"><span class="string">        ops       [容许误差下降值，切分的最少样本数]。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestIndex feature的index坐标</span></span><br><span class="line"><span class="string">        bestValue 切分的最优值</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。</span></span><br><span class="line">    <span class="comment"># 之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。</span></span><br><span class="line">    <span class="comment"># 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分</span></span><br><span class="line">    tolS = ops[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 划分最小 size 小于，就不继续划分了</span></span><br><span class="line">    tolN = ops[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 如果结果集(最后一列为1个变量)，就返回退出</span></span><br><span class="line">    <span class="comment"># .T 对数据集进行转置</span></span><br><span class="line">    <span class="comment"># .tolist()[0] 转化为数组并取第0列</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">set</span>(dataSet[:, -<span class="number">1</span>].T.tolist()[<span class="number">0</span>])) == <span class="number">1</span>: <span class="comment"># 如果集合size为1，不用继续划分。</span></span><br><span class="line">        <span class="comment">#  exit cond 1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    <span class="comment"># 计算行列值</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 无分类误差的总方差和</span></span><br><span class="line">    <span class="comment"># the choice of the best feature is driven by Reduction in RSS error from mean</span></span><br><span class="line">    S = errType(dataSet)</span><br><span class="line">    <span class="comment"># inf 正无穷大</span></span><br><span class="line">    bestS, bestIndex, bestValue = inf, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环处理每一列对应的feature值</span></span><br><span class="line">    <span class="keyword">for</span> featIndex <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>): <span class="comment"># 对于每个特征</span></span><br><span class="line">        <span class="comment"># [0]表示这一列的[所有行]，不要[0]就是一个array[[所有行]]</span></span><br><span class="line">        <span class="keyword">for</span> splitVal <span class="keyword">in</span> <span class="built_in">set</span>(dataSet[:, featIndex].T.tolist()[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># 对该列进行分组，然后组内的成员的val值进行 二元切分</span></span><br><span class="line">            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)</span><br><span class="line">            <span class="comment"># 判断二元切分的方式的元素数量是否符合预期</span></span><br><span class="line">            <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            newS = errType(mat0) + errType(mat1)</span><br><span class="line">            <span class="comment"># 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差</span></span><br><span class="line">            <span class="comment"># 如果划分后误差小于 bestS，则说明找到了新的bestS</span></span><br><span class="line">            <span class="keyword">if</span> newS &lt; bestS:</span><br><span class="line">                bestIndex = featIndex</span><br><span class="line">                bestValue = splitVal</span><br><span class="line">                bestS = newS</span><br><span class="line">    <span class="comment"># 判断二元切分的方式的元素误差是否符合预期</span></span><br><span class="line">    <span class="comment"># if the decrease (S-bestS) is less than a threshold don&#x27;t do the split</span></span><br><span class="line">    <span class="keyword">if</span> (S - bestS) &lt; tolS:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)</span><br><span class="line">    <span class="comment"># 对整体的成员进行判断，是否符合预期</span></span><br><span class="line">    <span class="comment"># 如果集合的 size 小于 tolN </span></span><br><span class="line">    <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN): <span class="comment"># 当最佳划分后，集合过小，也不划分，产生叶节点</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    <span class="keyword">return</span> bestIndex, bestValue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># assume dataSet is NumPy Mat so we can array filtering</span></span><br><span class="line"><span class="comment"># 假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span>(<span class="params">dataSet, leafType=regLeaf, errType=regErr, ops=(<span class="params"><span class="number">1</span>, <span class="number">4</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;createTree(获取回归树)</span></span><br><span class="line"><span class="string">        Description: 递归函数: 如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet      加载的原始数据集</span></span><br><span class="line"><span class="string">        leafType     建立叶子点的函数</span></span><br><span class="line"><span class="string">        errType      误差计算函数</span></span><br><span class="line"><span class="string">        ops=(1, 4)   [容许误差下降值，切分的最少样本数]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retTree    决策树最后的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 选择最好的切分方式:  feature索引值，最优切分值</span></span><br><span class="line">    <span class="comment"># choose the best split</span></span><br><span class="line">    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)</span><br><span class="line">    <span class="comment"># if the splitting hit a stop condition return val</span></span><br><span class="line">    <span class="comment"># 如果 splitting 达到一个停止条件，那么返回 val</span></span><br><span class="line">    <span class="keyword">if</span> feat <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    retTree = &#123;&#125;</span><br><span class="line">    retTree[<span class="string">&#x27;spInd&#x27;</span>] = feat</span><br><span class="line">    retTree[<span class="string">&#x27;spVal&#x27;</span>] = val</span><br><span class="line">    <span class="comment"># 大于在右边，小于在左边，分为2个数据集</span></span><br><span class="line">    lSet, rSet = binSplitDataSet(dataSet, feat, val)</span><br><span class="line">    <span class="comment"># 递归的进行调用，在左右子树中继续递归生成树</span></span><br><span class="line">    retTree[<span class="string">&#x27;left&#x27;</span>] = createTree(lSet, leafType, errType, ops)</span><br><span class="line">    retTree[<span class="string">&#x27;right&#x27;</span>] = createTree(rSet, leafType, errType, ops)</span><br><span class="line">    <span class="keyword">return</span> retTree</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<blockquote>
<p>测试算法: 使用测试数据上的R^2值来分析模型的效果</p>
</blockquote>
<blockquote>
<p>使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</p>
</blockquote>

        <h2 id="2、树剪枝"   >
          <a href="#2、树剪枝" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、树剪枝" class="headerlink" title="2、树剪枝"></a>2、树剪枝</h2>
      <p>一棵树如果节点过多，表明该模型可能对数据进行了 “过拟合”。</p>
<p>通过降低决策树的复杂度来避免过拟合的过程称为 <code>剪枝（pruning）</code>。在函数 chooseBestSplit() 中提前终止条件，实际上是在进行一种所谓的 <code>预剪枝（prepruning）</code>操作。另一个形式的剪枝需要使用测试集和训练集，称作 <code>后剪枝（postpruning）</code>。</p>

        <h3 id="2-1、预剪枝-prepruning"   >
          <a href="#2-1、预剪枝-prepruning" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1、预剪枝-prepruning" class="headerlink" title="2.1、预剪枝(prepruning)"></a>2.1、预剪枝(prepruning)</h3>
      <p>顾名思义，预剪枝就是及早的停止树增长，在构造决策树的同时进行剪枝。</p>
<p>所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。</p>

        <h3 id="2-2、后剪枝-postpruning"   >
          <a href="#2-2、后剪枝-postpruning" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2、后剪枝-postpruning" class="headerlink" title="2.2、后剪枝(postpruning)"></a>2.2、后剪枝(postpruning)</h3>
      <p>决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。合并也被称作 <code>塌陷处理</code> ，在回归树中一般采用取需要合并的所有子树的平均值。后剪枝是目前最普遍的做法。</p>
<p>后剪枝 prune() 的伪代码如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于已有的树切分测试数据:</span><br><span class="line">    如果存在任一子集是一棵树，则在该子集递归剪枝过程</span><br><span class="line">    计算将当前两个叶节点合并后的误差</span><br><span class="line">    计算不合并的误差</span><br><span class="line">    如果合并会降低误差的话，就将叶节点合并</span><br></pre></td></tr></table></div></figure>


        <h3 id="2-3、剪枝-代码"   >
          <a href="#2-3、剪枝-代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3、剪枝-代码" class="headerlink" title="2.3、剪枝 代码"></a>2.3、剪枝 代码</h3>
      <p>回归树剪枝函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断节点是否是一个字典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isTree</span>(<span class="params">obj</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        测试输入变量是否是一棵树,即是否是一个字典</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        obj -- 输入变量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回布尔类型的结果。如果 obj 是一个字典，返回true，否则返回 false</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="built_in">type</span>(obj).__name__ == <span class="string">&#x27;dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算左右枝丫的均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMean</span>(<span class="params">tree</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        从上往下遍历树直到叶节点为止，如果找到两个叶节点则计算它们的平均值。</span></span><br><span class="line"><span class="string">        对 tree 进行塌陷处理，即返回树平均值。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 输入的树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回 tree 节点的平均值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;right&#x27;</span>] = getMean(tree[<span class="string">&#x27;right&#x27;</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;left&#x27;</span>] = getMean(tree[<span class="string">&#x27;left&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> (tree[<span class="string">&#x27;left&#x27;</span>]+tree[<span class="string">&#x27;right&#x27;</span>])/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否适合合并分枝</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span>(<span class="params">tree, testData</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        从上而下找到叶节点，用测试数据集来判断将这些叶节点合并是否能降低测试误差</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 待剪枝的树</span></span><br><span class="line"><span class="string">        testData -- 剪枝所需要的测试数据 testData </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tree -- 剪枝完成的树</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值</span></span><br><span class="line">    <span class="keyword">if</span> shape(testData)[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> getMean(tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断分枝是否是dict字典，如果是就将测试数据集进行切分</span></span><br><span class="line">    <span class="keyword">if</span> (isTree(tree[<span class="string">&#x27;right&#x27;</span>]) <span class="keyword">or</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>])):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">&#x27;spInd&#x27;</span>], tree[<span class="string">&#x27;spVal&#x27;</span>])</span><br><span class="line">    <span class="comment"># 如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;left&#x27;</span>] = prune(tree[<span class="string">&#x27;left&#x27;</span>], lSet)</span><br><span class="line">    <span class="comment"># 如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;right&#x27;</span>] = prune(tree[<span class="string">&#x27;right&#x27;</span>], rSet)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。</span></span><br><span class="line">    <span class="comment"># 1. 如果正确 </span></span><br><span class="line">    <span class="comment">#   * 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较</span></span><br><span class="line">    <span class="comment">#   * 如果 合并的总方差 &lt; 不合并的总方差，那么就进行合并</span></span><br><span class="line">    <span class="comment"># 注意返回的结果:  如果可以合并，原来的dict就变为了 数值</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]) <span class="keyword">and</span> <span class="keyword">not</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">&#x27;spInd&#x27;</span>], tree[<span class="string">&#x27;spVal&#x27;</span>])</span><br><span class="line">        <span class="comment"># power(x, y)表示x的y次方</span></span><br><span class="line">        errorNoMerge = <span class="built_in">sum</span>(power(lSet[:, -<span class="number">1</span>] - tree[<span class="string">&#x27;left&#x27;</span>], <span class="number">2</span>)) + <span class="built_in">sum</span>(power(rSet[:, -<span class="number">1</span>] - tree[<span class="string">&#x27;right&#x27;</span>], <span class="number">2</span>))</span><br><span class="line">        treeMean = (tree[<span class="string">&#x27;left&#x27;</span>] + tree[<span class="string">&#x27;right&#x27;</span>])/<span class="number">2.0</span></span><br><span class="line">        errorMerge = <span class="built_in">sum</span>(power(testData[:, -<span class="number">1</span>] - treeMean, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果 合并的总方差 &lt; 不合并的总方差，那么就进行合并</span></span><br><span class="line">        <span class="keyword">if</span> errorMerge &lt; errorNoMerge:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;merging&quot;</span></span><br><span class="line">            <span class="keyword">return</span> treeMean</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> tree</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tree</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="3、模型树"   >
          <a href="#3、模型树" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、模型树" class="headerlink" title="3、模型树"></a>3、模型树</h2>
      
        <h3 id="3-1、模型树-简介"   >
          <a href="#3-1、模型树-简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1、模型树-简介" class="headerlink" title="3.1、模型树 简介"></a>3.1、模型树 简介</h3>
      <p>用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的 <code>分段线性（piecewise linear）</code> 是指模型由多个线性片段组成。</p>
<p>我们看一下图 9-4 中的数据，如果使用两条直线拟合是否比使用一组常数来建模好呢？答案显而易见。可以设计两条分别从 0.0<del>0.3、从 0.3</del>1.0 的直线，于是就可以得到两个线性模型。因为数据集里的一部分数据（0.0<del>0.3）以某个线性模型建模，而另一部分数据（0.3</del>1.0）则以另一个线性模型建模，因此我们说采用了所谓的分段线性模型。</p>
<p>决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。</p>
<p><img src="img/RegTree_3.png" alt="分段线性数据"></p>
<p>将之前的回归树的代码稍作修改，就可以在叶节点生成线性模型而不是常数值。下面将利用树生成算法对数据进行划分，且每份切分数据都能很容易被线性模型所表示。这个算法的关键在于误差的计算。</p>
<p>那么为了找到最佳切分，应该怎样计算误差呢？前面用于回归树的误差计算方法这里不能再用。稍加变化，对于给定的数据集，应该先用模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到了所需的误差。</p>

        <h3 id="3-2、模型树-代码"   >
          <a href="#3-2、模型树-代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2、模型树-代码" class="headerlink" title="3.2、模型树 代码"></a>3.2、模型树 代码</h3>
      <p>模型树的叶节点生成函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到模型的ws系数: f(x) = x0 + x1*featrue1+ x3*featrue2 ...</span></span><br><span class="line"><span class="comment"># create linear model and return coeficients</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelLeaf</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        当数据不再需要切分的时候，生成叶节点的模型。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        调用 linearSolve 函数，返回得到的 回归系数ws</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算线性模型的误差值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelErr</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        在给定数据集上计算误差。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        调用 linearSolve 函数，返回 yHat 和 Y 之间的平方误差。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    yHat = X * ws</span><br><span class="line">    <span class="comment"># print corrcoef(yHat, Y, rowvar=0)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(power(Y - yHat, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment"># helper function used in two places</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linearSolve</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        将数据集格式化成目标变量Y和自变量X，执行简单的线性回归，得到ws</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ws -- 执行线性回归的回归系数 </span></span><br><span class="line"><span class="string">        X -- 格式化自变量X</span></span><br><span class="line"><span class="string">        Y -- 格式化目标变量Y</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 产生一个关于1的矩阵</span></span><br><span class="line">    X = mat(ones((m, n)))</span><br><span class="line">    Y = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># X的0列为1，常数项，用于计算平衡误差</span></span><br><span class="line">    X[:, <span class="number">1</span>: n] = dataSet[:, <span class="number">0</span>: n-<span class="number">1</span>]</span><br><span class="line">    Y = dataSet[:, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转置矩阵*矩阵</span></span><br><span class="line">    xTx = X.T * X</span><br><span class="line">    <span class="comment"># 如果矩阵的逆不存在，会造成程序异常</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">&#x27;This matrix is singular, cannot do inverse,\ntry increasing the second value of ops&#x27;</span>)</span><br><span class="line">    <span class="comment"># 最小二乘法求最优解:  w0*1+w1*x1=y</span></span><br><span class="line">    ws = xTx.I * (X.T * Y)</span><br><span class="line">    <span class="keyword">return</span> ws, X, Y</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="3-3、模型树-运行结果"   >
          <a href="#3-3、模型树-运行结果" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3、模型树-运行结果" class="headerlink" title="3.3、模型树 运行结果"></a>3.3、模型树 运行结果</h3>
      <p><img src="img/RegTree_4.png" alt="模型树运行结果"></p>

        <h2 id="4、树回归-项目案例"   >
          <a href="#4、树回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、树回归-项目案例" class="headerlink" title="4、树回归 项目案例"></a>4、树回归 项目案例</h2>
      
        <h3 id="4-1、项目案例1-树回归与标准回归的比较"   >
          <a href="#4-1、项目案例1-树回归与标准回归的比较" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1、项目案例1-树回归与标准回归的比较" class="headerlink" title="4.1、项目案例1: 树回归与标准回归的比较"></a>4.1、项目案例1: 树回归与标准回归的比较</h3>
      
        <h4 id="4-1-1、项目概述"   >
          <a href="#4-1-1、项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-1、项目概述" class="headerlink" title="4.1.1、项目概述"></a>4.1.1、项目概述</h4>
      <p>前面介绍了模型树、回归树和一般的回归方法，下面测试一下哪个模型最好。</p>
<p>这些模型将在某个数据上进行测试，该数据涉及人的智力水平和自行车的速度的关系。当然，数据是假的。</p>

        <h4 id="4-1-2、开发流程"   >
          <a href="#4-1-2、开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-2、开发流程" class="headerlink" title="4.1.2、开发流程"></a>4.1.2、开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</span><br><span class="line">分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</span><br><span class="line">训练算法: 模型树的构建</span><br><span class="line">测试算法: 使用测试数据上的R^2值来分析模型的效果</span><br><span class="line">使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<blockquote>
<p>准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</p>
</blockquote>
<p>数据存储格式:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3.000000	46.852122</span><br><span class="line">23.000000	178.676107</span><br><span class="line">0.000000	86.154024</span><br><span class="line">6.000000	68.707614</span><br><span class="line">15.000000	139.737693</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</p>
</blockquote>
<p><img src="img/RegTree_5.png" alt="骑自行车速度和智商之间的关系"></p>
<blockquote>
<p>训练算法: 模型树的构建</p>
</blockquote>
<p>用树回归进行预测的代码</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回归树测试案例</span></span><br><span class="line"><span class="comment"># 为了和 modelTreeEval() 保持一致，保留两个输入参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regTreeEval</span>(<span class="params">model, inDat</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对 回归树 进行预测</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- 指定模型，可选值为 回归树模型 或者 模型树模型，这里为回归树</span></span><br><span class="line"><span class="string">        inDat -- 输入的测试数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float(model) -- 将输入的模型数据转换为 浮点数 返回</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型树测试案例</span></span><br><span class="line"><span class="comment"># 对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，</span></span><br><span class="line"><span class="comment"># 也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelTreeEval</span>(<span class="params">model, inDat</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对 模型树 进行预测</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- 输入模型，可选值为 回归树模型 或者 模型树模型，这里为模型树模型</span></span><br><span class="line"><span class="string">        inDat -- 输入的测试数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float(X * model) -- 将测试数据乘以 回归系数 得到一个预测值 ，转化为 浮点数 返回</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = shape(inDat)[<span class="number">1</span>]</span><br><span class="line">    X = mat(ones((<span class="number">1</span>, n+<span class="number">1</span>)))</span><br><span class="line">    X[:, <span class="number">1</span>: n+<span class="number">1</span>] = inDat</span><br><span class="line">    <span class="comment"># print X, model</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(X * model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测的结果</span></span><br><span class="line"><span class="comment"># 在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。</span></span><br><span class="line"><span class="comment"># modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。</span></span><br><span class="line"><span class="comment"># 此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上</span></span><br><span class="line"><span class="comment"># 调用modelEval()函数，该函数的默认值为regTreeEval()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeForeCast</span>(<span class="params">tree, inData, modelEval=regTreeEval</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对特定模型的树进行预测，可以是 回归树 也可以是 模型树</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 已经训练好的树的模型</span></span><br><span class="line"><span class="string">        inData -- 输入的测试数据</span></span><br><span class="line"><span class="string">        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回预测值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree):</span><br><span class="line">        <span class="keyword">return</span> modelEval(tree, inData)</span><br><span class="line">    <span class="keyword">if</span> inData[tree[<span class="string">&#x27;spInd&#x27;</span>]] &lt;= tree[<span class="string">&#x27;spVal&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">            <span class="keyword">return</span> treeForeCast(tree[<span class="string">&#x27;left&#x27;</span>], inData, modelEval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> modelEval(tree[<span class="string">&#x27;left&#x27;</span>], inData)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">            <span class="keyword">return</span> treeForeCast(tree[<span class="string">&#x27;right&#x27;</span>], inData, modelEval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> modelEval(tree[<span class="string">&#x27;right&#x27;</span>], inData)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createForeCast</span>(<span class="params">tree, testData, modelEval=regTreeEval</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 已经训练好的树的模型</span></span><br><span class="line"><span class="string">        inData -- 输入的测试数据</span></span><br><span class="line"><span class="string">        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回预测值矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="built_in">len</span>(testData)</span><br><span class="line">    yHat = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># print yHat</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        yHat[i, <span class="number">0</span>] = treeForeCast(tree, mat(testData[i]), modelEval)</span><br><span class="line">        <span class="comment"># print &quot;yHat==&gt;&quot;, yHat[i, 0]</span></span><br><span class="line">    <span class="keyword">return</span> yHat</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<blockquote>
<p>测试算法: 使用测试数据上的R^2值来分析模型的效果</p>
</blockquote>
<p>R^2 判定系数就是拟合优度判定系数，它体现了回归模型中自变量的变异在因变量的变异中所占的比例。如 R^2=0.99999 表示在因变量 y 的变异中有 99.999% 是由于变量 x 引起。当 R^2=1 时表示，所有观测点都落在拟合的直线或曲线上；当 R^2=0 时，表示自变量与因变量不存在直线或曲线关系。</p>
<p>所以我们看出， R^2 的值越接近 1.0 越好。</p>
<blockquote>
<p>使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</p>
</blockquote>

        <h2 id="5、附加-Python-中-GUI-的使用"   >
          <a href="#5、附加-Python-中-GUI-的使用" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、附加-Python-中-GUI-的使用" class="headerlink" title="5、附加 Python 中 GUI 的使用"></a>5、附加 Python 中 GUI 的使用</h2>
      
        <h3 id="5-1、使用-Python-的-Tkinter-库创建-GUI"   >
          <a href="#5-1、使用-Python-的-Tkinter-库创建-GUI" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-1、使用-Python-的-Tkinter-库创建-GUI" class="headerlink" title="5.1、使用 Python 的 Tkinter 库创建 GUI"></a>5.1、使用 Python 的 Tkinter 库创建 GUI</h3>
      <p>如果能让用户不需要任何指令就可以按照他们自己的方式来分析数据，就不需要对数据做出过多解释。其中一个能同时支持数据呈现和用户交互的方式就是构建一个图形用户界面(GUI，Graphical User Interface)，如图9-7所示。</p>
<p><img src="img/GUI%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="GUI示例图" title="GUI示例图"></p>

        <h3 id="5-2、用-Tkinter-创建-GUI"   >
          <a href="#5-2、用-Tkinter-创建-GUI" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-2、用-Tkinter-创建-GUI" class="headerlink" title="5.2、用 Tkinter 创建 GUI"></a>5.2、用 Tkinter 创建 GUI</h3>
      <p>Python 有很多 GUI 框架，其中一个易于使用的 Tkinter，是随 Python 的标准版编译版本发布的。Tkinter 可以在 Windows、Mac OS和大多数的 Linux 平台上使用。</p>

        <h3 id="5-3、集成-Matplotlib-和-Tkinter"   >
          <a href="#5-3、集成-Matplotlib-和-Tkinter" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-3、集成-Matplotlib-和-Tkinter" class="headerlink" title="5.3、集成 Matplotlib 和 Tkinter"></a>5.3、集成 Matplotlib 和 Tkinter</h3>
      <p>MatPlotlib 的构建程序包含一个前端，也就是面向用户的一些代码，如 plot() 和 scatter() 方法等。事实上，它同时创建了一个后端，用于实现绘图和不同应用之间接口。</p>
<p>通过改变后端可以将图像绘制在PNG、PDF、SVG等格式的文件上。下面将设置后端为 TkAgg (Agg 是一个 C++ 的库，可以从图像创建光栅图)。TkAgg可以在所选GUI框架上调用Agg，把 Agg 呈现在画布上。我们可以在Tk的GUI上放置一个画布，并用 .grid()来调整布局。</p>

        <h3 id="5-4、用treeExplore-的GUI构建的模型树示例图"   >
          <a href="#5-4、用treeExplore-的GUI构建的模型树示例图" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-4、用treeExplore-的GUI构建的模型树示例图" class="headerlink" title="5.4、用treeExplore 的GUI构建的模型树示例图"></a>5.4、用treeExplore 的GUI构建的模型树示例图</h3>
      <p><img src="img/GUI%E6%9B%B4%E5%A5%BD%E7%9A%84%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="取得更好预测效果的GUI示例图" title="取得更好预测效果的GUI示例图"></p>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="6、树回归-小结"   >
          <a href="#6、树回归-小结" class="heading-link"><i class="fas fa-link"></i></a><a href="#6、树回归-小结" class="headerlink" title="6、树回归 小结"></a>6、树回归 小结</h2>
      <p>数据集中经常包含一些复杂的相关关系，使得输入数据和目标变量之间呈现非线性关系。对这些复杂的关系建模，一种可行的方式是使用树来对预测值分段，包括分段常数或分段直线。一般采用树结构来对这种数据建模。相应地，若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型师线性回归方程则称为模型树。</p>
<p>CART 算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART 算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。一棵过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝（在树的构建过程中就进行剪枝）和后剪枝（当树构建完毕再进行剪枝），预剪枝更有效但需要用户定义一些参数。</p>
<p>Tkinter 是 Python 的一个 GUI 工具包。虽然并不是唯一的包，但它最常用。利用 Tkinter ，我们可以轻轻松松绘制各种部件并安排它们的位置。另外，可以为 Tkinter 构造一个特殊的部件来显示 Matplotlib 绘出的图。所以，Matplotlib 和 Tkinter 的集成可以构建出更强大的 GUI ，用户可以以更自然的方式来探索机器学习算法的奥妙。</p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="http://example.com">TainTear</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="http://example.com/2021/08/09/ml_9/">http://example.com/2021/08/09/ml_9/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/10/ml_10/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第10章 K-Means（K-均值）聚类算法</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/08/ml_8/"><span class="paginator-prev__text">第8章 预测数值型数据-回归</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%91%E5%9B%9E%E5%BD%92-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">
          树回归 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%91%E5%9B%9E%E5%BD%92-%E5%9C%BA%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">
          树回归 场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">
          1、树回归 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.</span> <span class="toc-text">
          1.1、树回归 原理概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2%E3%80%81%E6%A0%91%E6%9E%84%E5%BB%BA%E7%AE%97%E6%B3%95-%E6%AF%94%E8%BE%83"><span class="toc-number">3.2.</span> <span class="toc-text">
          1.2、树构建算法 比较</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1%E3%80%81%E9%99%84%E5%8A%A0-%E5%90%84%E5%B8%B8%E8%A7%81%E6%A0%91%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%88%92%E5%88%86%E5%88%86%E6%94%AF%E6%96%B9%E5%BC%8F"><span class="toc-number">3.2.1.</span> <span class="toc-text">
          1.2.1、附加 各常见树构造算法的划分分支方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">3.3.</span> <span class="toc-text">
          1.3、树回归 工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.4.</span> <span class="toc-text">
          1.4、树回归 开发流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E7%AE%97%E6%B3%95%E7%89%B9%E7%82%B9"><span class="toc-number">3.5.</span> <span class="toc-text">
          1.5、树回归 算法特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6%E3%80%81%E5%9B%9E%E5%BD%92%E6%A0%91-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">3.6.</span> <span class="toc-text">
          1.6、回归树 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-1%E3%80%81%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">3.6.1.</span> <span class="toc-text">
          1.6.1、项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-2%E3%80%81%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.6.2.</span> <span class="toc-text">
          1.6.2、开发流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E6%A0%91%E5%89%AA%E6%9E%9D"><span class="toc-number">4.</span> <span class="toc-text">
          2、树剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E3%80%81%E9%A2%84%E5%89%AA%E6%9E%9D-prepruning"><span class="toc-number">4.1.</span> <span class="toc-text">
          2.1、预剪枝(prepruning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2%E3%80%81%E5%90%8E%E5%89%AA%E6%9E%9D-postpruning"><span class="toc-number">4.2.</span> <span class="toc-text">
          2.2、后剪枝(postpruning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3%E3%80%81%E5%89%AA%E6%9E%9D-%E4%BB%A3%E7%A0%81"><span class="toc-number">4.3.</span> <span class="toc-text">
          2.3、剪枝 代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E6%A8%A1%E5%9E%8B%E6%A0%91"><span class="toc-number">5.</span> <span class="toc-text">
          3、模型树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E3%80%81%E6%A8%A1%E5%9E%8B%E6%A0%91-%E7%AE%80%E4%BB%8B"><span class="toc-number">5.1.</span> <span class="toc-text">
          3.1、模型树 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E3%80%81%E6%A8%A1%E5%9E%8B%E6%A0%91-%E4%BB%A3%E7%A0%81"><span class="toc-number">5.2.</span> <span class="toc-text">
          3.2、模型树 代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%E3%80%81%E6%A8%A1%E5%9E%8B%E6%A0%91-%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">5.3.</span> <span class="toc-text">
          3.3、模型树 运行结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">6.</span> <span class="toc-text">
          4、树回归 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E3%80%81%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B1-%E6%A0%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A0%87%E5%87%86%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">6.1.</span> <span class="toc-text">
          4.1、项目案例1: 树回归与标准回归的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1%E3%80%81%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">6.1.1.</span> <span class="toc-text">
          4.1.1、项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2%E3%80%81%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">6.1.2.</span> <span class="toc-text">
          4.1.2、开发流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E9%99%84%E5%8A%A0-Python-%E4%B8%AD-GUI-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">
          5、附加 Python 中 GUI 的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1%E3%80%81%E4%BD%BF%E7%94%A8-Python-%E7%9A%84-Tkinter-%E5%BA%93%E5%88%9B%E5%BB%BA-GUI"><span class="toc-number">7.1.</span> <span class="toc-text">
          5.1、使用 Python 的 Tkinter 库创建 GUI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2%E3%80%81%E7%94%A8-Tkinter-%E5%88%9B%E5%BB%BA-GUI"><span class="toc-number">7.2.</span> <span class="toc-text">
          5.2、用 Tkinter 创建 GUI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3%E3%80%81%E9%9B%86%E6%88%90-Matplotlib-%E5%92%8C-Tkinter"><span class="toc-number">7.3.</span> <span class="toc-text">
          5.3、集成 Matplotlib 和 Tkinter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4%E3%80%81%E7%94%A8treeExplore-%E7%9A%84GUI%E6%9E%84%E5%BB%BA%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE"><span class="toc-number">7.4.</span> <span class="toc-text">
          5.4、用treeExplore 的GUI构建的模型树示例图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%A0%91%E5%9B%9E%E5%BD%92-%E5%B0%8F%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">
          6、树回归 小结</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>