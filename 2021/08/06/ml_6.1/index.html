<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="支持向量机 概述       支持向量机(Support Vector Machines, SVM): 是一种监督学习算法。  支持向量(Support Vector)就是离分隔超平面最近的那些点。 机(Machine)就是表示一种算法，而不是表示机器。                      支持向量机 场景        要给左右两边的点进行分类">
<meta property="og:type" content="article">
<meta property="og:title" content="第6章 支持向量机">
<meta property="og:url" content="http://example.com/2021/08/06/ml_6.1/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:description" content="支持向量机 概述       支持向量机(Support Vector Machines, SVM): 是一种监督学习算法。  支持向量(Support Vector)就是离分隔超平面最近的那些点。 机(Machine)就是表示一种算法，而不是表示机器。                      支持向量机 场景        要给左右两边的点进行分类">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_1.jpg">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_3_linearly-separable.jpg">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/k_2.jpg">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_2_separating-hyperplane.jpg">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_4_point2line-distance.jpg">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_5_Lagrangemultiplier.png">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F.jpg">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F.png">
<meta property="og:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_6_radial-basis-function.jpg">
<meta property="article:published_time" content="2021-08-05T19:07:57.000Z">
<meta property="article:modified_time" content="2021-08-28T19:35:45.821Z">
<meta property="article:author" content="TainTear">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/08/06/ml_6.1/img/SVM_1.jpg"><title>第6章 支持向量机 | TainTear's Blog</title><link ref="canonical" href="http://example.com/2021/08/06/ml_6.1/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第6章 支持向量机</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/SVM_1.jpg" alt="支持向量机_首页"></p>

        <h2 id="支持向量机-概述"   >
          <a href="#支持向量机-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#支持向量机-概述" class="headerlink" title="支持向量机 概述"></a>支持向量机 概述</h2>
      <p>支持向量机(Support Vector Machines, SVM): 是一种监督学习算法。</p>
<ul>
<li>支持向量(Support Vector)就是离分隔超平面最近的那些点。</li>
<li>机(Machine)就是表示一种算法，而不是表示机器。</li>
</ul>

        <h2 id="支持向量机-场景"   >
          <a href="#支持向量机-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#支持向量机-场景" class="headerlink" title="支持向量机 场景"></a>支持向量机 场景</h2>
      <ul>
<li>要给左右两边的点进行分类</li>
<li>明显发现: 选择D会比B、C分隔的效果要好很多。</li>
</ul>
<p><img src="img/SVM_3_linearly-separable.jpg" alt="线性可分"></p>

        <h2 id="支持向量机-原理"   >
          <a href="#支持向量机-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#支持向量机-原理" class="headerlink" title="支持向量机 原理"></a>支持向量机 原理</h2>
      
        <h3 id="SVM-工作原理"   >
          <a href="#SVM-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM-工作原理" class="headerlink" title="SVM 工作原理"></a>SVM 工作原理</h3>
      <p><img src="img/k_2.jpg" alt="k_2" title="k_2"></p>
<p>对于上述的苹果和香蕉，我们想象为2种水果类型的炸弹。（保证距离最近的炸弹，距离它们最远）</p>
<ol>
<li>寻找最大分类间距</li>
<li>转而通过拉格朗日函数求优化的问题</li>
</ol>
<ul>
<li>数据可以通过画一条直线就可以将它们完全分开，这组数据叫<code>线性可分(linearly separable)</code>数据，而这条分隔直线称为<code>分隔超平面(separating hyperplane)</code>。</li>
<li>如果数据集上升到1024维呢？那么需要1023维来分隔数据集，也就说需要N-1维的对象来分隔，这个对象叫做<code>超平面(hyperlane)</code>，也就是分类的决策边界。</li>
</ul>
<p><img src="img/SVM_2_separating-hyperplane.jpg" alt="分隔超平面"></p>

        <h3 id="寻找最大间隔"   >
          <a href="#寻找最大间隔" class="heading-link"><i class="fas fa-link"></i></a><a href="#寻找最大间隔" class="headerlink" title="寻找最大间隔"></a>寻找最大间隔</h3>
      
        <h4 id="为什么寻找最大间隔"   >
          <a href="#为什么寻找最大间隔" class="heading-link"><i class="fas fa-link"></i></a><a href="#为什么寻找最大间隔" class="headerlink" title="为什么寻找最大间隔"></a>为什么寻找最大间隔</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">摘录地址: http://slideplayer.com/slide/8610144  (第12条信息)</span><br><span class="line">Support Vector Machines: Slide 12 Copyright © 2001, 2003, Andrew W. Moore Why Maximum Margin? </span><br><span class="line"></span><br><span class="line">1.Intuitively this feels safest. </span><br><span class="line">2.If we’ve made a small error in the location of the boundary (it’s been jolted in its perpendicular direction) this gives us least chance of causing a misclassification. </span><br><span class="line">3.CV is easy since the model is immune to removal of any non-support-vector datapoints. </span><br><span class="line">4.There’s some theory that this is a good thing. </span><br><span class="line">5.Empirically it works very very well. </span><br><span class="line"></span><br><span class="line">* * *</span><br><span class="line"></span><br><span class="line">1. 直觉上是最安全的</span><br><span class="line">2. 如果我们在边界的位置发生了一个小错误（它在垂直方向上被颠倒），这给我们最小的可能导致错误分类。</span><br><span class="line">3. CV（cross validation 交叉验证）很容易，因为该模型对任何非支持向量数据点的去除是免疫的。</span><br><span class="line">4. 有一些理论表明这是一件好东西。</span><br><span class="line">5. 从经验角度上说它的效果非常非常好。</span><br></pre></td></tr></table></div></figure>


        <h4 id="怎么寻找最大间隔"   >
          <a href="#怎么寻找最大间隔" class="heading-link"><i class="fas fa-link"></i></a><a href="#怎么寻找最大间隔" class="headerlink" title="怎么寻找最大间隔"></a>怎么寻找最大间隔</h4>
      <blockquote>
<p>点到超平面的距离</p>
</blockquote>
<ul>
<li>分隔超平面<code>函数间距</code>:  $$y(x)=w^Tx+b$$</li>
<li>分类的结果:  $$f(x)=sign(w^Tx+b)$$  (sign表示&gt;0为1，&lt;0为-1，=0为0) </li>
<li>点到超平面的<code>几何间距</code>: $$d(x)=(w^Tx+b)/||w||$$  （||w||表示w矩阵的二范数=&gt; $$\sqrt{w^T*w}$$, 点到超平面的距离也是类似的）</li>
</ul>
<p><img src="img/SVM_4_point2line-distance.jpg" alt="点到直线的几何距离"></p>
<blockquote>
<p>拉格朗日乘子法</p>
</blockquote>
<ul>
<li>类别标签用-1、1，是为了后期方便 $$label*(w^Tx+b)$$ 的标识和距离计算；如果 $$label*(w^Tx+b)&gt;0$$ 表示预测正确，否则预测错误。</li>
<li>现在目标很明确，就是要找到<code>w</code>和<code>b</code>，因此我们必须要找到最小间隔的数据点，也就是前面所说的<code>支持向量</code>。<ul>
<li>也就说，让最小的距离取最大.(最小的距离: 就是最小间隔的数据点；最大: 就是最大间距，为了找出最优超平面–最终就是支持向量)</li>
<li>目标函数: $$arg: max_{w, b} \left( min[label*(w^Tx+b)]*\frac{1}{||w||} \right) $$<ol>
<li>如果 $$label*(w^Tx+b)&gt;0$$ 表示预测正确，也称<code>函数间隔</code>，$$||w||$$ 可以理解为归一化，也称<code>几何间隔</code>。</li>
<li>令 $$label*(w^Tx+b)&gt;=1$$， 因为0～1之间，得到的点是存在误判的可能性，所以要保障 $$min[label*(w^Tx+b)]=1$$，才能更好降低噪音数据影响。</li>
<li>所以本质上是求 $$arg: max_{w, b}  \frac{1}{||w||} $$；也就说，我们约束(前提)条件是: $$label*(w^Tx+b)=1$$</li>
</ol>
</li>
</ul>
</li>
<li>新的目标函数求解:  $$arg: max_{w, b}  \frac{1}{||w||} $$<ul>
<li>=&gt; 就是求: $$arg: min_{w, b} ||w|| $$ (求矩阵会比较麻烦，如果x只是 $$\frac{1}{2}*x^2$$ 的偏导数，那么。。同样是求最小值)</li>
<li>=&gt; 就是求: $$arg: min_{w, b} (\frac{1}{2}*||w||^2)$$ (二次函数求导，求极值，平方也方便计算)</li>
<li>本质上就是求线性不等式的二次优化问题(求分隔超平面，等价于求解相应的凸二次规划问题)</li>
</ul>
</li>
<li>通过拉格朗日乘子法，求二次优化问题<ul>
<li>假设需要求极值的目标函数 (objective function) 为 f(x,y)，限制条件为 φ(x,y)=M  # M=1</li>
<li>设g(x,y)=M-φ(x,y)   # 临时φ(x,y)表示下文中 $$label*(w^Tx+b)$$</li>
<li>定义一个新函数: F(x,y,λ)=f(x,y)+λg(x,y)</li>
<li>a为λ（a&gt;=0），代表要引入的拉格朗日乘子(Lagrange multiplier)</li>
<li>那么:  $$L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha_i * [1 - label * (w^Tx+b)]$$</li>
<li>因为: $$label*(w^Tx+b)&gt;=1, \alpha&gt;=0$$ , 所以 $$\alpha*[1-label*(w^Tx+b)]&lt;=0$$ , $$\sum_{i=1}^{n} \alpha_i * [1-label*(w^Tx+b)]&lt;=0$$ </li>
<li>当 $$label*(w^Tx+b)&gt;1$$ 则 $$\alpha=0$$ ，表示该点为<font color=red>非支持向量</font></li>
<li>相当于求解:  $$max_{\alpha} L(w,b,\alpha) = \frac{1}{2} *||w||^2$$ </li>
<li>如果求:  $$min_{w, b} \frac{1}{2} *||w||^2$$ , 也就是要求:  $$min_{w, b} \left( max_{\alpha} L(w,b,\alpha)\right)$$ </li>
</ul>
</li>
<li>现在转化到对偶问题的求解<ul>
<li>$$min_{w, b} \left(max_{\alpha} L(w,b,\alpha) \right) $$ &gt;= $$max_{\alpha} \left(min_{w, b}\ L(w,b,\alpha) \right) $$ </li>
<li>现在分2步</li>
<li>先求:  $$min_{w, b} L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha_i * [1 - label * (w^Tx+b)]$$</li>
<li>就是求<code>L(w,b,a)</code>关于[w, b]的偏导数, 得到<code>w和b的值</code>，并化简为: <code>L和a的方程</code>。</li>
<li>参考:  如果公式推导还是不懂，也可以参考《统计学习方法》李航-P103&lt;学习的对偶算法&gt;<br><img src="img/SVM_5_Lagrangemultiplier.png" alt="计算拉格朗日函数的对偶函数"></li>
</ul>
</li>
<li>终于得到课本上的公式:  $$max_{\alpha} \left( \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i, j=1}^{m} label_i \ast label_j \ast \alpha_i \ast \alpha_j \ast &lt;x_i, x_j&gt; \right) $$</li>
<li>约束条件:  $$a&gt;=0$$ 并且 $$\sum_{i=1}^{m} a_i \ast label_i=0$$</li>
</ul>
<blockquote>
<p>松弛变量(slack variable)</p>
</blockquote>
<p>参考地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://blog.csdn.net/wusecaiyun/article/details/49659183" >http://blog.csdn.net/wusecaiyun/article/details/49659183</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p><img src="img/SVM_%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F.jpg" alt="松弛变量公式"></p>
<ul>
<li>我们知道几乎所有的数据都不那么干净, 通过引入松弛变量来 <code>允许数据点可以处于分隔面错误的一侧</code>。</li>
<li>约束条件:  $$C&gt;=a&gt;=0$$ 并且 $$\sum_{i=1}^{m} a_i \ast label_i=0$$</li>
<li>总的来说: <ul>
<li><img src="img/%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F.png" alt="松弛变量"> 表示 <code>松弛变量</code></li>
<li>常量C是 <code>惩罚因子</code>, 表示离群点的权重（用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0” ）<ul>
<li>$$label*(w^Tx+b) &gt; 1$$ and alpha = 0 (在边界外，就是非支持向量)</li>
<li>$$label*(w^Tx+b) = 1$$ and 0&lt; alpha &lt; C (在分割超平面上，就支持向量)</li>
<li>$$label*(w^Tx+b) &lt; 1$$ and alpha = C (在分割超平面内，是误差点 -&gt; C表示它该受到的惩罚因子程度)</li>
<li>参考地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/question/48351234/answer/110486455" >https://www.zhihu.com/question/48351234/answer/110486455</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</li>
<li>C值越大，表示离群点影响越大，就越容易过度拟合；反之有可能欠拟合。</li>
<li>我们看到，目标函数控制了离群点的数目和程度，使大部分样本点仍然遵守限制条件。</li>
<li>例如: 正类有10000个样本，而负类只给了100个（C越大表示100个负样本的影响越大，就会出现过度拟合，所以C决定了负样本对模型拟合程度的影响！，C就是一个非常关键的优化点！）</li>
</ul>
</li>
<li>这一结论十分直接，SVM中的主要工作就是要求解 alpha.</li>
</ul>

        <h3 id="SMO-高效优化算法"   >
          <a href="#SMO-高效优化算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#SMO-高效优化算法" class="headerlink" title="SMO 高效优化算法"></a>SMO 高效优化算法</h3>
      <ul>
<li>SVM有很多种实现，最流行的一种实现是:  <code>序列最小优化(Sequential Minimal Optimization, SMO)算法</code>。</li>
<li>下面还会介绍一种称为 <code>核函数(kernel)</code> 的方式将SVM扩展到更多数据集上。</li>
<li>注意: <code>SVM几何含义比较直观，但其算法实现较复杂，牵扯大量数学公式的推导。</code></li>
</ul>
<blockquote>
<p>序列最小优化(Sequential Minimal Optimization, SMO)</p>
</blockquote>
<ul>
<li>创建作者: John Platt</li>
<li>创建时间: 1996年</li>
<li>SMO用途: 用于训练 SVM</li>
<li>SMO目标: 求出一系列 alpha 和 b,一旦求出 alpha，就很容易计算出权重向量 w 并得到分隔超平面。</li>
<li>SMO思想: 是将大优化问题分解为多个小优化问题来求解的。</li>
<li>SMO原理: 每次循环选择两个 alpha 进行优化处理，一旦找出一对合适的 alpha，那么就增大一个同时减少一个。<ul>
<li>这里指的合适必须要符合一定的条件<ol>
<li>这两个 alpha 必须要在间隔边界之外</li>
<li>这两个 alpha 还没有进行过区间化处理或者不在边界上。</li>
</ol>
</li>
<li>之所以要同时改变2个 alpha；原因是我们有一个约束条件:  $$\sum_{i=1}^{m} a_i \ast label_i=0$$；如果只是修改一个 alpha，很可能导致约束条件失效。</li>
</ul>
</li>
</ul>
<blockquote>
<p>SMO 伪代码大致如下: </p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">创建一个 alpha 向量并将其初始化为0向量</span><br><span class="line">当迭代次数小于最大迭代次数时(外循环)</span><br><span class="line">    对数据集中的每个数据向量(内循环): </span><br><span class="line">        如果该数据向量可以被优化</span><br><span class="line">            随机选择另外一个数据向量</span><br><span class="line">            同时优化这两个向量</span><br><span class="line">            如果两个向量都不能被优化，退出内循环</span><br><span class="line">    如果所有向量都没被优化，增加迭代数目，继续下一次循环</span><br></pre></td></tr></table></div></figure>


        <h3 id="SVM-开发流程"   >
          <a href="#SVM-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM-开发流程" class="headerlink" title="SVM 开发流程"></a>SVM 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任意方法。</span><br><span class="line">准备数据: 需要数值型数据。</span><br><span class="line">分析数据: 有助于可视化分隔超平面。</span><br><span class="line">训练算法: SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。</span><br><span class="line">测试算法: 十分简单的计算过程就可以实现。</span><br><span class="line">使用算法: 几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。</span><br></pre></td></tr></table></div></figure>


        <h3 id="SVM-算法特点"   >
          <a href="#SVM-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM-算法特点" class="headerlink" title="SVM 算法特点"></a>SVM 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 泛化（由具体的、个别的扩大为一般的，就是说: 模型训练完后的新样本）错误率低，计算开销不大，结果易理解。</span><br><span class="line">缺点: 对参数调节和核函数的选择敏感，原始分类器不加修改仅适合于处理二分类问题。</span><br><span class="line">使用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="课本案例（无核函数）"   >
          <a href="#课本案例（无核函数）" class="heading-link"><i class="fas fa-link"></i></a><a href="#课本案例（无核函数）" class="headerlink" title="课本案例（无核函数）"></a>课本案例（无核函数）</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>对小规模数据点进行分类</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <blockquote>
<p>收集数据</p>
</blockquote>
<p>文本文件格式: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.542485</span>	<span class="number">1.977398</span>	-<span class="number">1</span></span><br><span class="line"><span class="number">3.018896</span>	<span class="number">2.556416</span>	-<span class="number">1</span></span><br><span class="line"><span class="number">7.551510</span>	-<span class="number">1.580030</span>	<span class="number">1</span></span><br><span class="line"><span class="number">2.114999</span>	-<span class="number">0.004466</span>	-<span class="number">1</span></span><br><span class="line"><span class="number">8.127113</span>	<span class="number">1.274372</span>	<span class="number">1</span></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对文件进行逐行解析，从而得到第行的类标签和整个特征矩阵</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        fileName 文件名</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataMat  特征矩阵</span></span><br><span class="line"><span class="string">        labelMat 类标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 无</p>
</blockquote>
<blockquote>
<p>训练算法</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoSimple</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;smoSimple</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMatIn    特征集合</span></span><br><span class="line"><span class="string">        classLabels  类别标签</span></span><br><span class="line"><span class="string">        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。</span></span><br><span class="line"><span class="string">            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。</span></span><br><span class="line"><span class="string">            可以通过调节该参数达到不同的结果。</span></span><br><span class="line"><span class="string">        toler   容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）</span></span><br><span class="line"><span class="string">        maxIter 退出前最大的循环次数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        b       模型的常量值</span></span><br><span class="line"><span class="string">        alphas  拉格朗日乘子</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)</span><br><span class="line">    <span class="comment"># 矩阵转置 和 .T 一样的功能</span></span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 b和alphas(alpha有点类似权重值。)</span></span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    alphas = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 没有任何alpha改变的情况下遍历数据的次数</span></span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">iter</span> &lt; maxIter):</span><br><span class="line">        <span class="comment"># w = calcWs(alphas, dataMatIn, classLabels)</span></span><br><span class="line">        <span class="comment"># print(&quot;w:&quot;, w)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录alpha是否已经进行优化，每次循环时设为0，然后再对整个集合顺序遍历</span></span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="comment"># print &#x27;alphas=&#x27;, alphas</span></span><br><span class="line">            <span class="comment"># print &#x27;labelMat=&#x27;, labelMat</span></span><br><span class="line">            <span class="comment"># print &#x27;multiply(alphas, labelMat)=&#x27;, multiply(alphas, labelMat)</span></span><br><span class="line">            <span class="comment"># 我们预测的类别 y[i] = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*label[n]*x[n]</span></span><br><span class="line">            fXi = <span class="built_in">float</span>(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b</span><br><span class="line">            <span class="comment"># 预测结果与真实结果比对，计算误差Ei</span></span><br><span class="line">            Ei = fXi - <span class="built_in">float</span>(labelMat[i])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值)</span></span><br><span class="line">            <span class="comment"># 0&lt;=alphas[i]&lt;=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。</span></span><br><span class="line">            <span class="comment"># 表示发生错误的概率: labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            # 检验训练样本(xi, yi)是否满足KKT条件</span></span><br><span class="line"><span class="string">            yi*f(i) &gt;= 1 and alpha = 0 (outside the boundary)</span></span><br><span class="line"><span class="string">            yi*f(i) == 1 and 0&lt;alpha&lt; C (on the boundary)</span></span><br><span class="line"><span class="string">            yi*f(i) &lt;= 1 and alpha = C (between the boundary)</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> ((labelMat[i]*Ei &lt; -toler) <span class="keyword">and</span> (alphas[i] &lt; C)) <span class="keyword">or</span> ((labelMat[i]*Ei &gt; toler) <span class="keyword">and</span> (alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果满足优化的条件，我们就随机选取非i的一个点，进行优化比较</span></span><br><span class="line">                j = selectJrand(i, m)</span><br><span class="line">                <span class="comment"># 预测j的结果</span></span><br><span class="line">                fXj = <span class="built_in">float</span>(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b</span><br><span class="line">                Ej = fXj - <span class="built_in">float</span>(labelMat[j])</span><br><span class="line">                alphaIold = alphas[i].copy()</span><br><span class="line">                alphaJold = alphas[j].copy()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句</span></span><br><span class="line">                <span class="comment"># labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。</span></span><br><span class="line">                <span class="keyword">if</span> (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] - alphas[i])</span><br><span class="line">                    H = <span class="built_in">min</span>(C, C + alphas[j] - alphas[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = <span class="built_in">min</span>(C, alphas[j] + alphas[i])</span><br><span class="line">                <span class="comment"># 如果相同，就没法优化了</span></span><br><span class="line">                <span class="keyword">if</span> L == H:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;L==H&quot;</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程</span></span><br><span class="line">                <span class="comment"># 参考《统计学习方法》李航-P125~P128&lt;序列最小最优化算法&gt;</span></span><br><span class="line">                eta = <span class="number">2.0</span> * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T</span><br><span class="line">                <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;eta&gt;=0&quot;</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 计算出一个新的alphas[j]值</span></span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                <span class="comment"># 并使用辅助函数，以及L和H对其进行调整</span></span><br><span class="line">                alphas[j] = clipAlpha(alphas[j], H, L)</span><br><span class="line">                <span class="comment"># 检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">abs</span>(alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>):</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;j not moving enough&quot;</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反</span></span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])</span><br><span class="line">                <span class="comment"># 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。</span></span><br><span class="line">                <span class="comment"># w= Σ[1~n] ai*yi*xi =&gt; b = yj- Σ[1~n] ai*yi(xi*xj)</span></span><br><span class="line">                <span class="comment"># 所以:   b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)</span></span><br><span class="line">                <span class="comment"># 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍</span></span><br><span class="line">                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T</span><br><span class="line">                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T</span><br><span class="line">                <span class="keyword">if</span> (<span class="number">0</span> &lt; alphas[i]) <span class="keyword">and</span> (C &gt; alphas[i]):</span><br><span class="line">                    b = b1</span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">0</span> &lt; alphas[j]) <span class="keyword">and</span> (C &gt; alphas[j]):</span><br><span class="line">                    b = b2</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">                alphaPairsChanged += <span class="number">1</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;iter: %d i:%d, pairs changed %d&quot;</span> % (<span class="built_in">iter</span>, i, alphaPairsChanged))</span><br><span class="line">        <span class="comment"># 在for循环外，检查alpha值是否做了更新，如果更新则将iter设为0后继续运行程序</span></span><br><span class="line">        <span class="comment"># 直到更新完毕后，iter次循环无变化，才退出循环。</span></span><br><span class="line">        <span class="keyword">if</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;iteration number: %d&quot;</span> % <span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">return</span> b, alphas</span><br></pre></td></tr></table></div></figure>

<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-simple.py" >完整代码地址: SVM简化版，应用简化版SMO算法处理小规模数据集</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-simple.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-simple.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py" >完整代码地址: SVM完整版，使用完整 Platt SMO算法加速优化，优化点: 选择alpha的方式不同</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="核函数-kernel-使用"   >
          <a href="#核函数-kernel-使用" class="heading-link"><i class="fas fa-link"></i></a><a href="#核函数-kernel-使用" class="headerlink" title="核函数(kernel) 使用"></a>核函数(kernel) 使用</h2>
      <ul>
<li>对于线性可分的情况，效果明显</li>
<li>对于非线性的情况也一样，此时需要用到一种叫<code>核函数(kernel)</code>的工具将数据转化为分类器易于理解的形式。</li>
</ul>
<blockquote>
<p>利用核函数将数据映射到高维空间</p>
</blockquote>
<ul>
<li>使用核函数: 可以将数据从某个特征空间到另一个特征空间的映射。（通常情况下: 这种映射会将低维特征空间映射到高维空间。）</li>
<li>如果觉得特征空间很装逼、很难理解。</li>
<li>可以把核函数想象成一个包装器(wrapper)或者是接口(interface)，它能将数据从某个很难处理的形式转换成为另一个较容易处理的形式。</li>
<li>经过空间转换后: 低维需要解决的非线性问题，就变成了高维需要解决的线性问题。</li>
<li>SVM 优化特别好的地方，在于所有的运算都可以写成内积(inner product: 是指2个向量相乘，得到单个标量 或者 数值)；内积替换成核函数的方式被称为<code>核技巧(kernel trick)</code>或者<code>核&quot;变电&quot;(kernel substation)</code></li>
<li>核函数并不仅仅应用于支持向量机，很多其他的机器学习算法也都用到核函数。最流行的核函数: 径向基函数(radial basis function)</li>
<li>径向基函数的高斯版本，其具体的公式为: </li>
</ul>
<p><img src="img/SVM_6_radial-basis-function.jpg" alt="径向基函数的高斯版本"></p>

        <h3 id="项目案例-手写数字识别的优化（有核函数）"   >
          <a href="#项目案例-手写数字识别的优化（有核函数）" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-手写数字识别的优化（有核函数）" class="headerlink" title="项目案例: 手写数字识别的优化（有核函数）"></a>项目案例: 手写数字识别的优化（有核函数）</h3>
      
        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">你的老板要求: 你写的那个手写识别程序非常好，但是它占用内存太大。顾客无法通过无线的方式下载我们的应用。</span><br><span class="line">所以: 我们可以考虑使用支持向量机，保留支持向量就行（knn需要保留所有的向量），就可以获得非常好的效果。</span><br></pre></td></tr></table></div></figure>


        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <blockquote>
<p>收集数据: 提供的文本文件</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">00000000000000001111000000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000011111111100000000</span><br><span class="line">00000000000000011111111110000000</span><br><span class="line">00000000000000011111111110000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000001111111111100000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000000111111111000000000</span><br><span class="line">00000000000001111111111000000000</span><br><span class="line">00000000000011111111111000000000</span><br><span class="line">00000000000111111111110000000000</span><br><span class="line">00000000001111111111111000000000</span><br><span class="line">00000001111111111111111000000000</span><br><span class="line">00000011111111111111110000000000</span><br><span class="line">00000111111111111111110000000000</span><br><span class="line">00000111111111111111110000000000</span><br><span class="line">00000001111111111111110000000000</span><br><span class="line">00000001111111011111110000000000</span><br><span class="line">00000000111100011111110000000000</span><br><span class="line">00000000000000011111110000000000</span><br><span class="line">00000000000000011111100000000000</span><br><span class="line">00000000000000111111110000000000</span><br><span class="line">00000000000000011111110000000000</span><br><span class="line">00000000000000011111110000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000000111111110000000</span><br><span class="line">00000000000000000111111100000000</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 基于二值图像构造向量</p>
</blockquote>
<p><code>将 32*32的文本转化为 1*1024的矩阵</code></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span>(<span class="params">filename</span>):</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line">    fr = <span class="built_in">open</span>(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>, <span class="number">32</span> * i + j] = <span class="built_in">int</span>(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadImages</span>(<span class="params">dirName</span>):</span></span><br><span class="line">    <span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line">    hwLabels = []</span><br><span class="line">    <span class="built_in">print</span>(dirName)</span><br><span class="line">    trainingFileList = listdir(dirName)  <span class="comment"># load the training set</span></span><br><span class="line">    m = <span class="built_in">len</span>(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = <span class="built_in">int</span>(fileStr.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> classNumStr == <span class="number">9</span>:</span><br><span class="line">            hwLabels.append(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hwLabels.append(<span class="number">1</span>)</span><br><span class="line">        trainingMat[i, :] = img2vector(<span class="string">&#x27;%s/%s&#x27;</span> % (dirName, fileNameStr))</span><br><span class="line">    <span class="keyword">return</span> trainingMat, hwLabels</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 对图像向量进行目测</p>
</blockquote>
<blockquote>
<p>训练算法: 采用两种不同的核函数，并对径向基核函数采用不同的设置来运行SMO算法</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTrans</span>(<span class="params">X, A, kTup</span>):</span>  <span class="comment"># calc the kernel or transform data to a higher dimensional space</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    核转换函数</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X     dataMatIn数据集</span></span><br><span class="line"><span class="string">        A     dataMatIn数据集的第i行的数据</span></span><br><span class="line"><span class="string">        kTup  核函数的信息</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    K = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">if</span> kTup[<span class="number">0</span>] == <span class="string">&#x27;lin&#x27;</span>:</span><br><span class="line">        <span class="comment"># linear kernel:   m*n * n*1 = m*1</span></span><br><span class="line">        K = X * A.T</span><br><span class="line">    <span class="keyword">elif</span> kTup[<span class="number">0</span>] == <span class="string">&#x27;rbf&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            deltaRow = X[j, :] - A</span><br><span class="line">            K[j] = deltaRow * deltaRow.T</span><br><span class="line">        <span class="comment"># 径向基函数的高斯版本</span></span><br><span class="line">        K = exp(K / (-<span class="number">1</span> * kTup[<span class="number">1</span>] ** <span class="number">2</span>))  <span class="comment"># divide in NumPy is element-wise not matrix like Matlab</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">&#x27;Houston We Have a Problem -- That Kernel is not recognized&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoP</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter, kTup=(<span class="params"><span class="string">&#x27;lin&#x27;</span>, <span class="number">0</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    完整SMO算法外循环，与smoSimple有些类似，但这里的循环退出条件更多一些</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMatIn    数据集</span></span><br><span class="line"><span class="string">        classLabels  类别标签</span></span><br><span class="line"><span class="string">        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。</span></span><br><span class="line"><span class="string">            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。</span></span><br><span class="line"><span class="string">            可以通过调节该参数达到不同的结果。</span></span><br><span class="line"><span class="string">        toler   容错率</span></span><br><span class="line"><span class="string">        maxIter 退出前最大的循环次数</span></span><br><span class="line"><span class="string">        kTup    包含核函数信息的元组</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        b       模型的常量值</span></span><br><span class="line"><span class="string">        alphas  拉格朗日乘子</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个 optStruct 对象</span></span><br><span class="line">    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)</span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    entireSet = <span class="literal">True</span></span><br><span class="line">    alphaPairsChanged = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环遍历: 循环maxIter次 并且 （alphaPairsChanged存在可以改变 or 所有行遍历一遍）</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">iter</span> &lt; maxIter) <span class="keyword">and</span> ((alphaPairsChanged &gt; <span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  当entireSet=true or 非边界alpha对没有了；就开始寻找 alpha对，然后决定是否要进行else。</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            <span class="comment"># 在数据集上遍历所有可能的alpha</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(oS.m):</span><br><span class="line">                <span class="comment"># 是否存在alpha对，存在就+1</span></span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="comment"># print(&quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span></span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对已存在 alpha对，选出非边界的alpha值，进行优化。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 遍历所有的非边界alpha值，也就是不在边界0或C上的值。</span></span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; <span class="number">0</span>) * (oS.alphas.A &lt; C))[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="comment"># print(&quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span></span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            entireSet = <span class="literal">False</span>  <span class="comment"># toggle entire set loop</span></span><br><span class="line">        <span class="keyword">elif</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            entireSet = <span class="literal">True</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;iteration number: %d&quot;</span> % <span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">return</span> oS.b, oS.alphas</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 便携一个函数来测试不同的和函数并计算错误率</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testDigits</span>(<span class="params">kTup=(<span class="params"><span class="string">&#x27;rbf&#x27;</span>, <span class="number">10</span></span>)</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 导入训练数据</span></span><br><span class="line">    dataArr, labelArr = loadImages(<span class="string">&#x27;data/6.SVM/trainingDigits&#x27;</span>)</span><br><span class="line">    b, alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">10000</span>, kTup)</span><br><span class="line">    datMat = mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd = nonzero(alphas.A &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    sVs = datMat[svInd]</span><br><span class="line">    labelSV = labelMat[svInd]</span><br><span class="line">    <span class="comment"># print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])</span></span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)</span><br><span class="line">        <span class="comment"># 1*m * m*1 = 1*1 单个预测结果</span></span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]): errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the training error rate is: %f&quot;</span> % (<span class="built_in">float</span>(errorCount) / m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 导入测试数据</span></span><br><span class="line">    dataArr, labelArr = loadImages(<span class="string">&#x27;data/6.SVM/testDigits&#x27;</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    datMat = mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)</span><br><span class="line">        <span class="comment"># 1*m * m*1 = 1*1 单个预测结果</span></span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]): errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the test error rate is: %f&quot;</span> % (<span class="built_in">float</span>(errorCount) / m))</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 一个图像识别的完整应用还需要一些图像处理的知识，这里并不打算深入介绍</p>
</blockquote>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~houfachao" >geekidentity</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="http://example.com">TainTear</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="http://example.com/2021/08/06/ml_6.1/">http://example.com/2021/08/06/ml_6.1/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/06/ml_6.2/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第6章 SVM</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/05/ml_5/"><span class="paginator-prev__text">第5章 Logistic回归</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">
          支持向量机 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-%E5%9C%BA%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">
          支持向量机 场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">
          支持向量机 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">
          SVM 工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BB%E6%89%BE%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94"><span class="toc-number">3.2.</span> <span class="toc-text">
          寻找最大间隔</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%BB%E6%89%BE%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94"><span class="toc-number">3.2.1.</span> <span class="toc-text">
          为什么寻找最大间隔</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E5%AF%BB%E6%89%BE%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94"><span class="toc-number">3.2.2.</span> <span class="toc-text">
          怎么寻找最大间隔</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SMO-%E9%AB%98%E6%95%88%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">
          SMO 高效优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.4.</span> <span class="toc-text">
          SVM 开发流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-%E7%AE%97%E6%B3%95%E7%89%B9%E7%82%B9"><span class="toc-number">3.5.</span> <span class="toc-text">
          SVM 算法特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BE%E6%9C%AC%E6%A1%88%E4%BE%8B%EF%BC%88%E6%97%A0%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-number">3.6.</span> <span class="toc-text">
          课本案例（无核函数）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">3.6.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">3.6.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0-kernel-%E4%BD%BF%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">
          核函数(kernel) 使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E7%9A%84%E4%BC%98%E5%8C%96%EF%BC%88%E6%9C%89%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">
          项目案例: 手写数字识别的优化（有核函数）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0-1"><span class="toc-number">4.1.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B-1"><span class="toc-number">4.1.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>