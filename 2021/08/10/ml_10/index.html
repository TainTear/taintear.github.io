<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="聚类       聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，">
<meta property="og:type" content="article">
<meta property="og:title" content="第10章 K-Means（K-均值）聚类算法">
<meta property="og:url" content="http://example.com/2021/08/10/ml_10/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:description" content="聚类       聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/K-Means_%E9%A6%96%E9%A1%B5.jpg">
<meta property="og:image" content="http://example.com/img/apachecn-k-means-term-1.jpg">
<meta property="og:image" content="http://example.com/img/apachecn-k-means-run-result-1.jpg">
<meta property="og:image" content="http://example.com/img/apachecn-kmeans-partial-best-result-1.jpg">
<meta property="og:image" content="http://example.com/img/apachecn-bikmeans-run-result-1.jpg">
<meta property="article:published_time" content="2021-08-09T19:07:57.000Z">
<meta property="article:modified_time" content="2021-08-29T06:24:14.506Z">
<meta property="article:author" content="TainTear">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/K-Means_%E9%A6%96%E9%A1%B5.jpg"><title>第10章 K-Means（K-均值）聚类算法 | TainTear's Blog</title><link ref="canonical" href="http://example.com/2021/08/10/ml_10/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第10章 K-Means（K-均值）聚类算法</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-10</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><p><img src="/img/K-Means_%E9%A6%96%E9%A1%B5.jpg" alt="K-Means（K-均值）聚类算法_首页"></p>

        <h2 id="聚类"   >
          <a href="#聚类" class="heading-link"><i class="fas fa-link"></i></a><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2>
      <p>聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，你对你的目标是未知的，同样以动物为例，对于一个动物集来说，你并不清楚这个数据集内部有多少种类的动物，你能做的只是利用聚类方法将它自动按照特征分为多类，然后人为给出这个聚类结果的定义（即簇识别）。例如，你将一个动物集分为了三簇（类），然后通过观察这三类动物的特征，你为每一个簇起一个名字，如大象、狗、猫等，这就是聚类的基本思想。     </p>
<p>至于“相似”这一概念，是利用距离这个评价标准来衡量的，我们通过计算对象与对象之间的距离远近来判断它们是否属于同一类别，即是否是同一个簇。至于距离如何计算，科学家们提出了许多种距离的计算方法，其中欧式距离是最为简单和常用的，除此之外还有曼哈顿距离和余弦相似性距离等。</p>
<p>欧式距离，我想大家再熟悉不过了，但为免有一些基础薄弱的同学，在此再说明一下，它的定义为:<br>对于x点坐标为(x1,x2,x3,…,xn)和 y点坐标为(y1,y2,y3,…,yn)，两者的欧式距离为:</p>
<p>$$<br>d(x,y)<br>    ={\sqrt{<br>            (x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2} + \cdots +(x_{n}-y_{n})^{2}<br>        }}<br>    ={\sqrt{<br>            \sum_{ {i=1} }^{n}(x_{i}-y_{i})^{2}<br>        }}<br>$$</p>
<p>在二维平面，它就是我们初中时就学过的两点距离公式</p>

        <h2 id="K-Means-算法"   >
          <a href="#K-Means-算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-算法" class="headerlink" title="K-Means 算法"></a>K-Means 算法</h2>
      <p>K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 <code>K-均值</code> 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.<br>簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.<br>聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.  </p>
<p><strong>优点</strong>:</p>
<ul>
<li>属于无监督学习，无须准备训练集</li>
<li>原理简单，实现起来较为容易</li>
<li>结果可解释性较好</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>需手动设置k值</strong>。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性</li>
<li>可能收敛到局部最小值, 在大规模数据集上收敛较慢</li>
<li>对于异常点、离群点敏感</li>
</ul>
<p>使用数据类型 : 数值型数据</p>

        <h3 id="K-Means-场景"   >
          <a href="#K-Means-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-场景" class="headerlink" title="K-Means 场景"></a>K-Means 场景</h3>
      <p>kmeans，如前所述，用于数据集内种类属性不明晰，希望能够通过数据挖掘出或自动归类出有相似特点的对象的场景。其商业界的应用场景一般为挖掘出具有相似特点的潜在客户群体以便公司能够重点研究、对症下药。  </p>
<p>例如，在2000年和2004年的美国总统大选中，候选人的得票数比较接近或者说非常接近。任一候选人得到的普选票数的最大百分比为50.7%而最小百分比为47.9% 如果1%的选民将手中的选票投向另外的候选人，那么选举结果就会截然不同。 实际上，如果妥善加以引导与吸引，少部分选民就会转换立场。尽管这类选举者占的比例较低，但当候选人的选票接近时，这些人的立场无疑会对选举结果产生非常大的影响。如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们？ 答案就是聚类（Clustering)。</p>
<p>那么，具体如何实施呢？首先，收集用户的信息，可以同时收集用户满意或不满意的信息，这是因为任何对用户重要的内容都可能影响用户的投票结果。然后，将这些信息输入到某个聚类算法中。接着，对聚类结果中的每一个簇（最好选择最大簇 ）， 精心构造能够吸引该簇选民的消息。最后， 开展竞选活动并观察上述做法是否有效。</p>
<p>另一个例子就是产品部门的市场调研了。为了更好的了解自己的用户，产品部门可以采用聚类的方法得到不同特征的用户群体，然后针对不同的用户群体可以对症下药，为他们提供更加精准有效的服务。</p>

        <h3 id="K-Means-术语"   >
          <a href="#K-Means-术语" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-术语" class="headerlink" title="K-Means 术语"></a>K-Means 术语</h3>
      <ul>
<li>簇: 所有数据的点集合，簇中的对象是相似的。</li>
<li>质心: 簇中所有点的中心（计算所有点的均值而来）.</li>
<li>SSE: Sum of Sqared Error（误差平方和）, 它被用来评估模型的好坏，SSE 值越小，表示越接近它们的质心. 聚类效果越好。由于对误差取了平方，因此更加注重那些远离中心的点（一般为边界点或离群点）。详情见kmeans的评价标准。</li>
</ul>
<p>有关 <code>簇</code> 和 <code>质心</code> 术语更形象的介绍, 请参考下图:</p>
<p><img src="/img/apachecn-k-means-term-1.jpg" alt="K-Means 术语图"></p>

        <h3 id="K-Means-工作流程"   >
          <a href="#K-Means-工作流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-工作流程" class="headerlink" title="K-Means 工作流程"></a>K-Means 工作流程</h3>
      <ol>
<li>首先, 随机确定 K 个初始点作为质心（<strong>不必是数据中的点</strong>）。</li>
<li>然后将数据集中的每个点分配到一个簇中, 具体来讲, 就是为每个点找到距其最近的质心, 并将其分配该质心所对应的簇. 这一步完成之后, 每个簇的质心更新为该簇所有点的平均值.</li>
<li>重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束。</li>
</ol>
<p>上述过程的 <code>伪代码</code> 如下:</p>
<ul>
<li>创建 k 个点作为起始质心（通常是随机选择）</li>
<li>当任意一个点的簇分配结果发生改变时（不改变时算法结束）<ul>
<li>对数据集中的每个数据点<ul>
<li>对每个质心<ul>
<li>计算质心与数据点之间的距离</li>
</ul>
</li>
<li>将数据点分配到距其最近的簇</li>
</ul>
</li>
<li>对每一个簇, 计算簇中所有点的均值并将均值作为质心</li>
</ul>
</li>
</ul>

        <h3 id="K-Means-开发流程"   >
          <a href="#K-Means-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-开发流程" class="headerlink" title="K-Means 开发流程"></a>K-Means 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 使用任意方法</span><br><span class="line">准备数据: 需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算</span><br><span class="line">分析数据: 使用任意方法</span><br><span class="line">训练算法: 不适用于无监督学习，即无监督学习不需要训练步骤</span><br><span class="line">测试算法: 应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.</span><br><span class="line">使用算法: 可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.</span><br></pre></td></tr></table></div></figure>

        <h3 id="K-Means-的评价标准"   >
          <a href="#K-Means-的评价标准" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-的评价标准" class="headerlink" title="K-Means 的评价标准"></a>K-Means 的评价标准</h3>
      <p>k-means算法因为手动选取k值和初始化随机质心的缘故，每一次的结果不会完全一样，而且由于手动选取k值，我们需要知道我们选取的k值是否合理，聚类效果好不好，那么如何来评价某一次的聚类效果呢？也许将它们画在图上直接观察是最好的办法，但现实是，我们的数据不会仅仅只有两个特征，一般来说都有十几个特征，而观察十几维的空间对我们来说是一个无法完成的任务。因此，我们需要一个公式来帮助我们判断聚类的性能，这个公式就是<strong>SSE</strong> (Sum of Squared Error, 误差平方和 ），它其实就是每一个点到其簇内质心的距离的平方值的总和，这个数值对应kmeans函数中<strong>clusterAssment</strong>矩阵的第一列之和。 <strong>SSE</strong>值越小表示数据点越接近于它们的质心，聚类效果也越好。 因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低<strong>SSE</strong>值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。</p>

        <h3 id="K-Means-聚类算法函数"   >
          <a href="#K-Means-聚类算法函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-聚类算法函数" class="headerlink" title="K-Means 聚类算法函数"></a>K-Means 聚类算法函数</h3>
      
        <h4 id="从文件加载数据集"   >
          <a href="#从文件加载数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#从文件加载数据集" class="headerlink" title="从文件加载数据集"></a>从文件加载数据集</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文本中构建矩阵，加载文本文件，然后处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span>    <span class="comment"># 通用函数，用来解析以 tab 键分隔的 floats（浮点数），例如: 1.658985	4.285136</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        fltLine = <span class="built_in">map</span>(<span class="built_in">float</span>,curLine)    <span class="comment"># 映射所有的元素为 float（浮点数）类型</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br></pre></td></tr></table></div></figure>


        <h4 id="计算两个向量的欧氏距离"   >
          <a href="#计算两个向量的欧氏距离" class="heading-link"><i class="fas fa-link"></i></a><a href="#计算两个向量的欧氏距离" class="headerlink" title="计算两个向量的欧氏距离"></a>计算两个向量的欧氏距离</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个向量的欧式距离（可根据场景选择其他距离公式）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span>(<span class="params">vecA, vecB</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(<span class="built_in">sum</span>(power(vecA - vecB, <span class="number">2</span>))) <span class="comment"># la.norm(vecA-vecB)</span></span><br></pre></td></tr></table></div></figure>


        <h4 id="构建一个包含-K-个随机质心的集合"   >
          <a href="#构建一个包含-K-个随机质心的集合" class="heading-link"><i class="fas fa-link"></i></a><a href="#构建一个包含-K-个随机质心的集合" class="headerlink" title="构建一个包含 K 个随机质心的集合"></a>构建一个包含 K 个随机质心的集合</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为给定数据集构建一个包含 k 个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成。然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span>(<span class="params">dataSet, k</span>):</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>] <span class="comment"># 列的数量，即数据的特征个数</span></span><br><span class="line">    centroids = mat(zeros((k,n))) <span class="comment"># 创建k个质心矩阵</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n): <span class="comment"># 创建随机簇质心，并且在每一维的边界内</span></span><br><span class="line">        minJ = <span class="built_in">min</span>(dataSet[:,j])    <span class="comment"># 最小值</span></span><br><span class="line">        rangeJ = <span class="built_in">float</span>(<span class="built_in">max</span>(dataSet[:,j]) - minJ)    <span class="comment"># 范围 = 最大值 - 最小值</span></span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))    <span class="comment"># 随机生成，mat为numpy函数，需要在最开始写上 from numpy import *</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></div></figure>


        <h4 id="K-Means-聚类算法"   >
          <a href="#K-Means-聚类算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-聚类算法" class="headerlink" title="K-Means 聚类算法"></a>K-Means 聚类算法</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># k-means 聚类算法</span></span><br><span class="line"><span class="comment"># 该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。</span></span><br><span class="line"><span class="comment"># 这个过程重复数次，直到数据点的簇分配结果不再改变位置。</span></span><br><span class="line"><span class="comment"># 运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span>(<span class="params">dataSet, k, distMeas=distEclud, createCent=randCent</span>):</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]    <span class="comment"># 行数，即数据个数</span></span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))    <span class="comment"># 创建一个与 dataSet 行数一样，但是有两列的矩阵，用来保存簇分配结果</span></span><br><span class="line">    centroids = createCent(dataSet, k)    <span class="comment"># 创建质心，随机k个质心</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):    <span class="comment"># 循环每一个数据点并分配到最近的质心中去</span></span><br><span class="line">            minDist = inf; minIndex = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])    <span class="comment"># 计算数据点到质心的距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:    <span class="comment"># 如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）</span></span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:    <span class="comment"># 簇分配结果改变</span></span><br><span class="line">                clusterChanged = <span class="literal">True</span>    <span class="comment"># 簇改变</span></span><br><span class="line">                clusterAssment[i, :] = minIndex,minDist**<span class="number">2</span>    <span class="comment"># 更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方</span></span><br><span class="line">        <span class="built_in">print</span> centroids</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> <span class="built_in">range</span>(k): <span class="comment"># 更新质心</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A==cent)[<span class="number">0</span>]] <span class="comment"># 获取该簇中的所有点</span></span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) <span class="comment"># 将质心修改为簇中所有点的平均值，mean 就是求平均值的</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></div></figure>


        <h4 id="测试函数"   >
          <a href="#测试函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#测试函数" class="headerlink" title="测试函数"></a>测试函数</h4>
      <ol>
<li>测试一下以上的基础函数是否可以如预期运行, 请看: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>测试一下 kMeans 函数是否可以如预期运行, 请看: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> </li>
</ol>
<p>参考运行结果如下:<br><img src="/img/apachecn-k-means-run-result-1.jpg" alt="K-Means 运行结果1"></p>

        <h3 id="K-Means-聚类算法的缺陷"   >
          <a href="#K-Means-聚类算法的缺陷" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-聚类算法的缺陷" class="headerlink" title="K-Means 聚类算法的缺陷"></a>K-Means 聚类算法的缺陷</h3>
      <blockquote>
<p>在 kMeans 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）.</p>
</blockquote>
<p>局部最小值的的情况如下:<br><img src="/img/apachecn-kmeans-partial-best-result-1.jpg" alt="K-Means 局部最小值1"><br>出现这个问题有很多原因，可能是k值取的不合适，可能是距离函数不合适，可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。</p>
<p>为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大<strong>SSE</strong>值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法，令k设为2。</p>
<p>为了保持簇总数不变，可以将某两个簇进行合并。从上图中很明显就可以看出，应该将上图下部两个出错的簇质心进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？</p>
<p>有两种可以量化的办法: 合并最近的质心，或者合并两个使得<strong>SSE</strong>增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总<strong>SSE</strong>值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。</p>
<p>因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法.   </p>

        <h3 id="二分-K-Means-聚类算法"   >
          <a href="#二分-K-Means-聚类算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分-K-Means-聚类算法" class="headerlink" title="二分 K-Means 聚类算法"></a>二分 K-Means 聚类算法</h3>
      <p>该算法首先将所有点作为一个簇，然后将该簇一分为二。<br>之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。<br>上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。  </p>

        <h4 id="二分-K-Means-聚类算法伪代码"   >
          <a href="#二分-K-Means-聚类算法伪代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分-K-Means-聚类算法伪代码" class="headerlink" title="二分 K-Means 聚类算法伪代码"></a>二分 K-Means 聚类算法伪代码</h4>
      <ul>
<li>将所有点看成一个簇</li>
<li>当簇数目小于 k 时</li>
<li>对于每一个簇<ul>
<li>计算总误差</li>
<li>在给定的簇上面进行 KMeans 聚类（k=2）</li>
<li>计算将该簇一分为二之后的总误差</li>
</ul>
</li>
<li>选择使得误差最小的那个簇进行划分操作</li>
</ul>
<p>另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。<br>接下来主要介绍该做法的python2代码实现</p>

        <h4 id="二分-K-Means-聚类算法代码"   >
          <a href="#二分-K-Means-聚类算法代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分-K-Means-聚类算法代码" class="headerlink" title="二分 K-Means 聚类算法代码"></a>二分 K-Means 聚类算法代码</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分 KMeans 聚类算法, 基于 kMeans 基础之上的优化，以避免陷入局部最小值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKMeans</span>(<span class="params">dataSet, k, distMeas=distEclud</span>):</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment"># 保存每个数据点的簇分配结果和平方误差</span></span><br><span class="line">    centroid0 = mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>] <span class="comment"># 质心初始化为所有数据点的均值</span></span><br><span class="line">    centList =[centroid0] <span class="comment"># 初始化只有 1 个质心的 list</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m): <span class="comment"># 计算所有数据点到初始质心的距离平方误差</span></span><br><span class="line">        clusterAssment[j,<span class="number">1</span>] = distMeas(mat(centroid0), dataSet[j,:])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">len</span>(centList) &lt; k): <span class="comment"># 当质心数量小于 k 时</span></span><br><span class="line">        lowestSSE = inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(centList)): <span class="comment"># 对每一个质心</span></span><br><span class="line">            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:] <span class="comment"># 获取当前簇 i 下的所有数据点</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas) <span class="comment"># 将当前簇 i 进行二分 kMeans 处理</span></span><br><span class="line">            sseSplit = <span class="built_in">sum</span>(splitClustAss[:,<span class="number">1</span>]) <span class="comment"># 将二分 kMeans 结果中的平方和的距离进行求和</span></span><br><span class="line">            sseNotSplit = <span class="built_in">sum</span>(clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>]) <span class="comment"># 将未参与二分 kMeans 分配结果中的平方和的距离进行求和</span></span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;sseSplit, and notSplit: &quot;</span>,sseSplit,sseNotSplit</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE: <span class="comment"># 总的（未拆分和已拆分）误差和越小，越相似，效果越优化，划分的结果更好（注意: 这里的理解很重要，不明白的地方可以和我们一起讨论）</span></span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        <span class="comment"># 找出最好的簇分配结果    </span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>] = <span class="built_in">len</span>(centList) <span class="comment"># 调用二分 kMeans 的结果，默认簇是 0,1. 当然也可以改成其它的数字</span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>] = bestCentToSplit <span class="comment"># 更新为最佳质心</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;the bestCentToSplit is: &#x27;</span>,bestCentToSplit</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;the len of bestClustAss is: &#x27;</span>, <span class="built_in">len</span>(bestClustAss)</span><br><span class="line">        <span class="comment"># 更新质心列表</span></span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>,:].tolist()[<span class="number">0</span>] <span class="comment"># 更新原质心 list 中的第 i 个质心为使用二分 kMeans 后 bestNewCents 的第一个质心</span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>,:].tolist()[<span class="number">0</span>]) <span class="comment"># 添加 bestNewCents 的第二个质心</span></span><br><span class="line">        clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>],:]= bestClustAss <span class="comment"># 重新分配最好簇下的数据（质心）以及SSE</span></span><br><span class="line">    <span class="keyword">return</span> mat(centList), clusterAssment</span><br></pre></td></tr></table></div></figure>


        <h4 id="测试二分-KMeans-聚类算法"   >
          <a href="#测试二分-KMeans-聚类算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#测试二分-KMeans-聚类算法" class="headerlink" title="测试二分 KMeans 聚类算法"></a>测试二分 KMeans 聚类算法</h4>
      <ul>
<li>测试一下二分 KMeans 聚类算法，请看: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
<p>上述函数可以运行多次，聚类会收敛到全局最小值，而原始的 kMeans() 函数偶尔会陷入局部最小值。<br>运行参考结果如下:<br><img src="/img/apachecn-bikmeans-run-result-1.jpg" alt="二分 K-Means 运行结果1"></p>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuxin" >那伊抹微笑</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>,  <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuzhaoqing" >清都江水郎</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong> </li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="http://example.com">TainTear</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="http://example.com/2021/08/10/ml_10/">http://example.com/2021/08/10/ml_10/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/11/ml_11/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第11章 使用 Apriori 算法进行关联分析</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/09/ml_9/"><span class="paginator-prev__text">第9章 树回归</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">
          聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-Means-%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">
          K-Means 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E5%9C%BA%E6%99%AF"><span class="toc-number">2.1.</span> <span class="toc-text">
          K-Means 场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E6%9C%AF%E8%AF%AD"><span class="toc-number">2.2.</span> <span class="toc-text">
          K-Means 术语</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">
          K-Means 工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.</span> <span class="toc-text">
          K-Means 开发流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E7%9A%84%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86"><span class="toc-number">2.5.</span> <span class="toc-text">
          K-Means 的评价标准</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.</span> <span class="toc-text">
          K-Means 聚类算法函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E6%96%87%E4%BB%B6%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.6.1.</span> <span class="toc-text">
          从文件加载数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E5%90%91%E9%87%8F%E7%9A%84%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB"><span class="toc-number">2.6.2.</span> <span class="toc-text">
          计算两个向量的欧氏距离</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8C%85%E5%90%AB-K-%E4%B8%AA%E9%9A%8F%E6%9C%BA%E8%B4%A8%E5%BF%83%E7%9A%84%E9%9B%86%E5%90%88"><span class="toc-number">2.6.3.</span> <span class="toc-text">
          构建一个包含 K 个随机质心的集合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">2.6.4.</span> <span class="toc-text">
          K-Means 聚类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.5.</span> <span class="toc-text">
          测试函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">2.7.</span> <span class="toc-text">
          K-Means 聚类算法的缺陷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">2.8.</span> <span class="toc-text">
          二分 K-Means 聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E5%88%86-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="toc-number">2.8.1.</span> <span class="toc-text">
          二分 K-Means 聚类算法伪代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E5%88%86-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81"><span class="toc-number">2.8.2.</span> <span class="toc-text">
          二分 K-Means 聚类算法代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%BA%8C%E5%88%86-KMeans-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">2.8.3.</span> <span class="toc-text">
          测试二分 KMeans 聚类算法</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>