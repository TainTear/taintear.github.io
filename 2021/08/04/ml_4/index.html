<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta name="description" content="朴素贝叶斯 概述       贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。                     贝叶斯理论 &amp; 条件概率">
<meta property="og:type" content="article">
<meta property="og:title" content="第4章 基于概率论的分类方法-朴素贝叶斯">
<meta property="og:url" content="http://example.com/2021/08/04/ml_4/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:description" content="朴素贝叶斯 概述       贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。                     贝叶斯理论 &amp; 条件概率">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/NavieBayesian_headPage_xy.png">
<meta property="og:image" content="http://example.com/2021/08/04/ml_4/img/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.png">
<meta property="og:image" content="http://example.com/img/NB_2.png">
<meta property="og:image" content="http://example.com/img/NB_3.png">
<meta property="og:image" content="http://example.com/img/NB_4.png">
<meta property="og:image" content="http://example.com/img/NB_5.png">
<meta property="og:image" content="http://example.com/img/NB_6.png">
<meta property="og:image" content="http://example.com/img/NB_7.png">
<meta property="article:published_time" content="2021-08-03T19:07:57.000Z">
<meta property="article:modified_time" content="2021-08-29T06:16:48.162Z">
<meta property="article:author" content="TainTear">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/NavieBayesian_headPage_xy.png"><title>第4章 基于概率论的分类方法-朴素贝叶斯 | TainTear's Blog</title><link ref="canonical" href="http://example.com/2021/08/04/ml_4/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第4章 基于概率论的分类方法-朴素贝叶斯</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-04</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="/img/NavieBayesian_headPage_xy.png" alt="朴素贝叶斯_首页" title="朴素贝叶斯首页"></p>

        <h2 id="朴素贝叶斯-概述"   >
          <a href="#朴素贝叶斯-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-概述" class="headerlink" title="朴素贝叶斯 概述"></a>朴素贝叶斯 概述</h2>
      <p><code>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。</code></p>

        <h2 id="贝叶斯理论-amp-条件概率"   >
          <a href="#贝叶斯理论-amp-条件概率" class="heading-link"><i class="fas fa-link"></i></a><a href="#贝叶斯理论-amp-条件概率" class="headerlink" title="贝叶斯理论 &amp; 条件概率"></a>贝叶斯理论 &amp; 条件概率</h2>
      
        <h3 id="贝叶斯理论"   >
          <a href="#贝叶斯理论" class="heading-link"><i class="fas fa-link"></i></a><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h3>
      <p>我们现在有一个数据集，它由两类数据组成，数据分布如下图所示: </p>
<p><img src="img/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.png" alt="朴素贝叶斯示例数据分布" title="参数已知的概率分布"></p>
<p>我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别: </p>
<ul>
<li>如果 p1(x,y) &gt; p2(x,y) ，那么类别为1</li>
<li>如果 p2(x,y) &gt; p1(x,y) ，那么类别为2</li>
</ul>
<p>也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。</p>

        <h3 id="条件概率"   >
          <a href="#条件概率" class="heading-link"><i class="fas fa-link"></i></a><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3>
      <p>如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。</p>
<p>有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。</p>
<p><img src="/img/NB_2.png" alt="包含 7 块石头的集合"></p>
<p>如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？</p>
<p><img src="/img/NB_3.png" alt="7块石头放入两个桶中"></p>
<p>计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3 。</p>
<p>条件概率的计算公式如下: </p>
<p>P(white|bucketB) = P(white and bucketB) / P(bucketB)</p>
<p>首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。</p>
<p>另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法: </p>
<p><img src="/img/NB_4.png" alt="计算p(c|x)的方法"></p>

        <h3 id="使用条件概率来分类"   >
          <a href="#使用条件概率来分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#使用条件概率来分类" class="headerlink" title="使用条件概率来分类"></a>使用条件概率来分类</h3>
      <p>上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y):</p>
<ul>
<li>如果 p1(x, y) &gt; p2(x, y), 那么属于类别 1;</li>
<li>如果 p2(x, y) &gt; p1(X, y), 那么属于类别 2.</li>
</ul>
<p>这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到: </p>
<p><img src="/img/NB_5.png" alt="应用贝叶斯准则"></p>
<p>使用上面这些定义，可以定义贝叶斯分类准则为:</p>
<ul>
<li>如果 P(c1|x, y) &gt; P(c2|x, y), 那么属于类别 c1;</li>
<li>如果 P(c2|x, y) &gt; P(c1|x, y), 那么属于类别 c2.</li>
</ul>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>我们假设特征之间  <strong>相互独立</strong> 。所谓 <b>独立(independence)</b> 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，<b>每个特征同等重要</b>。</p>
<p><b>Note:</b> 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。</p>

        <h2 id="朴素贝叶斯-场景"   >
          <a href="#朴素贝叶斯-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-场景" class="headerlink" title="朴素贝叶斯 场景"></a>朴素贝叶斯 场景</h2>
      <p>机器学习的一个重要应用就是文档的自动分类。</p>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。</p>

        <h2 id="朴素贝叶斯-原理"   >
          <a href="#朴素贝叶斯-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-原理" class="headerlink" title="朴素贝叶斯 原理"></a>朴素贝叶斯 原理</h2>
      
        <h3 id="朴素贝叶斯-工作原理"   >
          <a href="#朴素贝叶斯-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-工作原理" class="headerlink" title="朴素贝叶斯 工作原理"></a>朴素贝叶斯 工作原理</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">提取所有文档中的词条并进行去重</span><br><span class="line">获取文档的所有类别</span><br><span class="line">计算每个类别中的文档数目</span><br><span class="line">对每篇训练文档: </span><br><span class="line">    对每个类别: </span><br><span class="line">        如果词条出现在文档中--&gt;增加该词条的计数值（for循环或者矩阵相加）</span><br><span class="line">        增加所有词条的计数值（此类别下词条总数）</span><br><span class="line">对每个类别: </span><br><span class="line">    对每个词条: </span><br><span class="line">        将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）</span><br><span class="line">返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）</span><br></pre></td></tr></table></div></figure>


        <h3 id="朴素贝叶斯-开发流程"   >
          <a href="#朴素贝叶斯-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-开发流程" class="headerlink" title="朴素贝叶斯 开发流程"></a>朴素贝叶斯 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法。</span><br><span class="line">准备数据: 需要数值型或者布尔型数据。</span><br><span class="line">分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。</span><br><span class="line">训练算法: 计算不同的独立特征的条件概率。</span><br><span class="line">测试算法: 计算错误率。</span><br><span class="line">使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。</span><br></pre></td></tr></table></div></figure>


        <h3 id="朴素贝叶斯-算法特点"   >
          <a href="#朴素贝叶斯-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-算法特点" class="headerlink" title="朴素贝叶斯 算法特点"></a>朴素贝叶斯 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 在数据较少的情况下仍然有效，可以处理多类别问题。</span><br><span class="line">缺点: 对于输入数据的准备方式较为敏感。</span><br><span class="line">适用数据类型: 标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h2 id="朴素贝叶斯-项目案例"   >
          <a href="#朴素贝叶斯-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-项目案例" class="headerlink" title="朴素贝叶斯 项目案例"></a>朴素贝叶斯 项目案例</h2>
      
        <h3 id="项目案例1-屏蔽社区留言板的侮辱性言论"   >
          <a href="#项目案例1-屏蔽社区留言板的侮辱性言论" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-屏蔽社区留言板的侮辱性言论" class="headerlink" title="项目案例1: 屏蔽社区留言板的侮辱性言论"></a>项目案例1: 屏蔽社区留言板的侮辱性言论</h3>
      <p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法</span><br><span class="line">准备数据: 从文本中构建词向量</span><br><span class="line">分析数据: 检查词条确保解析的正确性</span><br><span class="line">训练算法: 从词向量计算概率</span><br><span class="line">测试算法: 根据现实情况修改分类器</span><br><span class="line">使用算法: 对社区留言板言论进行分类</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>本例是我们自己构造的词表:</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建数据集</span></span><br><span class="line"><span class="string">    :return: 单词列表postingList, 所属类别classVec</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    postingList = [[<span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;flea&#x27;</span>, <span class="string">&#x27;problems&#x27;</span>, <span class="string">&#x27;help&#x27;</span>, <span class="string">&#x27;please&#x27;</span>], <span class="comment">#[0,0,1,1,1......]</span></span><br><span class="line">                   [<span class="string">&#x27;maybe&#x27;</span>, <span class="string">&#x27;not&#x27;</span>, <span class="string">&#x27;take&#x27;</span>, <span class="string">&#x27;him&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;park&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;cute&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;him&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;posting&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;mr&#x27;</span>, <span class="string">&#x27;licks&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;steak&#x27;</span>, <span class="string">&#x27;how&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;him&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;quit&#x27;</span>, <span class="string">&#x27;buying&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;food&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList, classVec</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 从文本中构建词向量</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取所有单词的集合</span></span><br><span class="line"><span class="string">    :param dataSet: 数据集</span></span><br><span class="line"><span class="string">    :return: 所有单词的集合(即不含重复元素的单词列表)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    vocabSet = <span class="built_in">set</span>([])  <span class="comment"># create empty set</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 操作符 | 用于求两个集合的并集</span></span><br><span class="line">        vocabSet = vocabSet | <span class="built_in">set</span>(document)  <span class="comment"># union of the two sets</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(vocabSet)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span>(<span class="params">vocabList, inputSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    遍历查看该单词是否出现，出现该单词则将该单词置1</span></span><br><span class="line"><span class="string">    :param vocabList: 所有单词集合列表</span></span><br><span class="line"><span class="string">    :param inputSet: 输入数据集</span></span><br><span class="line"><span class="string">    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 创建一个和词汇表等长的向量，并将其元素都设置为0</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * <span class="built_in">len</span>(vocabList)<span class="comment"># [0,0......]</span></span><br><span class="line">    <span class="comment"># 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;the word: %s is not in my Vocabulary!&quot;</span> % word</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<p>检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOPosts, listClasses = bayes.loadDataSet()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList = bayes.createVocabList(listOPosts)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList</span><br><span class="line">[<span class="string">&#x27;cute&#x27;</span>, <span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;help&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>, <span class="string">&#x27;quit&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;problems&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;park&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;flea&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>, <span class="string">&#x27;licks&#x27;</span>, <span class="string">&#x27;food&#x27;</span>, <span class="string">&#x27;not&#x27;</span>, <span class="string">&#x27;him&#x27;</span>, <span class="string">&#x27;buying&#x27;</span>, <span class="string">&#x27;posting&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;maybe&#x27;</span>, <span class="string">&#x27;please&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;how&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;take&#x27;</span>, <span class="string">&#x27;mr&#x27;</span>, <span class="string">&#x27;steak&#x27;</span>, <span class="string">&#x27;my&#x27;</span>]</span><br></pre></td></tr></table></div></figure>

<p>检查函数有效性。例如: myVocabList 中索引为 2 的元素是什么单词？应该是是 help 。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">0</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">3</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 从词向量计算概率</p>
</blockquote>
<p>现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 <b>w</b>. 粗体的 <b>w</b> 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。</p>
<p><img src="/img/NB_6.png" alt="重写贝叶斯准则"></p>
<p>我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。</p>
<p>问: 上述代码实现中，为什么没有计算P(w)？</p>
<p>答: 根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个ci，P(w)是固定的。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。</p>
<p>首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。接下来计算 p(<b>w</b> | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2…wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)…p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。</p>
<p>朴素贝叶斯分类器训练函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据原版</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率，即trainCategory中所有的1的个数，</span></span><br><span class="line">    <span class="comment"># 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    p0Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line">    p1Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数</span></span><br><span class="line">    p0Denom = <span class="number">0.0</span></span><br><span class="line">    p1Denom = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="comment"># 是否是侮辱性文件</span></span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果是侮辱性文件，对侮辱性文件的向量进行加和</span></span><br><span class="line">            p1Num += trainMatrix[i] <span class="comment">#[0,1,1,....] + [0,1,1,....]-&gt;[0,2,2,...]</span></span><br><span class="line">            <span class="comment"># 对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在1类别下，每个单词出现的概率</span></span><br><span class="line">    p1Vect = p1Num / p1Denom<span class="comment"># [1,2,3,5]/90-&gt;[1/90,...]</span></span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在0类别下，每个单词出现的概率</span></span><br><span class="line">    p0Vect = p0Num / p0Denom</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 根据现实情况修改分类器</p>
</blockquote>
<p>在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。</p>
<p>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)… p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</p>
<p>下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。</p>
<p><img src="/img/NB_7.png" alt="函数图像"></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


<blockquote>
<p>使用算法: 对社区留言板言论进行分类</p>
</blockquote>
<p>朴素贝叶斯分类函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span>(<span class="params">vec2Classify, p0Vec, p1Vec, pClass1</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用算法: </span></span><br><span class="line"><span class="string">        # 将乘法转换为加法</span></span><br><span class="line"><span class="string">        乘法: P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)</span></span><br><span class="line"><span class="string">        加法: P(F1|C)*P(F2|C)....P(Fn|C)P(C) -&gt; log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line"><span class="string">    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量</span></span><br><span class="line"><span class="string">    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line"><span class="string">    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line"><span class="string">    :param pClass1: 类别1，侮辱性文件的出现概率</span></span><br><span class="line"><span class="string">    :return: 类别1 or 0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line">    <span class="comment"># 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，</span></span><br><span class="line">    <span class="comment"># 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。</span></span><br><span class="line">    <span class="comment"># 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。</span></span><br><span class="line">    <span class="comment"># 我的理解是: 这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来</span></span><br><span class="line">    p1 = <span class="built_in">sum</span>(vec2Classify * p1Vec) + log(pClass1) <span class="comment"># P(w|c1) * P(c1) ，即贝叶斯准则的分子</span></span><br><span class="line">    p0 = <span class="built_in">sum</span>(vec2Classify * p0Vec) + log(<span class="number">1.0</span> - pClass1) <span class="comment"># P(w|c0) * P(c0) ，即贝叶斯准则的分子·</span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    测试朴素贝叶斯算法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1. 加载数据集</span></span><br><span class="line">    listOPosts, listClasses = loadDataSet()</span><br><span class="line">    <span class="comment"># 2. 创建单词集合</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    <span class="comment"># 3. 计算单词是否出现并创建数据矩阵</span></span><br><span class="line">    trainMat = []</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        <span class="comment"># 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息</span></span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment"># 4. 训练数据</span></span><br><span class="line">    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))</span><br><span class="line">    <span class="comment"># 5. 测试数据</span></span><br><span class="line">    testEntry = [<span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="built_in">print</span> testEntry, <span class="string">&#x27;classified as: &#x27;</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br><span class="line">    testEntry = [<span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="built_in">print</span> testEntry, <span class="string">&#x27;classified as: &#x27;</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br></pre></td></tr></table></div></figure>



        <h3 id="项目案例2-使用朴素贝叶斯过滤垃圾邮件"   >
          <a href="#项目案例2-使用朴素贝叶斯过滤垃圾邮件" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例2-使用朴素贝叶斯过滤垃圾邮件" class="headerlink" title="项目案例2: 使用朴素贝叶斯过滤垃圾邮件"></a>项目案例2: 使用朴素贝叶斯过滤垃圾邮件</h3>
      <p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <p>使用朴素贝叶斯对电子邮件进行分类</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 提供文本文件</span><br><span class="line">准备数据: 将文本文件解析成词条向量</span><br><span class="line">分析数据: 检查词条确保解析的正确性</span><br><span class="line">训练算法: 使用我们之前建立的 trainNB() 函数</span><br><span class="line">测试算法: 使用朴素贝叶斯进行交叉验证</span><br><span class="line">使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>文本文件内容如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hi Peter,</span><br><span class="line"></span><br><span class="line">With Jose out of town, do you want to</span><br><span class="line">meet once in a while to keep things</span><br><span class="line">going and do some interesting stuff?</span><br><span class="line"></span><br><span class="line">Let me know</span><br><span class="line">Eugene</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>使用正则表达式来切分文本</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mySent = <span class="string">&#x27;This book is the best book on Python or M.L. I have ever laid eyes upon.&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>regEx = re.<span class="built_in">compile</span>(<span class="string">&#x27;\\W*&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens = regEx.split(mySent)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens</span><br><span class="line">[<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;best&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;Python&#x27;</span>, <span class="string">&#x27;or&#x27;</span>, <span class="string">&#x27;M.L.&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;have&#x27;</span>, <span class="string">&#x27;ever&#x27;</span>, <span class="string">&#x27;laid&#x27;</span>, <span class="string">&#x27;eyes&#x27;</span>, <span class="string">&#x27;upon&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用朴素贝叶斯进行交叉验证</p>
</blockquote>
<p>文件解析及完整的垃圾邮件测试函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切分文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span>(<span class="params">bigString</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        接收一个大字符串并将其解析为字符串列表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bigString -- 大字符串</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="comment"># 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串</span></span><br><span class="line">    listOfTokens = re.split(<span class="string">r&#x27;\W*&#x27;</span>, bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> <span class="built_in">len</span>(tok) &gt; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对贝叶斯垃圾邮件分类器进行自动化处理。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    docList = []</span><br><span class="line">    classList = []</span><br><span class="line">    fullText = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">26</span>):</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 1 类别</span></span><br><span class="line">        wordList = textParse(<span class="built_in">open</span>(<span class="string">&#x27;data/4.NaiveBayes/email/spam/%d.txt&#x27;</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 0 类别</span></span><br><span class="line">        wordList = textParse(<span class="built_in">open</span>(<span class="string">&#x27;data/4.NaiveBayes/email/ham/%d.txt&#x27;</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 创建词汇表    </span></span><br><span class="line">    vocabList = createVocabList(docList)</span><br><span class="line">    trainingSet = <span class="built_in">range</span>(<span class="number">50</span>)</span><br><span class="line">    testSet = []</span><br><span class="line">    <span class="comment"># 随机取 10 个邮件用来测试</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># random.uniform(x, y) 随机生成一个范围为 x ~ y 的实数</span></span><br><span class="line">        randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, <span class="built_in">len</span>(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat = []</span><br><span class="line">    trainClasses = []</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector = setOfWords2Vec(vocabList, docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the errorCount is: &#x27;</span>, errorCount</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the testSet length is :&#x27;</span>, <span class="built_in">len</span>(testSet)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the error rate is :&#x27;</span>, <span class="built_in">float</span>(errorCount)/<span class="built_in">len</span>(testSet)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
</blockquote>

        <h3 id="项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向"   >
          <a href="#项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="headerlink" title="项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向"></a>项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向</h3>
      <p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-2"   >
          <a href="#项目概述-2" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-2" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。</p>
<p>我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是哪些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。</p>

        <h4 id="开发流程-2"   >
          <a href="#开发流程-2" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-2" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</span><br><span class="line">准备数据: 将文本文件解析成词条向量</span><br><span class="line">分析数据: 检查词条确保解析的正确性</span><br><span class="line">训练算法: 使用我们之前建立的 trainNB0() 函数</span><br><span class="line">测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</span><br><span class="line">使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</p>
</blockquote>
<p>也就是导入 RSS 源，我们使用 python 下载文本，在<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://code.google.com/p/feedparser/" >http://code.google.com/p/feedparser/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>python setup.py install</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>文档词袋模型</p>
<p>我们将每个词的出现与否作为一个特征，这可以被描述为 <b>词集模型(set-of-words model)</b>。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 <b>词袋模型(bag-of-words model)</b>。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。</p>
<p>如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span>(<span class="params">vocaList, inputSet</span>):</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * <span class="built_in">len</span>(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocaList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个包含在所有文档中出现的不重复词的列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    vocabSet=<span class="built_in">set</span>([])    <span class="comment">#创建一个空集</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet=vocabSet|<span class="built_in">set</span>(document)   <span class="comment">#创建两个集合的并集</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(vocabSet)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2VecMN</span>(<span class="params">vocabList,inputSet</span>):</span></span><br><span class="line">    returnVec=[<span class="number">0</span>]*<span class="built_in">len</span>(vocabList)  <span class="comment">#创建一个其中所含元素都为0的向量</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">                returnVec[vocabList.index(word)]+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件解析</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span>(<span class="params">bigString</span>):</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    listOfTokens=re.split(<span class="string">r&#x27;\W*&#x27;</span>,bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> <span class="built_in">len</span>(tok)&gt;<span class="number">2</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计 </span></span><br><span class="line">    <span class="comment"># 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#RSS源分类器及高频词去除函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span>(<span class="params">vocabList,fullText</span>):</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    freqDict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> vocabList:  <span class="comment">#遍历词汇表中的每个词</span></span><br><span class="line">        freqDict[token]=fullText.count(token)  <span class="comment">#统计每个词在文本中出现的次数</span></span><br><span class="line">    sortedFreq=<span class="built_in">sorted</span>(freqDict.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)  <span class="comment">#根据每个词出现的次数从高到底对字典进行排序</span></span><br><span class="line">    <span class="keyword">return</span> sortedFreq[:<span class="number">30</span>]   <span class="comment">#返回出现次数最高的30个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">localWords</span>(<span class="params">feed1,feed0</span>):</span></span><br><span class="line">    <span class="keyword">import</span> feedparser</span><br><span class="line">    docList=[];classList=[];fullText=[]</span><br><span class="line">    minLen=<span class="built_in">min</span>(<span class="built_in">len</span>(feed1[<span class="string">&#x27;entries&#x27;</span>]),<span class="built_in">len</span>(feed0[<span class="string">&#x27;entries&#x27;</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(minLen):</span><br><span class="line">        wordList=textParse(feed1[<span class="string">&#x27;entries&#x27;</span>][i][<span class="string">&#x27;summary&#x27;</span>])   <span class="comment">#每次访问一条RSS源</span></span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        wordList=textParse(feed0[<span class="string">&#x27;entries&#x27;</span>][i][<span class="string">&#x27;summary&#x27;</span>])</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList=createVocabList(docList)</span><br><span class="line">    top30Words=calcMostFreq(vocabList,fullText)</span><br><span class="line">    <span class="keyword">for</span> pairW <span class="keyword">in</span> top30Words:</span><br><span class="line">        <span class="keyword">if</span> pairW[<span class="number">0</span>] <span class="keyword">in</span> vocabList:vocabList.remove(pairW[<span class="number">0</span>])    <span class="comment">#去掉出现次数最高的那些词</span></span><br><span class="line">    trainingSet=<span class="built_in">range</span>(<span class="number">2</span>*minLen);testSet=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        randIndex=<span class="built_in">int</span>(random.uniform(<span class="number">0</span>,<span class="built_in">len</span>(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat=[];trainClasses=[]</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V,p1V,pSpam=trainNBO(array(trainMat),array(trainClasses))</span><br><span class="line">    errorCount=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the error rate is:&#x27;</span>,<span class="built_in">float</span>(errorCount)/<span class="built_in">len</span>(testSet)</span><br><span class="line">    <span class="keyword">return</span> vocabList,p0V,p1V</span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯分类函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span>(<span class="params">vec2Classify,p0Vec,p1Vec,pClass1</span>):</span></span><br><span class="line">    p1=<span class="built_in">sum</span>(vec2Classify*p1Vec)+log(pClass1)</span><br><span class="line">    p0=<span class="built_in">sum</span>(vec2Classify*p0Vec)+log(<span class="number">1.0</span>-pClass1)</span><br><span class="line">    <span class="keyword">if</span> p1&gt;p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</p>
</blockquote>
<p>函数 localWords() 使用了两个 RSS 源作为参数，RSS 源要在函数外导入，这样做的原因是 RSS 源会随时间而改变，重新加载 RSS 源就会得到新的数据</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">&#x27;bayes&#x27;</span> <span class="keyword">from</span> <span class="string">&#x27;bayes.pyc&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ny=feedparser.parse(<span class="string">&#x27;http://newyork.craigslist.org/stp/index.rss&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sy=feedparser.parse(<span class="string">&#x27;http://sfbay.craigslist.org/stp/index.rss&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br></pre></td></tr></table></div></figure>
<p>为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值</p>
<p>接下来，我们要分析一下数据，显示地域相关的用词</p>
<p>可以先对向量pSF与pNY进行排序，然后按照顺序打印出来，将下面的代码添加到文件中: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#最具表征性的词汇显示函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTopWords</span>(<span class="params">ny,sf</span>):</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    vocabList,p0V,p1V=localWords(ny,sf)</span><br><span class="line">    topNY=[];topSF=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(p0V)):</span><br><span class="line">        <span class="keyword">if</span> p0V[i]&gt;-<span class="number">6.0</span>:topSF.append((vocabList[i],p0V[i]))</span><br><span class="line">        <span class="keyword">if</span> p1V[i]&gt;-<span class="number">6.0</span>:topNY.append((vocabList[i],p1V[i]))</span><br><span class="line">    sortedSF=<span class="built_in">sorted</span>(topSF,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedSF:</span><br><span class="line">        <span class="built_in">print</span> item[<span class="number">0</span>]</span><br><span class="line">    sortedNY=<span class="built_in">sorted</span>(topNY,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedNY:</span><br><span class="line">        <span class="built_in">print</span> item[<span class="number">0</span>]</span><br></pre></td></tr></table></div></figure>

<p>函数 getTopWords() 使用两个 RSS 源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储，与之前返回排名最高的 X 个单词不同，这里可以返回大于某个阈值的所有词，这些元组会按照它们的条件概率进行排序。</p>
<p>保存 bayes.py 文件，在python提示符下输入: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">&#x27;bayes&#x27;</span> <span class="keyword">from</span> <span class="string">&#x27;bayes.pyc&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.getTopWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br><span class="line">SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**</span><br><span class="line">how</span><br><span class="line">last</span><br><span class="line">man</span><br><span class="line">...</span><br><span class="line">veteran</span><br><span class="line">still</span><br><span class="line">ends</span><br><span class="line">late</span><br><span class="line">off</span><br><span class="line">own</span><br><span class="line">know</span><br><span class="line">NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**</span><br><span class="line">someone</span><br><span class="line">meet</span><br><span class="line">...</span><br><span class="line">apparel</span><br><span class="line">recalled</span><br><span class="line">starting</span><br><span class="line">strings</span><br></pre></td></tr></table></div></figure>

<p>当注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能，去掉这几行代码之后，错误率为54%，，而保留这些代码得到的错误率为70%。这里观察到，这些留言中出现次数最多的前30个词涵盖了所有用词的30%，vocabList的大小约为3000个词，也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定高频词中移除结构上的辅助词，该词表称为停用词表。</p>
<p>从最后输出的单词，可以看出程序输出了大量的停用词，可以移除固定的停用词看看结果如何，这样做的话，分类错误率也会降低。</p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuxin" >羊三</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="http://example.com">TainTear</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="http://example.com/2021/08/04/ml_4/">http://example.com/2021/08/04/ml_4/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/05/ml_5/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第5章 Logistic回归</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/03/ml_3/"><span class="paginator-prev__text">第3章 决策树</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">
          朴素贝叶斯 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA-amp-%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="toc-number">2.</span> <span class="toc-text">
          贝叶斯理论 &amp; 条件概率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA"><span class="toc-number">2.1.</span> <span class="toc-text">
          贝叶斯理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="toc-number">2.2.</span> <span class="toc-text">
          条件概率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E6%9D%A5%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.</span> <span class="toc-text">
          使用条件概率来分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E5%9C%BA%E6%99%AF"><span class="toc-number">3.</span> <span class="toc-text">
          朴素贝叶斯 场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E5%8E%9F%E7%90%86"><span class="toc-number">4.</span> <span class="toc-text">
          朴素贝叶斯 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">4.1.</span> <span class="toc-text">
          朴素贝叶斯 工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">4.2.</span> <span class="toc-text">
          朴素贝叶斯 开发流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E7%AE%97%E6%B3%95%E7%89%B9%E7%82%B9"><span class="toc-number">4.3.</span> <span class="toc-text">
          朴素贝叶斯 算法特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">5.</span> <span class="toc-text">
          朴素贝叶斯 项目案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B1-%E5%B1%8F%E8%94%BD%E7%A4%BE%E5%8C%BA%E7%95%99%E8%A8%80%E6%9D%BF%E7%9A%84%E4%BE%AE%E8%BE%B1%E6%80%A7%E8%A8%80%E8%AE%BA"><span class="toc-number">5.1.</span> <span class="toc-text">
          项目案例1: 屏蔽社区留言板的侮辱性言论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">5.1.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">5.1.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B2-%E4%BD%BF%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%BF%87%E6%BB%A4%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6"><span class="toc-number">5.2.</span> <span class="toc-text">
          项目案例2: 使用朴素贝叶斯过滤垃圾邮件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0-1"><span class="toc-number">5.2.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B-1"><span class="toc-number">5.2.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B3-%E4%BD%BF%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E4%BB%8E%E4%B8%AA%E4%BA%BA%E5%B9%BF%E5%91%8A%E4%B8%AD%E8%8E%B7%E5%8F%96%E5%8C%BA%E5%9F%9F%E5%80%BE%E5%90%91"><span class="toc-number">5.3.</span> <span class="toc-text">
          项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0-2"><span class="toc-number">5.3.1.</span> <span class="toc-text">
          项目概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B-2"><span class="toc-number">5.3.2.</span> <span class="toc-text">
          开发流程</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>