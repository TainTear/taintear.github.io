<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="TainTear&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="TainTear">
<meta name="twitter:card" content="summary"><title>TainTear's Blog</title><link ref="canonical" href="http://example.com/page/2/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/06/ml_6.2/">第6章 SVM</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><blockquote>
<p>声明</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">阅读本文前，需要您懂一些高等数学、概率论、线性代数的相关知识，以便更好理解。</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面这些关于 SVM 的理解，是根据知乎和其他博客或者网站中查询到的资料加以整理，</span><br><span class="line">并结合 ApacheCN 这段时间的撸代码和相关研究得到，有理解有误的地方还望大家指出，谢谢。</span><br><span class="line">再次感谢网上的大佬们的无私贡献。</span><br><span class="line"></span><br><span class="line">ApacheCN: http://www.apachecn.org/</span><br><span class="line">ApacheCN MachineLearning github: https://github.com/apachecn/AiLearning</span><br><span class="line"></span><br><span class="line">网上资料参考链接:https://www.zhihu.com/question/21094489</span><br><span class="line">            http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html</span><br><span class="line">            https://zhuanlan.zhihu.com/p/26891427?utm_medium=social&amp;utm_source=qq</span><br><span class="line">            https://zhuanlan.zhihu.com/p/21308801?utm_medium=social&amp;utm_source=qq</span><br><span class="line">            http://blog.csdn.net/v_july_v/article/details/7624837</span><br></pre></td></tr></table></div></figure>


        <h2 id="Overview"   >
          <a href="#Overview" class="heading-link"><i class="fas fa-link"></i></a><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2>
      
        <h3 id="What’s-the-SVM"   >
          <a href="#What’s-the-SVM" class="heading-link"><i class="fas fa-link"></i></a><a href="#What’s-the-SVM" class="headerlink" title="What’s the SVM?"></a>What’s the SVM?</h3>
      <p>^_^ 首先，支持向量机不是一种机器，而是一种机器学习算法。</p>
<p>1、SVM - Support Vector Machine ，俗称支持向量机，是一种 supervised learning （监督学习）算法，属于 classification （分类）的范畴。</p>
<p>2、在数据挖掘的应用中，与 unsupervised learning （无监督学习）的 Clustering（聚类）相对应和区别。</p>
<p>3、广泛应用于 Machine Learning （机器学习），Computer Vision （计算机视觉，装逼一点说，就是 cv）和 Data Mining （数据挖掘）当中。</p>

        <h3 id="“-Machine-（机）”-是什么？"   >
          <a href="#“-Machine-（机）”-是什么？" class="heading-link"><i class="fas fa-link"></i></a><a href="#“-Machine-（机）”-是什么？" class="headerlink" title="“ Machine （机）” 是什么？"></a>“ Machine （机）” 是什么？</h3>
      <p>Classification Machine，是分类器，这个没什么好说的。也可以理解为算法，机器学习领域里面常常用 “机” 也就是 machine 这个字表示算法。</p>

        <h3 id="“支持向量”-又是什么？"   >
          <a href="#“支持向量”-又是什么？" class="heading-link"><i class="fas fa-link"></i></a><a href="#“支持向量”-又是什么？" class="headerlink" title="“支持向量” 又是什么？"></a>“支持向量” 又是什么？</h3>
      <p><b>通俗理解</b>:<br>support vector （支持向量）的意思就是 <b>数据集中的某些点</b>，位置比较特殊。比如 x+y-2=0 这条直线，直线上面区域 x+y-2&gt;0 的全是 A 类，下面的 x+y-2&lt;0 的全是 B 类，我们找这条直线的时候，一般就看聚集在一起的两类数据，他们各自的 <b>最边缘</b> 位置的点，也就是最靠近划分直线的那几个点，而其他点对这条直线的最终位置的确定起不了作用，所以我姑且叫这些点叫 “支持点”（意思就是有用的点），但是在数学上，没这种说法，数学里的点，又可以叫向量，比如 二维点 (x,y) 就是二维向量，三维度的就是三维向量 (x,y,z)。所以 “<b>支持点</b>” 改叫 “<b>支持向量</b>” ，听起来比较专业，而且又装逼，何乐而不为呢？是吧… </p>
<p><b>不通俗的理解</b>:<br>在 maximum margin （最大间隔）上的这些点就叫 “支持向量”，我想补充的是为啥这些点就叫 “支持向量” ，因为最后的 classification machine （分类器）的表达式里只含有这些 “支持向量” 的信息，而与其他数据点无关: </p>
<p><img src="/img/supportVector%E5%85%AC%E5%BC%8F.jpg" alt="支持向量机公式" title="supportVector公式"></p>
<p>在这个表达式中，只有支持向量的系数 <img src="/img/alpha.png" alt="alphai" title="alphai"> 不等于 0 。</p>
<p>如果还是不怎么理解，不要紧，看下图:</p>
<p><img src="/img/supportVector.png" alt="supportVector" title="supportVector"></p>
<p>“支持向量” 就是图中用紫色框框圈出来的点…</p>

        <h2 id="Concept-（相关概念）"   >
          <a href="#Concept-（相关概念）" class="heading-link"><i class="fas fa-link"></i></a><a href="#Concept-（相关概念）" class="headerlink" title="Concept （相关概念）"></a>Concept （相关概念）</h2>
      <p>我们先看一张图</p>
<p><img src="/img/svm_2.png" alt="supportVectorMachine" title="supportVectorMachine"></p>
<p><code>linearly separable （线性可分）</code>: 如上图中的两组数据，它们之间已经分的足够开了，因此很容易就可以在图中画出一条直线将两组数据点分开。在这种情况下，这组数据就被称为<b>线性可分数据</b>。</p>
<p><code>separating hyperplane（分隔超平面）</code>: 上述将数据集分隔开来的直线称为<b>分隔超平面</b>。</p>
<p><code>hyperplane（超平面）</code>: 在上面给出的例子中，由于数据点都在二维平面上，所以此时分隔超平面就只是一条直线。但是，如果所给的数据集是三维的，那么此时用来分隔数据的就是一个平面。显而易见，更高纬度的情况可以依此类推。如果数据是 1024 维的，那么就需要一个 1023 维的某某对象（不是你们的男（女）票）来对数据进行分隔。这个 1023 维的某某对象到底应该叫什么呢？ N-1 维呢？该对象被称为<b>超平面</b>，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。</p>
<p><code>margin（间隔）</code>: 我们希望能通过上述的方式来构建分类器，即如果数据点离决策边界越远，那么其最后的预测结果也就越可信。既然这样，我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里所说的点到分隔面的距离就是 <b>间隔</b>。我们希望间隔尽可能地大，这是因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分类器尽可能健壮。</p>
<p><code>支持向量（support vector）</code> : 就是上面所说的离分隔超平面最近的那些点。</p>
<p> <code>分类器</code> : 分类器就是给定一个样本的数据，判定这个样本属于哪个类别的算法。例如在股票涨跌预测中，我们认为前一天的交易量和收盘价对于第二天的涨跌是有影响的，那么分类器就是通过样本的交易量和收盘价预测第二天的涨跌情况的算法。</p>
<p><code>特征</code> : 在分类问题中，输入到分类器中的数据叫做特征。以上面的股票涨跌预测问题为例，特征就是前一天的交易量和收盘价。</p>
<p><code>线性分类器</code> : 线性分类器是分类器中的一种，就是判定分类结果的根据是通过特征的线性组合得到的，不能通过特征的非线性运算结果作为判定根据。还以上面的股票涨跌预测问题为例，判断的依据只能是前一天的交易量和收盘价的线性组合，不能将交易量和收盘价进行开方，平方等运算。</p>

        <h2 id="How-does-it-work-（SVM-原理）"   >
          <a href="#How-does-it-work-（SVM-原理）" class="heading-link"><i class="fas fa-link"></i></a><a href="#How-does-it-work-（SVM-原理）" class="headerlink" title="How does it work? （SVM 原理）"></a>How does it work? （SVM 原理）</h2>
      
        <h3 id="1、引用知乎上-简之-大佬的回答"   >
          <a href="#1、引用知乎上-简之-大佬的回答" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、引用知乎上-简之-大佬的回答" class="headerlink" title="1、引用知乎上 @简之 大佬的回答:"></a>1、引用知乎上 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/people/wangjianzhi/answers" >@简之</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 大佬的回答:</h3>
      <p>首先我们讲个故事:</p>
<p>在很久以前的情人节，大侠要去救他的爱人，但魔鬼和他玩了一个游戏。</p>
<p>魔鬼在桌子上似乎有规律放了两种颜色的球，说: “你用一根棍分开它们？要求: 尽量在放更多球之后，仍然适用。”</p>
<p><img src="/img/story_1.png" alt="story_1" title="story_1"></p>
<p>于是大侠这样放，干的不错？ </p>
<p><img src="/img/story_2.png" alt="story_2" title="story_2"></p>
<p>然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。</p>
<p><img src="/img/story_3.png" alt="story_3" title="story_3"></p>
<p>SVM 就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。</p>
<p><img src="/img/story_4.png" alt="story_4" title="story_4"></p>
<p>现在即使魔鬼放了更多的球，棍仍然是一个好的分界线。</p>
<p><img src="/img/story_5.png" alt="story_5" title="story_5"></p>
<p>然后，在 SVM 工具箱中有另一个更加重要的 trick。 魔鬼看到大侠已经学会了一个 trick ，于是魔鬼给了大侠一个新的挑战。</p>
<p><img src="/img/story_6.png" alt="story_6" title="story_6"></p>
<p>现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。</p>
<p><img src="/img/story_7.png" alt="story_7" title="story_7"></p>
<p>现在，从魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。</p>
<p><img src="/img/story_8.png" alt="story_8" title="story_8"></p>
<p>再之后，无聊的大人们，把这些球叫做 <b>「data」</b>，把棍子叫做 <b>「classifier」</b>, 最大间隙 trick 叫做<b>「optimization」</b>， 拍桌子叫做<b>「kernelling」</b>, 那张纸叫做<b>「hyperplane」</b> 。</p>
<p>有梯子的童鞋，可以看一下这个地方，看视频来更直观的感受: </p>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=3liCbRZPrZA" >https://www.youtube.com/watch?v=3liCbRZPrZA</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="2、引用知乎-开膛手水货-大佬的回答，我认为是超级通俗的一个版本"   >
          <a href="#2、引用知乎-开膛手水货-大佬的回答，我认为是超级通俗的一个版本" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、引用知乎-开膛手水货-大佬的回答，我认为是超级通俗的一个版本" class="headerlink" title="2、引用知乎 @开膛手水货 大佬的回答，我认为是超级通俗的一个版本:"></a>2、引用知乎 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/people/kai-tang-shou-xin/answers" >@开膛手水货</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 大佬的回答，我认为是超级通俗的一个版本:</h3>
      <p>支持向量机是用来解决分类问题的。</p>
<p>先考虑最简单的情况，豌豆和米粒，用晒子很快可以分开，小颗粒漏下去，大颗粒保留。</p>
<p>用一个函数来表示就是当直径 d 大于某个值 D ，就判定为豌豆，小于某个值就是米粒。</p>
<p>d&gt;D, 豌豆</p>
<p>d&lt;D,米粒</p>
<p>在数轴上就是在d左边就是米粒，右边就是绿豆，这是一维的情况。</p>
<p>但是实际问题没这么简单，考虑的问题不单单是尺寸，一个花的两个品种，怎么分类？</p>
<p>假设决定他们分类的有两个属性，花瓣尺寸和颜色。单独用一个属性来分类，像刚才分米粒那样，就不行了。这个时候我们设置两个值 尺寸 x 和颜色 y.</p>
<p>我们把所有的数据都丢到 x-y 平面上作为点，按道理如果只有这两个属性决定了两个品种，数据肯定会按两类聚集在这个二维平面上。</p>
<p>我们只要找到一条直线，把这两类划分开来，分类就很容易了，以后遇到一个数据，就丢进这个平面，看在直线的哪一边，就是哪一类。</p>
<p>比如 x+y-2=0 这条直线，我们把数据 (x,y) 代入，只要认为 x+y-2&gt;0 的就是 A 类， x+y-2&lt;0 的就是 B 类。</p>
<p>以此类推，还有三维的，四维的，N维的 属性的分类，这样构造的也许就不是直线，而是平面，超平面。</p>
<p>一个三维的函数分类 : x+y+z-2=0，这就是个分类的平面了。</p>
<p>有时候，分类的那条线不一定是直线，还有可能是曲线，我们通过某些函数来转换，就可以转化成刚才的哪种多维的分类问题，这个就是核函数的思想。</p>
<p>例如: 分类的函数是个圆形 x^2+y^2-4=0 。这个时候令 x^2=a ; y^2=b ,还不就变成了a+b-4=0 这种直线问题了。</p>
<p>这就是支持向量机的思想。</p>

        <h3 id="3、引用-胡KF-大佬的回答（这个需要一些数学知识）"   >
          <a href="#3、引用-胡KF-大佬的回答（这个需要一些数学知识）" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、引用-胡KF-大佬的回答（这个需要一些数学知识）" class="headerlink" title="3、引用 @胡KF 大佬的回答（这个需要一些数学知识）:"></a>3、引用 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/people/hu-kf/answers" >@胡KF</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 大佬的回答（这个需要一些数学知识）:</h3>
      <p>如图的例子，（训练集）红色点是我们已知的分类1，（训练集）蓝色点是已知的分类2，我们想寻找一个分隔超平面（图中绿线）（因为示例是二维数据点，所以只是一条线，如果数据是三维的就是平面，如果是三维以上就是超平面）把这两类完全分开，这样的话再来一个样本点需要我们预测的话，我们就可以根据这个分界超平面预测出分类结果。</p>
<p><img src="/img/hu_1.jpg" alt="hu_1" title="hu_1"></p>
<p>那我们如何选择这个分类超平面呢？从数学上说，超平面的公式是,也就是说如何选取这个 <img src="/img/w.png" alt="w" title="w">（是个向量）。</p>
<p>传统方法是根据最小二乘错误法（least squared error），首先随便定选取一个随机平面，也就是随机选取 <img src="/img/w.png" alt="w" title="w"> 和 <img src="/img/b.png" alt="b" title="b">，然后想必会在训练集中产生大量的错误分类，也就是说，<img src="/img/hu_5.png" alt="wtx+b" title="wtx+b"> 结果应该大于 0 的时候小于 0 ，应该小于 0 的时候大于 0 。这时候有一个错误损失，也就是说对于所有错误的分类，他们的平方和（least squared error） 为: <img src="/img/hu_8.png" alt="平方和" title="平方和"> , 最小二乘法的目标就是让这个值趋于最小，对 <img src="/img/w.png" alt="w" title="w"> 求导取 0 ，采用梯度下降算法，可以求出错误平方和的极值，求出最优的 <img src="/img/w.png" alt="w" title="w"> ，也就是求出最优的超平面。（可以证明，如果基函数是指数族函数，求出的超平面是全局最优的）。</p>
<p>那我们 SVM 算法的思路是怎样的呢？</p>
<p>不同于传统的最小二乘策略的思想，我们采用一种新的思路，这个分界面有什么样的特征呢？</p>
<p>第一，它 “夹” 在两类样本点之间；第二，它离两类样本点中所有 “离它最近的点” ，都离它尽可能的远。如图所示: </p>
<p><img src="/img/hu_2.jpg" alt="hu_2" title="hu_2"></p>
<p>在虚线上的点，就是我们所找到的离分解超平面最近的样本点，X 类中找到了一个，O 类找到了两个。我们需要分类超平面离这三个样本点都尽可能的远，也就是说，它处在两条虚线的中间。这就是我们找到的分界超平面。</p>
<p>另外，这里我们就可以解释什么是 “支持向量” 了，支持向量就是虚线上的离分类超平面最近的样本点，因为每一个样本点都是一个多维的向量，向量的每一个维度都是这个样本点的一个特征。比如在根据身高，体重，特征进行男女分类的时候，每一个人是一个向量，向量有两个维度，第一维是身高，第二维是体重。</p>
<p>介绍完 SVM 的基本思想，我们来探讨一下如何用数学的方法进行 SVM 分类。</p>
<p>首先我们需要把刚刚说的最大间隔分类器的思想用数学公式表达出来。先定义几何间隔的概念，几何间隔就是在多维空间中一个多维点到一个超平面的距离，根据向量的知识可以算出来: </p>
<p><img src="/img/hu_3.png" alt="hu_3" title="hu_3"></p>
<p>然后对于所有的支持向量，使他们到超平面 <img src="/img/hu_5.png" alt="hu_5" title="hu_5"> 的距离最大，也就是</p>
<p><img src="/img/hu_4.png" alt="hu_4" title="hu_4"></p>
<p>因为对于所有支持向量，他们 <img src="/img/hu_5.png" alt="hu_5" title="hu_5"> 的值都是一定的，我们假设恒等于 1 ，那么上式变成了 <img src="/img/hu_6.png" alt="hu_6" title="hu_6"> ,并且对于所有的样本点，满足 <img src="/img/hu_10.png" alt="hu_10" title="hu_10"> 的约束，因此，可以利用拉格朗日乘数法计算出它的极值。也就是求出这个超平面。</p>
<p>推导过程略为复杂，详细了解可以参考凸二次规划知识，结合 SMO 算法理解 SVM 计算超平面的详细过程。</p>
<p>总之，在计算的过程中，我们不需要了解支持向量以外的其他样本点，只需要利用相对于所有样本点来说为数不多的支持向量，就可以求出分类超平面，计算复杂度大为降低。</p>

        <h3 id="4、引用知乎-靠靠靠谱-大佬的理解（这个需要的数学知识更加厉害一点）"   >
          <a href="#4、引用知乎-靠靠靠谱-大佬的理解（这个需要的数学知识更加厉害一点）" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、引用知乎-靠靠靠谱-大佬的理解（这个需要的数学知识更加厉害一点）" class="headerlink" title="4、引用知乎 @靠靠靠谱 大佬的理解（这个需要的数学知识更加厉害一点）:"></a>4、引用知乎 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/people/kao-kao-kao-pu/answers" >@靠靠靠谱</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 大佬的理解（这个需要的数学知识更加厉害一点）:</h3>
      <p>先看思维导图: </p>
<ul>
<li>左边是求解基本的SVM问题</li>
<li>右边是相关扩展</li>
</ul>
<p><img src="/img/k_1.jpg" alt="k_1" title="k_1"></p>
<p><b>什么是 SVM ？</b></p>
<p>Support Vector Machine, 一个普通的 SVM 就是一条直线罢了，用来完美划分 linearly separable 的两类。但这又不是一条普通的直线，这是无数条可以分类的直线当中最完美的，因为它恰好在两个类的中间，距离两个类的点都一样远。而所谓的 Support vector 就是这些离分界线最近的『点』。如果去掉这些点，直线多半是要改变位置的。可以说是这些 vectors （主，点点） support （谓，定义）了 machine （宾，分类器）…</p>
<p><img src="/img/k_2.jpg" alt="k_2" title="k_2"></p>
<p>所以谜底就在谜面上啊朋友们，只要找到了这些最靠近的点不就找到了 SVM 了嘛。</p>
<p>如果是高维的点，SVM 的分界线就是平面或者超平面。其实没有差，都是一刀切两块，我就统统叫直线了。</p>
<p><b>怎么求解 SVM ？</b></p>
<p>关于这条直线，我们知道 </p>
<p>(1)它离两边一样远，(2)最近距离就是到support vector的距离，其他距离只能更远。</p>
<p>于是自然而然可以得到重要表达 <b>I. direct representation</b></p>
<p><img src="/img/k_7.png" alt="k_7" title="k_7"></p>
<p>（可以把 margin 看作是 boundary 的函数，并且想要找到使得是使得 margin 最大化的boundary，而 margin(*) 这个函数是: 输入一个 boundary ，计算（正确分类的）所有苹果和香蕉中，到 boundary 的最小距离。）</p>
<p>又有最大又有最小看起来好矛盾。实际上『最大』是对这个整体使用不同 boundary 层面的最大，『最小』是在比较『点』的层面上的最小。外层在比较 boundary 找最大的 margin ，内层在比较点点找最小的距离。</p>
<p>其中距离，说白了就是点到直线的距离；只要定义带正负号的距离，是 {苹果+1} 面为正 {香蕉-1} 面为负的距离，互相乘上各自的 label <img src="/img/k_8.png" alt="k_8" title="k_8"> ，就和谐统一民主富强了。</p>
<p><img src="/img/k_9.png" alt="k_9" title="k_9"></p>
<p>到这里为止已经说完了所有关于SVM的直观了解，如果不想看求解，可以跳过下面一大段直接到 objective function 。</p>
<p>直接表达虽然清楚但是求解无从下手。做一些简单地等价变换（分母倒上来）可以得到 <b>II. canonical representation </b> （敲黑板）</p>
<p><img src="/img/k_10.png" alt="k_10" title="k_10"></p>
<p>要得到 <b>III. dual representation</b> 之前需要大概知道一下拉格朗日乘子法 (method of lagrange multiplier)，它是用在有各种约束条件(各种 “subject to” )下的目标函数，也就是直接可以求导可以引出 dual representation（怎么还没完摔）</p>
<p><img src="/img/k_11.png" alt="k_11" title="k_11"></p>
<p>稍微借用刚刚数学表达里面的内容看个有趣的东西: </p>
<p>还记得我们怎么预测一个新的水果是苹果还是香蕉吗？我们代入到分界的直线里，然后通过符号来判断。</p>
<p>刚刚w已经被表达出来了也就是说这个直线现在变成了:  <img src="/img/k_12.png" alt="k_12" title="k_12"></p>
<p>看似仿佛用到了所有的训练水果，但是其中 <img src="/img/k_13.png" alt="k_13" title="k_13"> 的水果都没有起到作用，剩下的就是小部分靠边边的 Support vectors 呀。</p>
<p><b>III. dual representation</b></p>
<p><img src="/img/k_14.png" alt="k_14" title="k_14"></p>
<p><b>如果香蕉和苹果不能用直线分割呢？</b></p>
<p><img src="/img/k_3.jpg" alt="k_3" title="k_3"></p>
<p>Kernel trick. </p>
<p>其实用直线分割的时候我们已经使用了 kernel ，那就是线性 kernel , <img src="/img/k_15.png" alt="k_15" title="k_15"></p>
<p>如果要替换 kernel 那么把目标函数里面的内积全部替换成新的 kernel function 就好了，就是这么简单。</p>
<p>第一个武侠大师的比喻已经说得很直观了，低维非线性的分界线其实在高维是可以线性分割的，可以理解为——『你们是虫子！』分得开个p…（大雾）</p>
<p><b>如果香蕉和苹果有交集呢？</b></p>
<p><img src="/img/k_4.jpg" alt="k_4" title="k_4"></p>
<p><img src="/img/k_16.png" alt="k_16" title="k_16"></p>
<p><b>如果还有梨呢？</b></p>
<p><img src="/img/k_5.jpg" alt="k_5" title="k_5"></p>
<p>可以每个类别做一次 SVM: 是苹果还是不是苹果？是香蕉还是不是香蕉？是梨子还是不是梨子？从中选出可能性最大的。这是 one-versus-the-rest approach。</p>
<p>也可以两两做一次 SVM: 是苹果还是香蕉？是香蕉还是梨子？是梨子还是苹果？最后三个分类器投票决定。这是 one-versus-one approace。</p>
<p>但这其实都多多少少有问题，比如苹果特别多，香蕉特别少，我就无脑判断为苹果也不会错太多；多个分类器要放到一个台面上，万一他们的 scale 没有在一个台面上也未可知。</p>
<p>课后题:<br>1、vector 不愿意 support 怎么办？<br>2、苹果好吃还是香蕉好吃？</p>
<p>最后送一张图我好爱哈哈哈 (Credit: Burr Settles)</p>

<p><img src="/img/k_6.png" alt="k_6" title="k_6"></p>
<p>[1] Bishop C M. Pattern recognition[J]. Machine Learning, 2006, 128.</p>
<p>[2] Friedman J, Hastie T, Tibshirani R. The elements of statistical learning[M]. Springer, Berlin: Springer series in statistics, 2001.</p>
<p>[3] James G, Witten D, Hastie T, et al. An introduction to statistical learning[M]. New York: springer, 2013.</p>



        <h2 id="理解和应用"   >
          <a href="#理解和应用" class="heading-link"><i class="fas fa-link"></i></a><a href="#理解和应用" class="headerlink" title="理解和应用"></a>理解和应用</h2>
      
        <h3 id="1、DataMining-（数据挖掘）"   >
          <a href="#1、DataMining-（数据挖掘）" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、DataMining-（数据挖掘）" class="headerlink" title="1、DataMining （数据挖掘）"></a>1、DataMining （数据挖掘）</h3>
      <p>做数据挖掘应用的一种重要算法，也是效果最好的分类算法之一。</p>
<p>举个例子，就是尽量把样本中的从更高纬度看起来在一起的样本合在一起，比如在一维（直线）空间里的样本从二维平面上可以分成不同类别，而在二维平面上分散的样本如果从第三维空间上来看就可以对他们做分类。</p>
<p>支持向量机算法目的是找出最优超平面，使分类间隔最大，要求不但正确分开，而且使分类间隔最大，在两类样本中离分类平面最近且位于平行于最优超平面的超平面上的点就是支持向量，为找到最优超平面只要找到所有支持向量即可。</p>
<p>对于非线性支持向量机，通常做法是把线性不可分转化成线性可分，通过一个非线性映射将输入到低维空间中的数据特性映射到高维线性特征空间中，在高维空间中求线性最优分类超平面。</p>

        <h3 id="2、scikit-learn-sklearn"   >
          <a href="#2、scikit-learn-sklearn" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、scikit-learn-sklearn" class="headerlink" title="2、scikit-learn (sklearn)"></a>2、scikit-learn (sklearn)</h3>
      <p>SVM 的基本原理基本上已经说的差不多了，下面咱们就来看看 SVM 在实际应用该如何使用了。幸运的是，在 python 下面，sklearn 提供了一个非常好用的机器学习算法，我们调用相关的包就好啦。</p>
<p><img src="/img/ml_map.png" alt="sklearn_map" title="sklearn"></p>

        <h2 id="小结"   >
          <a href="#小结" class="heading-link"><i class="fas fa-link"></i></a><a href="#小结" class="headerlink" title="小结"></a>小结</h2>
      <p>学习 SVM 需要有耐心，当初研究这个部分的时候，炼哥（github <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >jiangzhonglian</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>），法超大佬（github <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/geekidentity" >geekidentity</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>），羊三大佬（github <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/sheepmen" >sheepmen</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>），庭哥（github <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/wangyangting" >wangyangting</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>）都花费了好长时间，我只能躲在角落发抖….</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/06/ml_6.1/">第6章 支持向量机</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="/img/SVM_1.jpg" alt="支持向量机_首页"></p>

        <h2 id="支持向量机-概述"   >
          <a href="#支持向量机-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#支持向量机-概述" class="headerlink" title="支持向量机 概述"></a>支持向量机 概述</h2>
      <p>支持向量机(Support Vector Machines, SVM): 是一种监督学习算法。</p>
<ul>
<li>支持向量(Support Vector)就是离分隔超平面最近的那些点。</li>
<li>机(Machine)就是表示一种算法，而不是表示机器。</li>
</ul>

        <h2 id="支持向量机-场景"   >
          <a href="#支持向量机-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#支持向量机-场景" class="headerlink" title="支持向量机 场景"></a>支持向量机 场景</h2>
      <ul>
<li>要给左右两边的点进行分类</li>
<li>明显发现: 选择D会比B、C分隔的效果要好很多。</li>
</ul>
<p><img src="/img/SVM_3_linearly-separable.jpg" alt="线性可分"></p>

        <h2 id="支持向量机-原理"   >
          <a href="#支持向量机-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#支持向量机-原理" class="headerlink" title="支持向量机 原理"></a>支持向量机 原理</h2>
      
        <h3 id="SVM-工作原理"   >
          <a href="#SVM-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM-工作原理" class="headerlink" title="SVM 工作原理"></a>SVM 工作原理</h3>
      <p><img src="/img/k_2.jpg" alt="k_2" title="k_2"></p>
<p>对于上述的苹果和香蕉，我们想象为2种水果类型的炸弹。（保证距离最近的炸弹，距离它们最远）</p>
<ol>
<li>寻找最大分类间距</li>
<li>转而通过拉格朗日函数求优化的问题</li>
</ol>
<ul>
<li>数据可以通过画一条直线就可以将它们完全分开，这组数据叫<code>线性可分(linearly separable)</code>数据，而这条分隔直线称为<code>分隔超平面(separating hyperplane)</code>。</li>
<li>如果数据集上升到1024维呢？那么需要1023维来分隔数据集，也就说需要N-1维的对象来分隔，这个对象叫做<code>超平面(hyperlane)</code>，也就是分类的决策边界。</li>
</ul>
<p><img src="/img/SVM_2_separating-hyperplane.jpg" alt="分隔超平面"></p>

        <h3 id="寻找最大间隔"   >
          <a href="#寻找最大间隔" class="heading-link"><i class="fas fa-link"></i></a><a href="#寻找最大间隔" class="headerlink" title="寻找最大间隔"></a>寻找最大间隔</h3>
      
        <h4 id="为什么寻找最大间隔"   >
          <a href="#为什么寻找最大间隔" class="heading-link"><i class="fas fa-link"></i></a><a href="#为什么寻找最大间隔" class="headerlink" title="为什么寻找最大间隔"></a>为什么寻找最大间隔</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">摘录地址: http://slideplayer.com/slide/8610144  (第12条信息)</span><br><span class="line">Support Vector Machines: Slide 12 Copyright © 2001, 2003, Andrew W. Moore Why Maximum Margin? </span><br><span class="line"></span><br><span class="line">1.Intuitively this feels safest. </span><br><span class="line">2.If we’ve made a small error in the location of the boundary (it’s been jolted in its perpendicular direction) this gives us least chance of causing a misclassification. </span><br><span class="line">3.CV is easy since the model is immune to removal of any non-support-vector datapoints. </span><br><span class="line">4.There’s some theory that this is a good thing. </span><br><span class="line">5.Empirically it works very very well. </span><br><span class="line"></span><br><span class="line">* * *</span><br><span class="line"></span><br><span class="line">1. 直觉上是最安全的</span><br><span class="line">2. 如果我们在边界的位置发生了一个小错误（它在垂直方向上被颠倒），这给我们最小的可能导致错误分类。</span><br><span class="line">3. CV（cross validation 交叉验证）很容易，因为该模型对任何非支持向量数据点的去除是免疫的。</span><br><span class="line">4. 有一些理论表明这是一件好东西。</span><br><span class="line">5. 从经验角度上说它的效果非常非常好。</span><br></pre></td></tr></table></div></figure>


        <h4 id="怎么寻找最大间隔"   >
          <a href="#怎么寻找最大间隔" class="heading-link"><i class="fas fa-link"></i></a><a href="#怎么寻找最大间隔" class="headerlink" title="怎么寻找最大间隔"></a>怎么寻找最大间隔</h4>
      <blockquote>
<p>点到超平面的距离</p>
</blockquote>
<ul>
<li>分隔超平面<code>函数间距</code>:  $$y(x)=w^Tx+b$$</li>
<li>分类的结果:  $$f(x)=sign(w^Tx+b)$$  (sign表示&gt;0为1，&lt;0为-1，=0为0) </li>
<li>点到超平面的<code>几何间距</code>: $$d(x)=(w^Tx+b)/||w||$$  （||w||表示w矩阵的二范数=&gt; $$\sqrt{w^T*w}$$, 点到超平面的距离也是类似的）</li>
</ul>
<p><img src="img/SVM_4_point2line-distance.jpg" alt="点到直线的几何距离"></p>
<blockquote>
<p>拉格朗日乘子法</p>
</blockquote>
<ul>
<li>类别标签用-1、1，是为了后期方便 $$label*(w^Tx+b)$$ 的标识和距离计算；如果 $$label*(w^Tx+b)&gt;0$$ 表示预测正确，否则预测错误。</li>
<li>现在目标很明确，就是要找到<code>w</code>和<code>b</code>，因此我们必须要找到最小间隔的数据点，也就是前面所说的<code>支持向量</code>。<ul>
<li>也就说，让最小的距离取最大.(最小的距离: 就是最小间隔的数据点；最大: 就是最大间距，为了找出最优超平面–最终就是支持向量)</li>
<li>目标函数: $$arg: max_{w, b} \left( min[label*(w^Tx+b)]*\frac{1}{||w||} \right) $$<ol>
<li>如果 $$label*(w^Tx+b)&gt;0$$ 表示预测正确，也称<code>函数间隔</code>，$$||w||$$ 可以理解为归一化，也称<code>几何间隔</code>。</li>
<li>令 $$label*(w^Tx+b)&gt;=1$$， 因为0～1之间，得到的点是存在误判的可能性，所以要保障 $$min[label*(w^Tx+b)]=1$$，才能更好降低噪音数据影响。</li>
<li>所以本质上是求 $$arg: max_{w, b}  \frac{1}{||w||} $$；也就说，我们约束(前提)条件是: $$label*(w^Tx+b)=1$$</li>
</ol>
</li>
</ul>
</li>
<li>新的目标函数求解:  $$arg: max_{w, b}  \frac{1}{||w||} $$<ul>
<li>=&gt; 就是求: $$arg: min_{w, b} ||w|| $$ (求矩阵会比较麻烦，如果x只是 $$\frac{1}{2}*x^2$$ 的偏导数，那么。。同样是求最小值)</li>
<li>=&gt; 就是求: $$arg: min_{w, b} (\frac{1}{2}*||w||^2)$$ (二次函数求导，求极值，平方也方便计算)</li>
<li>本质上就是求线性不等式的二次优化问题(求分隔超平面，等价于求解相应的凸二次规划问题)</li>
</ul>
</li>
<li>通过拉格朗日乘子法，求二次优化问题<ul>
<li>假设需要求极值的目标函数 (objective function) 为 f(x,y)，限制条件为 φ(x,y)=M  # M=1</li>
<li>设g(x,y)=M-φ(x,y)   # 临时φ(x,y)表示下文中 $$label*(w^Tx+b)$$</li>
<li>定义一个新函数: F(x,y,λ)=f(x,y)+λg(x,y)</li>
<li>a为λ（a&gt;=0），代表要引入的拉格朗日乘子(Lagrange multiplier)</li>
<li>那么:  $$L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha_i * [1 - label * (w^Tx+b)]$$</li>
<li>因为: $$label*(w^Tx+b)&gt;=1, \alpha&gt;=0$$ , 所以 $$\alpha*[1-label*(w^Tx+b)]&lt;=0$$ , $$\sum_{i=1}^{n} \alpha_i * [1-label*(w^Tx+b)]&lt;=0$$ </li>
<li>当 $$label*(w^Tx+b)&gt;1$$ 则 $$\alpha=0$$ ，表示该点为<font color=red>非支持向量</font></li>
<li>相当于求解:  $$max_{\alpha} L(w,b,\alpha) = \frac{1}{2} *||w||^2$$ </li>
<li>如果求:  $$min_{w, b} \frac{1}{2} *||w||^2$$ , 也就是要求:  $$min_{w, b} \left( max_{\alpha} L(w,b,\alpha)\right)$$ </li>
</ul>
</li>
<li>现在转化到对偶问题的求解<ul>
<li>$$min_{w, b} \left(max_{\alpha} L(w,b,\alpha) \right) $$ &gt;= $$max_{\alpha} \left(min_{w, b}\ L(w,b,\alpha) \right) $$ </li>
<li>现在分2步</li>
<li>先求:  $$min_{w, b} L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha_i * [1 - label * (w^Tx+b)]$$</li>
<li>就是求<code>L(w,b,a)</code>关于[w, b]的偏导数, 得到<code>w和b的值</code>，并化简为: <code>L和a的方程</code>。</li>
<li>参考:  如果公式推导还是不懂，也可以参考《统计学习方法》李航-P103&lt;学习的对偶算法&gt;<br><img src="/img/SVM_5_Lagrangemultiplier.png" alt="计算拉格朗日函数的对偶函数"></li>
</ul>
</li>
<li>终于得到课本上的公式:  $$max_{\alpha} \left( \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i, j=1}^{m} label_i \ast label_j \ast \alpha_i \ast \alpha_j \ast &lt;x_i, x_j&gt; \right) $$</li>
<li>约束条件:  $$a&gt;=0$$ 并且 $$\sum_{i=1}^{m} a_i \ast label_i=0$$</li>
</ul>
<blockquote>
<p>松弛变量(slack variable)</p>
</blockquote>
<p>参考地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://blog.csdn.net/wusecaiyun/article/details/49659183" >http://blog.csdn.net/wusecaiyun/article/details/49659183</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p><img src="/img/SVM_%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F.jpg" alt="松弛变量公式"></p>
<ul>
<li>我们知道几乎所有的数据都不那么干净, 通过引入松弛变量来 <code>允许数据点可以处于分隔面错误的一侧</code>。</li>
<li>约束条件:  $$C&gt;=a&gt;=0$$ 并且 $$\sum_{i=1}^{m} a_i \ast label_i=0$$</li>
<li>总的来说: <ul>
<li><img src="/img/%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F.png" alt="松弛变量"> 表示 <code>松弛变量</code></li>
<li>常量C是 <code>惩罚因子</code>, 表示离群点的权重（用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0” ）<ul>
<li>$$label*(w^Tx+b) &gt; 1$$ and alpha = 0 (在边界外，就是非支持向量)</li>
<li>$$label*(w^Tx+b) = 1$$ and 0&lt; alpha &lt; C (在分割超平面上，就支持向量)</li>
<li>$$label*(w^Tx+b) &lt; 1$$ and alpha = C (在分割超平面内，是误差点 -&gt; C表示它该受到的惩罚因子程度)</li>
<li>参考地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/question/48351234/answer/110486455" >https://www.zhihu.com/question/48351234/answer/110486455</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</li>
<li>C值越大，表示离群点影响越大，就越容易过度拟合；反之有可能欠拟合。</li>
<li>我们看到，目标函数控制了离群点的数目和程度，使大部分样本点仍然遵守限制条件。</li>
<li>例如: 正类有10000个样本，而负类只给了100个（C越大表示100个负样本的影响越大，就会出现过度拟合，所以C决定了负样本对模型拟合程度的影响！，C就是一个非常关键的优化点！）</li>
</ul>
</li>
<li>这一结论十分直接，SVM中的主要工作就是要求解 alpha.</li>
</ul>

        <h3 id="SMO-高效优化算法"   >
          <a href="#SMO-高效优化算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#SMO-高效优化算法" class="headerlink" title="SMO 高效优化算法"></a>SMO 高效优化算法</h3>
      <ul>
<li>SVM有很多种实现，最流行的一种实现是:  <code>序列最小优化(Sequential Minimal Optimization, SMO)算法</code>。</li>
<li>下面还会介绍一种称为 <code>核函数(kernel)</code> 的方式将SVM扩展到更多数据集上。</li>
<li>注意: <code>SVM几何含义比较直观，但其算法实现较复杂，牵扯大量数学公式的推导。</code></li>
</ul>
<blockquote>
<p>序列最小优化(Sequential Minimal Optimization, SMO)</p>
</blockquote>
<ul>
<li>创建作者: John Platt</li>
<li>创建时间: 1996年</li>
<li>SMO用途: 用于训练 SVM</li>
<li>SMO目标: 求出一系列 alpha 和 b,一旦求出 alpha，就很容易计算出权重向量 w 并得到分隔超平面。</li>
<li>SMO思想: 是将大优化问题分解为多个小优化问题来求解的。</li>
<li>SMO原理: 每次循环选择两个 alpha 进行优化处理，一旦找出一对合适的 alpha，那么就增大一个同时减少一个。<ul>
<li>这里指的合适必须要符合一定的条件<ol>
<li>这两个 alpha 必须要在间隔边界之外</li>
<li>这两个 alpha 还没有进行过区间化处理或者不在边界上。</li>
</ol>
</li>
<li>之所以要同时改变2个 alpha；原因是我们有一个约束条件:  $$\sum_{i=1}^{m} a_i \ast label_i=0$$；如果只是修改一个 alpha，很可能导致约束条件失效。</li>
</ul>
</li>
</ul>
<blockquote>
<p>SMO 伪代码大致如下: </p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">创建一个 alpha 向量并将其初始化为0向量</span><br><span class="line">当迭代次数小于最大迭代次数时(外循环)</span><br><span class="line">    对数据集中的每个数据向量(内循环): </span><br><span class="line">        如果该数据向量可以被优化</span><br><span class="line">            随机选择另外一个数据向量</span><br><span class="line">            同时优化这两个向量</span><br><span class="line">            如果两个向量都不能被优化，退出内循环</span><br><span class="line">    如果所有向量都没被优化，增加迭代数目，继续下一次循环</span><br></pre></td></tr></table></div></figure>


        <h3 id="SVM-开发流程"   >
          <a href="#SVM-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM-开发流程" class="headerlink" title="SVM 开发流程"></a>SVM 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任意方法。</span><br><span class="line">准备数据: 需要数值型数据。</span><br><span class="line">分析数据: 有助于可视化分隔超平面。</span><br><span class="line">训练算法: SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。</span><br><span class="line">测试算法: 十分简单的计算过程就可以实现。</span><br><span class="line">使用算法: 几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。</span><br></pre></td></tr></table></div></figure>


        <h3 id="SVM-算法特点"   >
          <a href="#SVM-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM-算法特点" class="headerlink" title="SVM 算法特点"></a>SVM 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 泛化（由具体的、个别的扩大为一般的，就是说: 模型训练完后的新样本）错误率低，计算开销不大，结果易理解。</span><br><span class="line">缺点: 对参数调节和核函数的选择敏感，原始分类器不加修改仅适合于处理二分类问题。</span><br><span class="line">使用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="课本案例（无核函数）"   >
          <a href="#课本案例（无核函数）" class="heading-link"><i class="fas fa-link"></i></a><a href="#课本案例（无核函数）" class="headerlink" title="课本案例（无核函数）"></a>课本案例（无核函数）</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>对小规模数据点进行分类</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <blockquote>
<p>收集数据</p>
</blockquote>
<p>文本文件格式: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.542485</span>	<span class="number">1.977398</span>	-<span class="number">1</span></span><br><span class="line"><span class="number">3.018896</span>	<span class="number">2.556416</span>	-<span class="number">1</span></span><br><span class="line"><span class="number">7.551510</span>	-<span class="number">1.580030</span>	<span class="number">1</span></span><br><span class="line"><span class="number">2.114999</span>	-<span class="number">0.004466</span>	-<span class="number">1</span></span><br><span class="line"><span class="number">8.127113</span>	<span class="number">1.274372</span>	<span class="number">1</span></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对文件进行逐行解析，从而得到第行的类标签和整个特征矩阵</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        fileName 文件名</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataMat  特征矩阵</span></span><br><span class="line"><span class="string">        labelMat 类标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 无</p>
</blockquote>
<blockquote>
<p>训练算法</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoSimple</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;smoSimple</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMatIn    特征集合</span></span><br><span class="line"><span class="string">        classLabels  类别标签</span></span><br><span class="line"><span class="string">        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。</span></span><br><span class="line"><span class="string">            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。</span></span><br><span class="line"><span class="string">            可以通过调节该参数达到不同的结果。</span></span><br><span class="line"><span class="string">        toler   容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）</span></span><br><span class="line"><span class="string">        maxIter 退出前最大的循环次数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        b       模型的常量值</span></span><br><span class="line"><span class="string">        alphas  拉格朗日乘子</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)</span><br><span class="line">    <span class="comment"># 矩阵转置 和 .T 一样的功能</span></span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 b和alphas(alpha有点类似权重值。)</span></span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    alphas = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 没有任何alpha改变的情况下遍历数据的次数</span></span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">iter</span> &lt; maxIter):</span><br><span class="line">        <span class="comment"># w = calcWs(alphas, dataMatIn, classLabels)</span></span><br><span class="line">        <span class="comment"># print(&quot;w:&quot;, w)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录alpha是否已经进行优化，每次循环时设为0，然后再对整个集合顺序遍历</span></span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="comment"># print &#x27;alphas=&#x27;, alphas</span></span><br><span class="line">            <span class="comment"># print &#x27;labelMat=&#x27;, labelMat</span></span><br><span class="line">            <span class="comment"># print &#x27;multiply(alphas, labelMat)=&#x27;, multiply(alphas, labelMat)</span></span><br><span class="line">            <span class="comment"># 我们预测的类别 y[i] = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*label[n]*x[n]</span></span><br><span class="line">            fXi = <span class="built_in">float</span>(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b</span><br><span class="line">            <span class="comment"># 预测结果与真实结果比对，计算误差Ei</span></span><br><span class="line">            Ei = fXi - <span class="built_in">float</span>(labelMat[i])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值)</span></span><br><span class="line">            <span class="comment"># 0&lt;=alphas[i]&lt;=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。</span></span><br><span class="line">            <span class="comment"># 表示发生错误的概率: labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            # 检验训练样本(xi, yi)是否满足KKT条件</span></span><br><span class="line"><span class="string">            yi*f(i) &gt;= 1 and alpha = 0 (outside the boundary)</span></span><br><span class="line"><span class="string">            yi*f(i) == 1 and 0&lt;alpha&lt; C (on the boundary)</span></span><br><span class="line"><span class="string">            yi*f(i) &lt;= 1 and alpha = C (between the boundary)</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> ((labelMat[i]*Ei &lt; -toler) <span class="keyword">and</span> (alphas[i] &lt; C)) <span class="keyword">or</span> ((labelMat[i]*Ei &gt; toler) <span class="keyword">and</span> (alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果满足优化的条件，我们就随机选取非i的一个点，进行优化比较</span></span><br><span class="line">                j = selectJrand(i, m)</span><br><span class="line">                <span class="comment"># 预测j的结果</span></span><br><span class="line">                fXj = <span class="built_in">float</span>(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b</span><br><span class="line">                Ej = fXj - <span class="built_in">float</span>(labelMat[j])</span><br><span class="line">                alphaIold = alphas[i].copy()</span><br><span class="line">                alphaJold = alphas[j].copy()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句</span></span><br><span class="line">                <span class="comment"># labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。</span></span><br><span class="line">                <span class="keyword">if</span> (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] - alphas[i])</span><br><span class="line">                    H = <span class="built_in">min</span>(C, C + alphas[j] - alphas[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = <span class="built_in">min</span>(C, alphas[j] + alphas[i])</span><br><span class="line">                <span class="comment"># 如果相同，就没法优化了</span></span><br><span class="line">                <span class="keyword">if</span> L == H:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;L==H&quot;</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程</span></span><br><span class="line">                <span class="comment"># 参考《统计学习方法》李航-P125~P128&lt;序列最小最优化算法&gt;</span></span><br><span class="line">                eta = <span class="number">2.0</span> * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T</span><br><span class="line">                <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;eta&gt;=0&quot;</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 计算出一个新的alphas[j]值</span></span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                <span class="comment"># 并使用辅助函数，以及L和H对其进行调整</span></span><br><span class="line">                alphas[j] = clipAlpha(alphas[j], H, L)</span><br><span class="line">                <span class="comment"># 检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">abs</span>(alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>):</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;j not moving enough&quot;</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反</span></span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])</span><br><span class="line">                <span class="comment"># 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。</span></span><br><span class="line">                <span class="comment"># w= Σ[1~n] ai*yi*xi =&gt; b = yj- Σ[1~n] ai*yi(xi*xj)</span></span><br><span class="line">                <span class="comment"># 所以:   b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)</span></span><br><span class="line">                <span class="comment"># 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍</span></span><br><span class="line">                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T</span><br><span class="line">                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T</span><br><span class="line">                <span class="keyword">if</span> (<span class="number">0</span> &lt; alphas[i]) <span class="keyword">and</span> (C &gt; alphas[i]):</span><br><span class="line">                    b = b1</span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">0</span> &lt; alphas[j]) <span class="keyword">and</span> (C &gt; alphas[j]):</span><br><span class="line">                    b = b2</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">                alphaPairsChanged += <span class="number">1</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;iter: %d i:%d, pairs changed %d&quot;</span> % (<span class="built_in">iter</span>, i, alphaPairsChanged))</span><br><span class="line">        <span class="comment"># 在for循环外，检查alpha值是否做了更新，如果更新则将iter设为0后继续运行程序</span></span><br><span class="line">        <span class="comment"># 直到更新完毕后，iter次循环无变化，才退出循环。</span></span><br><span class="line">        <span class="keyword">if</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;iteration number: %d&quot;</span> % <span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">return</span> b, alphas</span><br></pre></td></tr></table></div></figure>

<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-simple.py" >完整代码地址: SVM简化版，应用简化版SMO算法处理小规模数据集</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-simple.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-simple.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py" >完整代码地址: SVM完整版，使用完整 Platt SMO算法加速优化，优化点: 选择alpha的方式不同</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="核函数-kernel-使用"   >
          <a href="#核函数-kernel-使用" class="heading-link"><i class="fas fa-link"></i></a><a href="#核函数-kernel-使用" class="headerlink" title="核函数(kernel) 使用"></a>核函数(kernel) 使用</h2>
      <ul>
<li>对于线性可分的情况，效果明显</li>
<li>对于非线性的情况也一样，此时需要用到一种叫<code>核函数(kernel)</code>的工具将数据转化为分类器易于理解的形式。</li>
</ul>
<blockquote>
<p>利用核函数将数据映射到高维空间</p>
</blockquote>
<ul>
<li>使用核函数: 可以将数据从某个特征空间到另一个特征空间的映射。（通常情况下: 这种映射会将低维特征空间映射到高维空间。）</li>
<li>如果觉得特征空间很装逼、很难理解。</li>
<li>可以把核函数想象成一个包装器(wrapper)或者是接口(interface)，它能将数据从某个很难处理的形式转换成为另一个较容易处理的形式。</li>
<li>经过空间转换后: 低维需要解决的非线性问题，就变成了高维需要解决的线性问题。</li>
<li>SVM 优化特别好的地方，在于所有的运算都可以写成内积(inner product: 是指2个向量相乘，得到单个标量 或者 数值)；内积替换成核函数的方式被称为<code>核技巧(kernel trick)</code>或者<code>核&quot;变电&quot;(kernel substation)</code></li>
<li>核函数并不仅仅应用于支持向量机，很多其他的机器学习算法也都用到核函数。最流行的核函数: 径向基函数(radial basis function)</li>
<li>径向基函数的高斯版本，其具体的公式为: </li>
</ul>
<p><img src="/img/SVM_6_radial-basis-function.jpg" alt="径向基函数的高斯版本"></p>

        <h3 id="项目案例-手写数字识别的优化（有核函数）"   >
          <a href="#项目案例-手写数字识别的优化（有核函数）" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-手写数字识别的优化（有核函数）" class="headerlink" title="项目案例: 手写数字识别的优化（有核函数）"></a>项目案例: 手写数字识别的优化（有核函数）</h3>
      
        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">你的老板要求: 你写的那个手写识别程序非常好，但是它占用内存太大。顾客无法通过无线的方式下载我们的应用。</span><br><span class="line">所以: 我们可以考虑使用支持向量机，保留支持向量就行（knn需要保留所有的向量），就可以获得非常好的效果。</span><br></pre></td></tr></table></div></figure>


        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <blockquote>
<p>收集数据: 提供的文本文件</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">00000000000000001111000000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000011111111100000000</span><br><span class="line">00000000000000011111111110000000</span><br><span class="line">00000000000000011111111110000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000001111111111100000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000000111111111100000000</span><br><span class="line">00000000000000111111111000000000</span><br><span class="line">00000000000001111111111000000000</span><br><span class="line">00000000000011111111111000000000</span><br><span class="line">00000000000111111111110000000000</span><br><span class="line">00000000001111111111111000000000</span><br><span class="line">00000001111111111111111000000000</span><br><span class="line">00000011111111111111110000000000</span><br><span class="line">00000111111111111111110000000000</span><br><span class="line">00000111111111111111110000000000</span><br><span class="line">00000001111111111111110000000000</span><br><span class="line">00000001111111011111110000000000</span><br><span class="line">00000000111100011111110000000000</span><br><span class="line">00000000000000011111110000000000</span><br><span class="line">00000000000000011111100000000000</span><br><span class="line">00000000000000111111110000000000</span><br><span class="line">00000000000000011111110000000000</span><br><span class="line">00000000000000011111110000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000011111111000000000</span><br><span class="line">00000000000000000111111110000000</span><br><span class="line">00000000000000000111111100000000</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 基于二值图像构造向量</p>
</blockquote>
<p><code>将 32*32的文本转化为 1*1024的矩阵</code></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span>(<span class="params">filename</span>):</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line">    fr = <span class="built_in">open</span>(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>, <span class="number">32</span> * i + j] = <span class="built_in">int</span>(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadImages</span>(<span class="params">dirName</span>):</span></span><br><span class="line">    <span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line">    hwLabels = []</span><br><span class="line">    <span class="built_in">print</span>(dirName)</span><br><span class="line">    trainingFileList = listdir(dirName)  <span class="comment"># load the training set</span></span><br><span class="line">    m = <span class="built_in">len</span>(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = <span class="built_in">int</span>(fileStr.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> classNumStr == <span class="number">9</span>:</span><br><span class="line">            hwLabels.append(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hwLabels.append(<span class="number">1</span>)</span><br><span class="line">        trainingMat[i, :] = img2vector(<span class="string">&#x27;%s/%s&#x27;</span> % (dirName, fileNameStr))</span><br><span class="line">    <span class="keyword">return</span> trainingMat, hwLabels</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 对图像向量进行目测</p>
</blockquote>
<blockquote>
<p>训练算法: 采用两种不同的核函数，并对径向基核函数采用不同的设置来运行SMO算法</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTrans</span>(<span class="params">X, A, kTup</span>):</span>  <span class="comment"># calc the kernel or transform data to a higher dimensional space</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    核转换函数</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X     dataMatIn数据集</span></span><br><span class="line"><span class="string">        A     dataMatIn数据集的第i行的数据</span></span><br><span class="line"><span class="string">        kTup  核函数的信息</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    K = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">if</span> kTup[<span class="number">0</span>] == <span class="string">&#x27;lin&#x27;</span>:</span><br><span class="line">        <span class="comment"># linear kernel:   m*n * n*1 = m*1</span></span><br><span class="line">        K = X * A.T</span><br><span class="line">    <span class="keyword">elif</span> kTup[<span class="number">0</span>] == <span class="string">&#x27;rbf&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            deltaRow = X[j, :] - A</span><br><span class="line">            K[j] = deltaRow * deltaRow.T</span><br><span class="line">        <span class="comment"># 径向基函数的高斯版本</span></span><br><span class="line">        K = exp(K / (-<span class="number">1</span> * kTup[<span class="number">1</span>] ** <span class="number">2</span>))  <span class="comment"># divide in NumPy is element-wise not matrix like Matlab</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">&#x27;Houston We Have a Problem -- That Kernel is not recognized&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoP</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter, kTup=(<span class="params"><span class="string">&#x27;lin&#x27;</span>, <span class="number">0</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    完整SMO算法外循环，与smoSimple有些类似，但这里的循环退出条件更多一些</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMatIn    数据集</span></span><br><span class="line"><span class="string">        classLabels  类别标签</span></span><br><span class="line"><span class="string">        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。</span></span><br><span class="line"><span class="string">            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。</span></span><br><span class="line"><span class="string">            可以通过调节该参数达到不同的结果。</span></span><br><span class="line"><span class="string">        toler   容错率</span></span><br><span class="line"><span class="string">        maxIter 退出前最大的循环次数</span></span><br><span class="line"><span class="string">        kTup    包含核函数信息的元组</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        b       模型的常量值</span></span><br><span class="line"><span class="string">        alphas  拉格朗日乘子</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个 optStruct 对象</span></span><br><span class="line">    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)</span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    entireSet = <span class="literal">True</span></span><br><span class="line">    alphaPairsChanged = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环遍历: 循环maxIter次 并且 （alphaPairsChanged存在可以改变 or 所有行遍历一遍）</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">iter</span> &lt; maxIter) <span class="keyword">and</span> ((alphaPairsChanged &gt; <span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  当entireSet=true or 非边界alpha对没有了；就开始寻找 alpha对，然后决定是否要进行else。</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            <span class="comment"># 在数据集上遍历所有可能的alpha</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(oS.m):</span><br><span class="line">                <span class="comment"># 是否存在alpha对，存在就+1</span></span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="comment"># print(&quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span></span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对已存在 alpha对，选出非边界的alpha值，进行优化。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 遍历所有的非边界alpha值，也就是不在边界0或C上的值。</span></span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; <span class="number">0</span>) * (oS.alphas.A &lt; C))[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="comment"># print(&quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter, i, alphaPairsChanged))</span></span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            entireSet = <span class="literal">False</span>  <span class="comment"># toggle entire set loop</span></span><br><span class="line">        <span class="keyword">elif</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            entireSet = <span class="literal">True</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;iteration number: %d&quot;</span> % <span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">return</span> oS.b, oS.alphas</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 便携一个函数来测试不同的和函数并计算错误率</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testDigits</span>(<span class="params">kTup=(<span class="params"><span class="string">&#x27;rbf&#x27;</span>, <span class="number">10</span></span>)</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 导入训练数据</span></span><br><span class="line">    dataArr, labelArr = loadImages(<span class="string">&#x27;data/6.SVM/trainingDigits&#x27;</span>)</span><br><span class="line">    b, alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">10000</span>, kTup)</span><br><span class="line">    datMat = mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd = nonzero(alphas.A &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    sVs = datMat[svInd]</span><br><span class="line">    labelSV = labelMat[svInd]</span><br><span class="line">    <span class="comment"># print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])</span></span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)</span><br><span class="line">        <span class="comment"># 1*m * m*1 = 1*1 单个预测结果</span></span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]): errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the training error rate is: %f&quot;</span> % (<span class="built_in">float</span>(errorCount) / m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 导入测试数据</span></span><br><span class="line">    dataArr, labelArr = loadImages(<span class="string">&#x27;data/6.SVM/testDigits&#x27;</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    datMat = mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    m, n = shape(datMat)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)</span><br><span class="line">        <span class="comment"># 1*m * m*1 = 1*1 单个预测结果</span></span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]): errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the test error rate is: %f&quot;</span> % (<span class="built_in">float</span>(errorCount) / m))</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 一个图像识别的完整应用还需要一些图像处理的知识，这里并不打算深入介绍</p>
</blockquote>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/6.SVM/svm-complete.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~houfachao" >geekidentity</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/05/ml_5/">第5章 Logistic回归</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-05</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="/img/LogisticRegression_headPage_xy.png" alt="朴素贝叶斯_首页" title="Logistic回归首页"></p>

        <h2 id="Logistic-回归-概述"   >
          <a href="#Logistic-回归-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-回归-概述" class="headerlink" title="Logistic 回归 概述"></a>Logistic 回归 概述</h2>
      <p><code>Logistic 回归 或者叫逻辑回归 虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。</code></p>

        <h2 id="须知概念"   >
          <a href="#须知概念" class="heading-link"><i class="fas fa-link"></i></a><a href="#须知概念" class="headerlink" title="须知概念"></a>须知概念</h2>
      
        <h3 id="Sigmoid-函数"   >
          <a href="#Sigmoid-函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h3>
      
        <h4 id="回归-概念"   >
          <a href="#回归-概念" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归-概念" class="headerlink" title="回归 概念"></a>回归 概念</h4>
      <p>假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），这个拟合的过程就叫做回归。进而可以得到对这些点的拟合直线方程，那么我们根据这个回归方程，怎么进行分类呢？请看下面。</p>

        <h4 id="二值型输出分类函数"   >
          <a href="#二值型输出分类函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#二值型输出分类函数" class="headerlink" title="二值型输出分类函数"></a>二值型输出分类函数</h4>
      <p>我们想要的函数应该是: 能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出 0 或 1.或许你之前接触过具有这种性质的函数，该函数称为 <code>海维塞得阶跃函数(Heaviside step function)</code>，或者直接称为 <code>单位阶跃函数</code>。然而，海维塞得阶跃函数的问题在于: 该函数在跳跃点上从 0 瞬间跳跃到 1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质（可以输出 0 或者 1 的性质），且数学上更易处理，这就是 Sigmoid 函数。 Sigmoid 函数具体的计算公式如下: </p>
<p><img src="/img/LR_1.png" alt="Sigmoid 函数计算公式"></p>
<p>下图给出了 Sigmoid 函数在不同坐标尺度下的两条曲线图。当 x 为 0 时，Sigmoid 函数值为 0.5 。随着 x 的增大，对应的 Sigmoid 值将逼近于 1 ; 而随着 x 的减小， Sigmoid 值将逼近于 0 。如果横坐标刻度足够大， Sigmoid 函数看起来很像一个阶跃函数。</p>
<p><img src="/img/LR_3.png" alt="Sigmoid 函数在不同坐标下的图片"></p>
<p>因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数（如下公式所示），然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以，Logistic 回归也是一种概率估计，比如这里Sigmoid 函数得出的值为0.5，可以理解为给定数据和参数，数据被分入 1 类的概率为0.5。想对Sigmoid 函数有更多了解，可以点开<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.desmos.com/calculator/bgontvxotm" >此链接</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>跟此函数互动。</p>

        <h3 id="基于最优化方法的回归系数确定"   >
          <a href="#基于最优化方法的回归系数确定" class="heading-link"><i class="fas fa-link"></i></a><a href="#基于最优化方法的回归系数确定" class="headerlink" title="基于最优化方法的回归系数确定"></a>基于最优化方法的回归系数确定</h3>
      <p>Sigmoid 函数的输入记为 z ，由下面公式得到: </p>
<p><img src="/img/LR_2.png" alt="Sigmoid 函数计算公式"></p>
<p>如果采用向量的写法，上述公式可以写成 <img src="/img/LR_4.png" alt="Sigmoid 函数计算公式向量形式"> ，它表示将这两个数值向量对应元素相乘然后全部加起来即得到 z 值。其中的向量 x 是分类器的输入数据，向量 w 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是——梯度上升法（Gradient Ascent）。</p>

        <h3 id="梯度上升法"   >
          <a href="#梯度上升法" class="heading-link"><i class="fas fa-link"></i></a><a href="#梯度上升法" class="headerlink" title="梯度上升法"></a>梯度上升法</h3>
      
        <h4 id="梯度的介绍"   >
          <a href="#梯度的介绍" class="heading-link"><i class="fas fa-link"></i></a><a href="#梯度的介绍" class="headerlink" title="梯度的介绍"></a>梯度的介绍</h4>
      <p>需要一点点向量方面的数学知识</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">向量 = 值 + 方向  </span><br><span class="line">梯度 = 向量</span><br><span class="line">梯度 = 梯度值 + 梯度方向</span><br></pre></td></tr></table></div></figure>


        <h4 id="梯度上升法的思想"   >
          <a href="#梯度上升法的思想" class="heading-link"><i class="fas fa-link"></i></a><a href="#梯度上升法的思想" class="headerlink" title="梯度上升法的思想"></a>梯度上升法的思想</h4>
      <p>要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示: </p>
<p><img src="/img/LR_5.png" alt="梯度上升计算公式"></p>
<p>这个梯度意味着要沿 x 的方向移动 <img src="img/LR_6.png" alt="f(x, y)对x求偏导"> ，沿 y 的方向移动 <img src="img/LR_7.png" alt="f(x, y)对y求偏导"> 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。</p>
<p><img src="/img/LR_8.png" alt="梯度上升"></p>
<p>上图展示的，梯度上升算法到达每个点后都会重新估计移动的方向。从 P0 开始，计算完该点的梯度，函数就根据梯度移动到下一点 P1。在 P1 点，梯度再次被重新计算，并沿着新的梯度方向移动到 P2 。如此循环迭代，直到满足停止条件。迭代过程中，梯度算子总是保证我们能选取到最佳的移动方向。</p>
<p>上图中的梯度上升算法沿梯度方向移动了一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作 α 。用向量来表示的话，梯度上升算法的迭代公式如下: </p>
<p><img src="/img/LR_9.png" alt="梯度上升迭代公式"></p>
<p>该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。</p>
<p>介绍一下几个相关的概念: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例如: y = w0 + w1x1 + w2x2 + ... + wnxn</span><br><span class="line">梯度: 参考上图的例子，二维图像，x方向代表第一个系数，也就是 w1，y方向代表第二个系数也就是 w2，这样的向量就是梯度。</span><br><span class="line">α: 上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长（step length）。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。</span><br><span class="line">步长通俗的理解，100米，如果我一步走10米，我需要走10步；如果一步走20米，我只需要走5步。这里的一步走多少米就是步长的意思。</span><br><span class="line">▽f(w): 代表沿着梯度变化的方向。</span><br></pre></td></tr></table></div></figure>


<p>   问: 有人会好奇为什么有些书籍上说的是梯度下降法（Gradient Decent）?</p>
<p>   答:  其实这个两个方法在此情况下本质上是相同的。关键在于代价函数（cost function）或者叫目标函数（objective function）。如果目标函数是损失函数，那就是最小化损失函数来求函数的最小值，就用梯度下降。 如果目标函数是似然函数（Likelihood function），就是要最大化似然函数来求函数的最大值，那就用梯度上升。在逻辑回归中， 损失函数和似然函数无非就是互为正负关系。</p>
<p>   只需要在迭代公式中的加法变成减法。因此，对应的公式可以写成</p>
<p><img src="/img/LR_10.png" alt="梯度下降迭代公式"></p>
<p><strong>局部最优现象 （Local Optima）</strong></p>
<p><img src="/img/LR_20.png" alt="梯度下降图_4"></p>
<p>上图表示参数 θ 与误差函数 J(θ) 的关系图 (这里的误差函数是损失函数，所以我们要最小化损失函数)，红色的部分是表示 J(θ) 有着比较高的取值，我们需要的是，能够让 J(θ) 的值尽量的低。也就是深蓝色的部分。θ0，θ1 表示 θ 向量的两个维度（此处的θ0，θ1是x0和x1的系数，也对应的是上文w0和w1）。</p>
<p>可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，如我们上图中的右边的梯度下降曲线，描述的是最终到达一个局部最小点，这是我们重新选择了一个初始点得到的。</p>
<p>看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。</p>

        <h2 id="Logistic-回归-原理"   >
          <a href="#Logistic-回归-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-回归-原理" class="headerlink" title="Logistic 回归 原理"></a>Logistic 回归 原理</h2>
      
        <h3 id="Logistic-回归-工作原理"   >
          <a href="#Logistic-回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-回归-工作原理" class="headerlink" title="Logistic 回归 工作原理"></a>Logistic 回归 工作原理</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每个回归系数初始化为 1</span><br><span class="line">重复 R 次:</span><br><span class="line">    计算整个数据集的梯度</span><br><span class="line">    使用 步长 x 梯度 更新回归系数的向量</span><br><span class="line">返回回归系数</span><br></pre></td></tr></table></div></figure>


        <h3 id="Logistic-回归-开发流程"   >
          <a href="#Logistic-回归-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-回归-开发流程" class="headerlink" title="Logistic 回归 开发流程"></a>Logistic 回归 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。</span><br><span class="line">分析数据: 采用任意方法对数据进行分析。</span><br><span class="line">训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。</span><br><span class="line">测试算法: 一旦训练步骤完成，分类将会很快。</span><br><span class="line">使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。</span><br></pre></td></tr></table></div></figure>


        <h3 id="Logistic-回归-算法特点"   >
          <a href="#Logistic-回归-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-回归-算法特点" class="headerlink" title="Logistic 回归 算法特点"></a>Logistic 回归 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 计算代价不高，易于理解和实现。</span><br><span class="line">缺点: 容易欠拟合，分类精度可能不高。</span><br><span class="line">适用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="附加-方向导数与梯度"   >
          <a href="#附加-方向导数与梯度" class="heading-link"><i class="fas fa-link"></i></a><a href="#附加-方向导数与梯度" class="headerlink" title="附加 方向导数与梯度"></a>附加 方向导数与梯度</h3>
      <p><img src="/img/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.png" alt="方向导数与梯度"></p>

        <h2 id="Logistic-回归-项目案例"   >
          <a href="#Logistic-回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-回归-项目案例" class="headerlink" title="Logistic 回归 项目案例"></a>Logistic 回归 项目案例</h2>
      
        <h3 id="项目案例1-使用-Logistic-回归在简单数据集上的分类"   >
          <a href="#项目案例1-使用-Logistic-回归在简单数据集上的分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-使用-Logistic-回归在简单数据集上的分类" class="headerlink" title="项目案例1: 使用 Logistic 回归在简单数据集上的分类"></a>项目案例1: 使用 Logistic 回归在简单数据集上的分类</h3>
      <p><a href="/src/py2.x/ml/5.Logistic/logistic.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>在一个简单的数据集上，采用梯度上升法找到 Logistic 回归分类器在此数据集上的最佳回归系数</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法</span><br><span class="line">准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳</span><br><span class="line">分析数据: 画出决策边界</span><br><span class="line">训练算法: 使用梯度上升找到最佳参数</span><br><span class="line">测试算法: 使用 Logistic 回归进行分类</span><br><span class="line">使用算法: 对简单数据集中数据进行分类</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>我们采用存储在 TestSet.txt 文本文件中的数据，存储格式如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-0.017612	14.053064	0</span><br><span class="line">-1.395634	4.662541	1</span><br><span class="line">-0.752157	6.538620	0</span><br><span class="line">-1.322371	7.152853	0</span><br><span class="line">0.423363	11.054677	0</span><br></pre></td></tr></table></div></figure>

<p>绘制在图中，如下图所示: </p>
<p><img src="/img/LR_11.png" alt="简单数据集绘制在图上"></p>
<blockquote>
<p>准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解析数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">file_name</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc: </span></span><br><span class="line"><span class="string">        加载并解析数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file_name -- 要解析的文件路径</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataMat -- 原始数据的特征</span></span><br><span class="line"><span class="string">        labelMat -- 原始数据的标签，也就是每条样本对应的类别。即目标向量</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># dataMat为原始数据， labelMat为原始数据的标签</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(file_name)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split()</span><br><span class="line">        <span class="comment"># 为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0</span></span><br><span class="line">        dataMat.append([<span class="number">1.0</span>, <span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(<span class="built_in">int</span>(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 采用任意方法对数据进行分析，此处不需要</p>
</blockquote>
<blockquote>
<p>训练算法: 使用梯度上升找到最佳参数</p>
</blockquote>
<p>定义sigmoid阶跃函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid阶跃函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">inX</span>):</span></span><br><span class="line">    <span class="comment"># return 1.0 / (1 + exp(-inX))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tanh是Sigmoid的变形，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * <span class="number">1.0</span>/(<span class="number">1</span>+exp(-<span class="number">2</span>*inX)) - <span class="number">1</span></span><br></pre></td></tr></table></div></figure>

<p>Logistic 回归梯度上升优化算法</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正常的处理方案</span></span><br><span class="line"><span class="comment"># 两个参数: 第一个参数==&gt; dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。</span></span><br><span class="line"><span class="comment"># 第二个参数==&gt; classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span>(<span class="params">dataMatIn, classLabels</span>):</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[1,1,2],[1,1,2]....]</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)             <span class="comment"># 转换为 NumPy 矩阵</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]</span></span><br><span class="line">    <span class="comment"># transpose() 行列转置函数</span></span><br><span class="line">    <span class="comment"># 将行向量转化为列向量   =&gt;  矩阵的转置</span></span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量</span></span><br><span class="line">    <span class="comment"># m-&gt;数据量，样本数 n-&gt;特征数</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># print m, n, &#x27;__&#x27;*10, shape(dataMatrix.transpose()), &#x27;__&#x27;*100</span></span><br><span class="line">    <span class="comment"># alpha代表向目标移动的步长</span></span><br><span class="line">    alpha = <span class="number">0.001</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    <span class="comment"># 生成一个长度和特征数相同的矩阵，此处n为3 -&gt; [[1],[1],[1]]</span></span><br><span class="line">    <span class="comment"># weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1</span></span><br><span class="line">    weights = ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(maxCycles):              <span class="comment">#heavy on matrix operations</span></span><br><span class="line">        <span class="comment"># m*3 的矩阵 * 3*1 的矩阵 ＝ m*1的矩阵</span></span><br><span class="line">        <span class="comment"># 那么乘上矩阵的意义，就代表: 通过公式得到的理论值</span></span><br><span class="line">        <span class="comment"># 参考地址:  矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145</span></span><br><span class="line">        <span class="comment"># print &#x27;dataMatrix====&#x27;, dataMatrix </span></span><br><span class="line">        <span class="comment"># print &#x27;weights====&#x27;, weights</span></span><br><span class="line">        <span class="comment"># n*3   *  3*1  = n*1</span></span><br><span class="line">        h = sigmoid(dataMatrix*weights)     <span class="comment"># 矩阵乘法</span></span><br><span class="line">        <span class="comment"># print &#x27;hhhhhhh====&#x27;, h</span></span><br><span class="line">        <span class="comment"># labelMat是实际值</span></span><br><span class="line">        error = (labelMat - h)              <span class="comment"># 向量相减</span></span><br><span class="line">        <span class="comment"># 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量</span></span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error <span class="comment"># 矩阵乘法，最后得到回归系数</span></span><br><span class="line">    <span class="keyword">return</span> array(weights)</span><br></pre></td></tr></table></div></figure>

<p>大家看到这儿可能会有一些疑惑，就是，我们在迭代中更新我们的回归系数，后边的部分是怎么计算出来的？为什么会是 alpha * dataMatrix.transpose() * error ?因为这就是我们所求的梯度，也就是对 f(w) 对 w 求一阶导数。具体推导如下:</p>
<p><img src="/img/LR_21.png" alt="f(w)对w求一阶导数"><br>可参考<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://blog.csdn.net/achuo/article/details/51160101" >http://blog.csdn.net/achuo/article/details/51160101</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>画出数据集和 Logistic 回归最佳拟合直线的函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span>(<span class="params">dataArr, labelMat, weights</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Desc:</span></span><br><span class="line"><span class="string">            将我们得到的数据可视化展示出来</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dataArr:样本数据的特征</span></span><br><span class="line"><span class="string">            labelMat:样本数据的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            weights:回归系数</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            None</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    n = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    xcord1 = []; ycord1 = []</span><br><span class="line">    xcord2 = []; ycord2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(labelMat[i])== <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord1, ycord1, s=<span class="number">30</span>, c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    ax.scatter(xcord2, ycord2, s=<span class="number">30</span>, c=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">    x = arange(-<span class="number">3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y的由来，卧槽，是不是没看懂？</span></span><br><span class="line"><span class="string">    首先理论上是这个样子的。</span></span><br><span class="line"><span class="string">    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])</span></span><br><span class="line"><span class="string">    w0*x0+w1*x1+w2*x2=f(x)</span></span><br><span class="line"><span class="string">    x0最开始就设置为1叻， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了</span></span><br><span class="line"><span class="string">    所以:  w0+w1*x+w2*y=0 =&gt; y = (-w0-w1*x)/w2   </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    y = (-weights[<span class="number">0</span>]-weights[<span class="number">1</span>]*x)/weights[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;X&#x27;</span>); plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用 Logistic 回归进行分类</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testLR</span>():</span></span><br><span class="line">    <span class="comment"># 1.收集并准备数据</span></span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">&quot;data/5.Logistic/TestSet.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print dataMat, &#x27;---\n&#x27;, labelMat</span></span><br><span class="line">    <span class="comment"># 2.训练模型，  f(x)=a1*x1+b2*x2+..+nn*xn中 (a1,b2, .., nn).T的矩阵值</span></span><br><span class="line">    <span class="comment"># 因为数组没有是复制n份， array的乘法就是乘法</span></span><br><span class="line">    dataArr = array(dataMat)</span><br><span class="line">    <span class="comment"># print dataArr</span></span><br><span class="line">    weights = gradAscent(dataArr, labelMat)</span><br><span class="line">    <span class="comment"># weights = stocGradAscent0(dataArr, labelMat)</span></span><br><span class="line">    <span class="comment"># weights = stocGradAscent1(dataArr, labelMat)</span></span><br><span class="line">    <span class="comment"># print &#x27;*&#x27;*30, weights</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据可视化</span></span><br><span class="line">    plotBestFit(dataArr, labelMat, weights)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 对简单数据集中数据进行分类</p>
</blockquote>

        <h4 id="注意"   >
          <a href="#注意" class="heading-link"><i class="fas fa-link"></i></a><a href="#注意" class="headerlink" title="注意"></a>注意</h4>
      <p>梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理 100 个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为 <code>随机梯度上升算法</code>。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习(online learning)算法。与 “在线学习” 相对应，一次处理所有数据被称作是 “批处理” （batch） 。</p>
<p>随机梯度上升算法可以写成如下的伪代码: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所有回归系数初始化为 1</span><br><span class="line">对数据集中每个样本</span><br><span class="line">    计算该样本的梯度</span><br><span class="line">    使用 alpha x gradient 更新回归系数值</span><br><span class="line">返回回归系数值</span><br></pre></td></tr></table></div></figure>

<p>以下是随机梯度上升算法的实现代码: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机梯度上升</span></span><br><span class="line"><span class="comment"># 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高</span></span><br><span class="line"><span class="comment"># 随机梯度上升一次只用一个样本点来更新回归系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span>(<span class="params">dataMatrix, classLabels</span>):</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># n*1的矩阵</span></span><br><span class="line">    <span class="comment"># 函数ones创建一个全1的数组</span></span><br><span class="line">    weights = ones(n)   <span class="comment"># 初始化长度为n的数组，元素全部为 1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵</span></span><br><span class="line">        h = sigmoid(<span class="built_in">sum</span>(dataMatrix[i]*weights))</span><br><span class="line">        <span class="comment"># print &#x27;dataMatrix[i]===&#x27;, dataMatrix[i]</span></span><br><span class="line">        <span class="comment"># 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数</span></span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        <span class="comment"># 0.01*(1*1)*(1*n)</span></span><br><span class="line">        <span class="built_in">print</span> weights, <span class="string">&quot;*&quot;</span>*<span class="number">10</span> , dataMatrix[i], <span class="string">&quot;*&quot;</span>*<span class="number">10</span> , error</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></div></figure>

<p>可以看到，随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别: 第一，后者的变量 h 和误差 error 都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是 NumPy 数组。</p>
<p>判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？下图展示了随机梯度上升算法在 200 次迭代过程中回归系数的变化情况。其中的系数2，也就是 X2 只经过了 50 次迭代就达到了稳定值，但系数 1 和 0 则需要更多次的迭代。如下图所示: </p>
<p><img src="/img/LR_12.png" alt="回归系数与迭代次数的关系图"></p>
<p>针对这个问题，我们改进了之前的随机梯度上升算法，如下: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机梯度上升算法（随机化）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span>(<span class="params">dataMatrix, classLabels, numIter=<span class="number">150</span></span>):</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)   <span class="comment"># 创建与列数相同的矩阵的系数矩阵，所有的元素都是1</span></span><br><span class="line">    <span class="comment"># 随机梯度, 循环150,观察是否收敛</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(numIter):</span><br><span class="line">        <span class="comment"># [0, 1, 2 .. m-1]</span></span><br><span class="line">        dataIndex = <span class="built_in">range</span>(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="comment"># i和j的不断增大，导致alpha的值不断减少，但是不为0</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span>    <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment"># 随机产生一个 0～len()之间的一个值</span></span><br><span class="line">            <span class="comment"># random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。</span></span><br><span class="line">            randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>,<span class="built_in">len</span>(dataIndex)))</span><br><span class="line">            <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn</span></span><br><span class="line">            h = sigmoid(<span class="built_in">sum</span>(dataMatrix[dataIndex[randIndex]]*weights))</span><br><span class="line">            error = classLabels[dataIndex[randIndex]] - h</span><br><span class="line">            <span class="comment"># print weights, &#x27;__h=%s&#x27; % h, &#x27;__&#x27;*20, alpha, &#x27;__&#x27;*20, error, &#x27;__&#x27;*20, dataMatrix[randIndex]</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></div></figure>

<p>上面的改进版随机梯度上升算法，我们修改了两处代码。</p>
<p>第一处改进为 alpha 的值。alpha 在每次迭代的时候都会调整，这回缓解上面波动图的数据波动或者高频波动。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。</p>
<p>第二处修改为 randIndex 更新，这里通过随机选取样本拉来更新回归系数。这种方法将减少周期性的波动。这种方法每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。</p>
<p>程序运行之后能看到类似于下图的结果图。</p>
<p><img src="/img/LR_13.png" alt="改进随机梯度下降结果图"></p>

        <h3 id="项目案例2-从疝气病症预测病马的死亡率"   >
          <a href="#项目案例2-从疝气病症预测病马的死亡率" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例2-从疝气病症预测病马的死亡率" class="headerlink" title="项目案例2: 从疝气病症预测病马的死亡率"></a>项目案例2: 从疝气病症预测病马的死亡率</h3>
      <p><a href="/src/py2.x/ml/5.Logistic/logistic.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>使用 Logistic 回归来预测患有疝病的马的存活问题。疝病是描述马胃肠痛的术语。然而，这种病不一定源自马的胃肠问题，其他问题也可能引发马疝病。这个数据集中包含了医院检测马疝病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 给定数据文件</span><br><span class="line">准备数据: 用 Python 解析文本文件并填充缺失值</span><br><span class="line">分析数据: 可视化并观察数据</span><br><span class="line">训练算法: 使用优化算法，找到最佳的系数</span><br><span class="line">测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，</span><br><span class="line">         通过改变迭代的次数和步长的参数来得到更好的回归系数</span><br><span class="line">使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，</span><br><span class="line">         这可以作为留给大家的一道习题</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 给定数据文件</p>
</blockquote>
<p>病马的训练数据已经给出来了，如下形式存储在文本文件中:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.000000	1.000000	39.200000	88.000000	20.000000	0.000000	0.000000	4.000000	1.000000	3.000000	4.000000	2.000000	0.000000	0.000000	0.000000	4.000000	2.000000	50.000000	85.000000	2.000000	2.000000	0.000000</span><br><span class="line">2.000000	1.000000	38.300000	40.000000	24.000000	1.000000	1.000000	3.000000	1.000000	3.000000	3.000000	1.000000	0.000000	0.000000	0.000000	1.000000	1.000000	33.000000	6.700000	0.000000	0.000000	1.000000</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 用 Python 解析文本文件并填充缺失值</p>
</blockquote>
<p>处理数据中的缺失值</p>
<p>假设有100个样本和20个特征，这些数据都是机器收集回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？<br>它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。</p>
<p>下面给出了一些可选的做法: </p>
<ul>
<li>使用可用特征的均值来填补缺失值；</li>
<li>使用特殊值来填补缺失值，如 -1；</li>
<li>忽略有缺失值的样本；</li>
<li>使用有相似样本的均值添补缺失值；</li>
<li>使用另外的机器学习算法预测缺失值。</li>
</ul>
<p>现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理需要做两件事: </p>
<ul>
<li><p>所有的缺失值必须用一个实数值来替换，因为我们使用的 NumPy 数据类型不允许包含缺失值。我们这里选择实数 0 来替换所有缺失值，恰好能适用于 Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下:</p>
<p>  weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</p>
<p>  如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新，即:</p>
<p>  weights = weights</p>
<p>  另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。</p>
</li>
<li><p>如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用 Logistic 回归进行分类时这种做法是合理的，而如果采用类似 kNN 的方法，则保留该条数据显得更加合理。</p>
</li>
</ul>
<p>原始的数据集经过预处理后，保存成两个文件: horseColicTest.txt 和 horseColicTraining.txt 。 </p>
<blockquote>
<p>分析数据: 可视化并观察数据</p>
</blockquote>
<p>将数据使用 MatPlotlib 打印出来，观察数据是否是我们想要的格式</p>
<blockquote>
<p>训练算法: 使用优化算法，找到最佳的系数</p>
</blockquote>
<p>下面给出 原始的梯度上升算法，随机梯度上升算法，改进版随机梯度上升算法 的代码: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正常的处理方案</span></span><br><span class="line"><span class="comment"># 两个参数: 第一个参数==&gt; dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。</span></span><br><span class="line"><span class="comment"># 第二个参数==&gt; classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span>(<span class="params">dataMatIn, classLabels</span>):</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[1,1,2],[1,1,2]....]</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)             <span class="comment"># 转换为 NumPy 矩阵</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]</span></span><br><span class="line">    <span class="comment"># transpose() 行列转置函数</span></span><br><span class="line">    <span class="comment"># 将行向量转化为列向量   =&gt;  矩阵的转置</span></span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量</span></span><br><span class="line">    <span class="comment"># m-&gt;数据量，样本数 n-&gt;特征数</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># print m, n, &#x27;__&#x27;*10, shape(dataMatrix.transpose()), &#x27;__&#x27;*100</span></span><br><span class="line">    <span class="comment"># alpha代表向目标移动的步长</span></span><br><span class="line">    alpha = <span class="number">0.001</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    <span class="comment"># 生成一个长度和特征数相同的矩阵，此处n为3 -&gt; [[1],[1],[1]]</span></span><br><span class="line">    <span class="comment"># weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1</span></span><br><span class="line">    weights = ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(maxCycles):              <span class="comment">#heavy on matrix operations</span></span><br><span class="line">        <span class="comment"># m*3 的矩阵 * 3*1 的单位矩阵 ＝ m*1的矩阵</span></span><br><span class="line">        <span class="comment"># 那么乘上单位矩阵的意义，就代表: 通过公式得到的理论值</span></span><br><span class="line">        <span class="comment"># 参考地址:  矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145</span></span><br><span class="line">        <span class="comment"># print &#x27;dataMatrix====&#x27;, dataMatrix </span></span><br><span class="line">        <span class="comment"># print &#x27;weights====&#x27;, weights</span></span><br><span class="line">        <span class="comment"># n*3   *  3*1  = n*1</span></span><br><span class="line">        h = sigmoid(dataMatrix*weights)     <span class="comment"># 矩阵乘法</span></span><br><span class="line">        <span class="comment"># print &#x27;hhhhhhh====&#x27;, h</span></span><br><span class="line">        <span class="comment"># labelMat是实际值</span></span><br><span class="line">        error = (labelMat - h)              <span class="comment"># 向量相减</span></span><br><span class="line">        <span class="comment"># 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量</span></span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error <span class="comment"># 矩阵乘法，最后得到回归系数</span></span><br><span class="line">    <span class="keyword">return</span> array(weights)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度上升</span></span><br><span class="line"><span class="comment"># 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高</span></span><br><span class="line"><span class="comment"># 随机梯度上升一次只用一个样本点来更新回归系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span>(<span class="params">dataMatrix, classLabels</span>):</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># n*1的矩阵</span></span><br><span class="line">    <span class="comment"># 函数ones创建一个全1的数组</span></span><br><span class="line">    weights = ones(n)   <span class="comment"># 初始化长度为n的数组，元素全部为 1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵</span></span><br><span class="line">        h = sigmoid(<span class="built_in">sum</span>(dataMatrix[i]*weights))</span><br><span class="line">        <span class="comment"># print &#x27;dataMatrix[i]===&#x27;, dataMatrix[i]</span></span><br><span class="line">        <span class="comment"># 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数</span></span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        <span class="comment"># 0.01*(1*1)*(1*n)</span></span><br><span class="line">        <span class="built_in">print</span> weights, <span class="string">&quot;*&quot;</span>*<span class="number">10</span> , dataMatrix[i], <span class="string">&quot;*&quot;</span>*<span class="number">10</span> , error</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度上升算法（随机化）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span>(<span class="params">dataMatrix, classLabels, numIter=<span class="number">150</span></span>):</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)   <span class="comment"># 创建与列数相同的矩阵的系数矩阵，所有的元素都是1</span></span><br><span class="line">    <span class="comment"># 随机梯度, 循环150,观察是否收敛</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(numIter):</span><br><span class="line">        <span class="comment"># [0, 1, 2 .. m-1]</span></span><br><span class="line">        dataIndex = <span class="built_in">range</span>(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="comment"># i和j的不断增大，导致alpha的值不断减少，但是不为0</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span>    <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment"># 随机产生一个 0～len()之间的一个值</span></span><br><span class="line">            <span class="comment"># random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。</span></span><br><span class="line">            randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>,<span class="built_in">len</span>(dataIndex)))</span><br><span class="line">            <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn</span></span><br><span class="line">            h = sigmoid(<span class="built_in">sum</span>(dataMatrix[dataIndex[randIndex]]*weights))</span><br><span class="line">            error = classLabels[dataIndex[randIndex]] - h</span><br><span class="line">            <span class="comment"># print weights, &#x27;__h=%s&#x27; % h, &#x27;__&#x27;*20, alpha, &#x27;__&#x27;*20, error, &#x27;__&#x27;*20, dataMatrix[randIndex]</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长的参数来得到更好的回归系数</p>
</blockquote>
<p>Logistic 回归分类函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分类函数，根据回归系数和特征向量来计算 Sigmoid的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span>(<span class="params">inX, weights</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc: </span></span><br><span class="line"><span class="string">        最终的分类函数，根据回归系数和特征向量来计算 Sigmoid 的值，大于0.5函数返回1，否则返回0</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inX -- 特征向量，features</span></span><br><span class="line"><span class="string">        weights -- 根据梯度下降/随机梯度下降 计算得到的回归系数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        如果 prob 计算大于 0.5 函数返回 1</span></span><br><span class="line"><span class="string">        否则返回 0</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    prob = sigmoid(<span class="built_in">sum</span>(inX*weights))</span><br><span class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>: <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开测试集和训练集,并对数据进行格式化处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        打开测试集和训练集，并对数据进行格式化处理</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        errorRate -- 分类错误率</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    frTrain = <span class="built_in">open</span>(<span class="string">&#x27;data/5.Logistic/horseColicTraining.txt&#x27;</span>)</span><br><span class="line">    frTest = <span class="built_in">open</span>(<span class="string">&#x27;data/5.Logistic/horseColicTest.txt&#x27;</span>)</span><br><span class="line">    trainingSet = []</span><br><span class="line">    trainingLabels = []</span><br><span class="line">    <span class="comment"># 解析训练数据集中的数据特征和Labels</span></span><br><span class="line">    <span class="comment"># trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(<span class="built_in">float</span>(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(<span class="built_in">float</span>(currLine[<span class="number">21</span>]))</span><br><span class="line">    <span class="comment"># 使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights</span></span><br><span class="line">    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, <span class="number">500</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    numTestVec = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        numTestVec += <span class="number">1.0</span></span><br><span class="line">        currLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(<span class="built_in">float</span>(currLine[i]))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(classifyVector(array(lineArr), trainWeights)) != <span class="built_in">int</span>(currLine[<span class="number">21</span>]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    errorRate = (<span class="built_in">float</span>(errorCount) / numTestVec)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;the error rate of this test is: %f&quot;</span> % errorRate</span><br><span class="line">    <span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 colicTest() 10次并求结果的平均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span>():</span></span><br><span class="line">    numTests = <span class="number">10</span></span><br><span class="line">    errorSum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(numTests):</span><br><span class="line">        errorSum += colicTest()</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;after %d iterations the average error rate is: %f&quot;</span> % (numTests, errorSum/<span class="built_in">float</span>(numTests)) </span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以作为留给大家的一道习题</p>
</blockquote>

        <h1 id="额外内容-可选读"   >
          <a href="#额外内容-可选读" class="heading-link"><i class="fas fa-link"></i></a><a href="#额外内容-可选读" class="headerlink" title="额外内容(可选读)"></a>额外内容(可选读)</h1>
      <p>在上文中，当Sigmoid函数大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。其实0.5也是可以改动的。 比如大于 0.9 的数据被分入 1 类，小于 0.9 即被归入 0 类。</p>

        <h2 id="Logistic回归-和-最大熵模型"   >
          <a href="#Logistic回归-和-最大熵模型" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic回归-和-最大熵模型" class="headerlink" title="Logistic回归 和 最大熵模型"></a>Logistic回归 和 最大熵模型</h2>
      <p>Logistic回归和最大熵模型 都属于对数线性模型 （log linear model）。 当类标签（class label）只有两个的时候，最大熵模型就是 logistic 回归模型。 学习它们的模型一般采用极大似然估计或者正则化的极大似然估计。Logistic 回归和最大熵模型学习可以形式化为无约束最优化问题。（关于最大熵模型，可以阅读《统计学习方法》 第六章。）</p>

        <h2 id="其他算法"   >
          <a href="#其他算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h2>
      <p>除了梯度下降，随机梯度下降，还有Conjugate Gradient，BFGS，L-BFGS，他们不需要指定alpha值（步长），而且比梯度下降更快，在现实中应用的也比较多。 当然这些算法相比随机梯度要复杂。</p>
<p>综上这些算法都有一个共通的缺点就是他们都是不断去逼近真实值，永远只是一个真实值的近似值而已。</p>

        <h2 id="多标签分类"   >
          <a href="#多标签分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h2>
      <p>逻辑回归也可以用作于多标签分类。 思路如下: </p>
<p>假设我们标签A中有a0,a1,a2….an个标签，对于每个标签 ai (ai 是标签A之一)，我们训练一个逻辑回归分类器。</p>
<p>即，训练该标签的逻辑回归分类器的时候，将ai看作一类标签，非ai的所有标签看作一类标签。那么相当于整个数据集里面只有两类标签: ai 和其他。</p>
<p>剩下步骤就跟我们训练正常的逻辑回归分类器一样了。</p>
<p>测试数据的时候，将查询点套用在每个逻辑回归分类器中的Sigmoid 函数，取值最高的对应标签为查询点的标签。</p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuxin" >羊三</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/04/ml_4/">第4章 基于概率论的分类方法-朴素贝叶斯</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-04</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="/img/NavieBayesian_headPage_xy.png" alt="朴素贝叶斯_首页" title="朴素贝叶斯首页"></p>

        <h2 id="朴素贝叶斯-概述"   >
          <a href="#朴素贝叶斯-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-概述" class="headerlink" title="朴素贝叶斯 概述"></a>朴素贝叶斯 概述</h2>
      <p><code>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。</code></p>

        <h2 id="贝叶斯理论-amp-条件概率"   >
          <a href="#贝叶斯理论-amp-条件概率" class="heading-link"><i class="fas fa-link"></i></a><a href="#贝叶斯理论-amp-条件概率" class="headerlink" title="贝叶斯理论 &amp; 条件概率"></a>贝叶斯理论 &amp; 条件概率</h2>
      
        <h3 id="贝叶斯理论"   >
          <a href="#贝叶斯理论" class="heading-link"><i class="fas fa-link"></i></a><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h3>
      <p>我们现在有一个数据集，它由两类数据组成，数据分布如下图所示: </p>
<p><img src="img/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.png" alt="朴素贝叶斯示例数据分布" title="参数已知的概率分布"></p>
<p>我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别: </p>
<ul>
<li>如果 p1(x,y) &gt; p2(x,y) ，那么类别为1</li>
<li>如果 p2(x,y) &gt; p1(x,y) ，那么类别为2</li>
</ul>
<p>也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。</p>

        <h3 id="条件概率"   >
          <a href="#条件概率" class="heading-link"><i class="fas fa-link"></i></a><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3>
      <p>如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。</p>
<p>有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。</p>
<p><img src="/img/NB_2.png" alt="包含 7 块石头的集合"></p>
<p>如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？</p>
<p><img src="/img/NB_3.png" alt="7块石头放入两个桶中"></p>
<p>计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3 。</p>
<p>条件概率的计算公式如下: </p>
<p>P(white|bucketB) = P(white and bucketB) / P(bucketB)</p>
<p>首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。</p>
<p>另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法: </p>
<p><img src="/img/NB_4.png" alt="计算p(c|x)的方法"></p>

        <h3 id="使用条件概率来分类"   >
          <a href="#使用条件概率来分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#使用条件概率来分类" class="headerlink" title="使用条件概率来分类"></a>使用条件概率来分类</h3>
      <p>上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y):</p>
<ul>
<li>如果 p1(x, y) &gt; p2(x, y), 那么属于类别 1;</li>
<li>如果 p2(x, y) &gt; p1(X, y), 那么属于类别 2.</li>
</ul>
<p>这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到: </p>
<p><img src="/img/NB_5.png" alt="应用贝叶斯准则"></p>
<p>使用上面这些定义，可以定义贝叶斯分类准则为:</p>
<ul>
<li>如果 P(c1|x, y) &gt; P(c2|x, y), 那么属于类别 c1;</li>
<li>如果 P(c2|x, y) &gt; P(c1|x, y), 那么属于类别 c2.</li>
</ul>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>我们假设特征之间  <strong>相互独立</strong> 。所谓 <b>独立(independence)</b> 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，<b>每个特征同等重要</b>。</p>
<p><b>Note:</b> 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。</p>

        <h2 id="朴素贝叶斯-场景"   >
          <a href="#朴素贝叶斯-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-场景" class="headerlink" title="朴素贝叶斯 场景"></a>朴素贝叶斯 场景</h2>
      <p>机器学习的一个重要应用就是文档的自动分类。</p>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。</p>

        <h2 id="朴素贝叶斯-原理"   >
          <a href="#朴素贝叶斯-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-原理" class="headerlink" title="朴素贝叶斯 原理"></a>朴素贝叶斯 原理</h2>
      
        <h3 id="朴素贝叶斯-工作原理"   >
          <a href="#朴素贝叶斯-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-工作原理" class="headerlink" title="朴素贝叶斯 工作原理"></a>朴素贝叶斯 工作原理</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">提取所有文档中的词条并进行去重</span><br><span class="line">获取文档的所有类别</span><br><span class="line">计算每个类别中的文档数目</span><br><span class="line">对每篇训练文档: </span><br><span class="line">    对每个类别: </span><br><span class="line">        如果词条出现在文档中--&gt;增加该词条的计数值（for循环或者矩阵相加）</span><br><span class="line">        增加所有词条的计数值（此类别下词条总数）</span><br><span class="line">对每个类别: </span><br><span class="line">    对每个词条: </span><br><span class="line">        将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）</span><br><span class="line">返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）</span><br></pre></td></tr></table></div></figure>


        <h3 id="朴素贝叶斯-开发流程"   >
          <a href="#朴素贝叶斯-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-开发流程" class="headerlink" title="朴素贝叶斯 开发流程"></a>朴素贝叶斯 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法。</span><br><span class="line">准备数据: 需要数值型或者布尔型数据。</span><br><span class="line">分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。</span><br><span class="line">训练算法: 计算不同的独立特征的条件概率。</span><br><span class="line">测试算法: 计算错误率。</span><br><span class="line">使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。</span><br></pre></td></tr></table></div></figure>


        <h3 id="朴素贝叶斯-算法特点"   >
          <a href="#朴素贝叶斯-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-算法特点" class="headerlink" title="朴素贝叶斯 算法特点"></a>朴素贝叶斯 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 在数据较少的情况下仍然有效，可以处理多类别问题。</span><br><span class="line">缺点: 对于输入数据的准备方式较为敏感。</span><br><span class="line">适用数据类型: 标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h2 id="朴素贝叶斯-项目案例"   >
          <a href="#朴素贝叶斯-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#朴素贝叶斯-项目案例" class="headerlink" title="朴素贝叶斯 项目案例"></a>朴素贝叶斯 项目案例</h2>
      
        <h3 id="项目案例1-屏蔽社区留言板的侮辱性言论"   >
          <a href="#项目案例1-屏蔽社区留言板的侮辱性言论" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-屏蔽社区留言板的侮辱性言论" class="headerlink" title="项目案例1: 屏蔽社区留言板的侮辱性言论"></a>项目案例1: 屏蔽社区留言板的侮辱性言论</h3>
      <p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法</span><br><span class="line">准备数据: 从文本中构建词向量</span><br><span class="line">分析数据: 检查词条确保解析的正确性</span><br><span class="line">训练算法: 从词向量计算概率</span><br><span class="line">测试算法: 根据现实情况修改分类器</span><br><span class="line">使用算法: 对社区留言板言论进行分类</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>本例是我们自己构造的词表:</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建数据集</span></span><br><span class="line"><span class="string">    :return: 单词列表postingList, 所属类别classVec</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    postingList = [[<span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;flea&#x27;</span>, <span class="string">&#x27;problems&#x27;</span>, <span class="string">&#x27;help&#x27;</span>, <span class="string">&#x27;please&#x27;</span>], <span class="comment">#[0,0,1,1,1......]</span></span><br><span class="line">                   [<span class="string">&#x27;maybe&#x27;</span>, <span class="string">&#x27;not&#x27;</span>, <span class="string">&#x27;take&#x27;</span>, <span class="string">&#x27;him&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;park&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;cute&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;him&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;posting&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;mr&#x27;</span>, <span class="string">&#x27;licks&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;steak&#x27;</span>, <span class="string">&#x27;how&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;him&#x27;</span>],</span><br><span class="line">                   [<span class="string">&#x27;quit&#x27;</span>, <span class="string">&#x27;buying&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;food&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList, classVec</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 从文本中构建词向量</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取所有单词的集合</span></span><br><span class="line"><span class="string">    :param dataSet: 数据集</span></span><br><span class="line"><span class="string">    :return: 所有单词的集合(即不含重复元素的单词列表)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    vocabSet = <span class="built_in">set</span>([])  <span class="comment"># create empty set</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 操作符 | 用于求两个集合的并集</span></span><br><span class="line">        vocabSet = vocabSet | <span class="built_in">set</span>(document)  <span class="comment"># union of the two sets</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(vocabSet)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span>(<span class="params">vocabList, inputSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    遍历查看该单词是否出现，出现该单词则将该单词置1</span></span><br><span class="line"><span class="string">    :param vocabList: 所有单词集合列表</span></span><br><span class="line"><span class="string">    :param inputSet: 输入数据集</span></span><br><span class="line"><span class="string">    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 创建一个和词汇表等长的向量，并将其元素都设置为0</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * <span class="built_in">len</span>(vocabList)<span class="comment"># [0,0......]</span></span><br><span class="line">    <span class="comment"># 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;the word: %s is not in my Vocabulary!&quot;</span> % word</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<p>检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOPosts, listClasses = bayes.loadDataSet()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList = bayes.createVocabList(listOPosts)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList</span><br><span class="line">[<span class="string">&#x27;cute&#x27;</span>, <span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;help&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>, <span class="string">&#x27;quit&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;problems&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;park&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;flea&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>, <span class="string">&#x27;licks&#x27;</span>, <span class="string">&#x27;food&#x27;</span>, <span class="string">&#x27;not&#x27;</span>, <span class="string">&#x27;him&#x27;</span>, <span class="string">&#x27;buying&#x27;</span>, <span class="string">&#x27;posting&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;maybe&#x27;</span>, <span class="string">&#x27;please&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;how&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;take&#x27;</span>, <span class="string">&#x27;mr&#x27;</span>, <span class="string">&#x27;steak&#x27;</span>, <span class="string">&#x27;my&#x27;</span>]</span><br></pre></td></tr></table></div></figure>

<p>检查函数有效性。例如: myVocabList 中索引为 2 的元素是什么单词？应该是是 help 。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">0</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">3</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 从词向量计算概率</p>
</blockquote>
<p>现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 <b>w</b>. 粗体的 <b>w</b> 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。</p>
<p><img src="/img/NB_6.png" alt="重写贝叶斯准则"></p>
<p>我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。</p>
<p>问: 上述代码实现中，为什么没有计算P(w)？</p>
<p>答: 根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个ci，P(w)是固定的。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。</p>
<p>首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。接下来计算 p(<b>w</b> | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2…wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)…p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。</p>
<p>朴素贝叶斯分类器训练函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据原版</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率，即trainCategory中所有的1的个数，</span></span><br><span class="line">    <span class="comment"># 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    p0Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line">    p1Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数</span></span><br><span class="line">    p0Denom = <span class="number">0.0</span></span><br><span class="line">    p1Denom = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="comment"># 是否是侮辱性文件</span></span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果是侮辱性文件，对侮辱性文件的向量进行加和</span></span><br><span class="line">            p1Num += trainMatrix[i] <span class="comment">#[0,1,1,....] + [0,1,1,....]-&gt;[0,2,2,...]</span></span><br><span class="line">            <span class="comment"># 对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在1类别下，每个单词出现的概率</span></span><br><span class="line">    p1Vect = p1Num / p1Denom<span class="comment"># [1,2,3,5]/90-&gt;[1/90,...]</span></span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在0类别下，每个单词出现的概率</span></span><br><span class="line">    p0Vect = p0Num / p0Denom</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 根据现实情况修改分类器</p>
</blockquote>
<p>在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。</p>
<p>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)… p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</p>
<p>下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。</p>
<p><img src="/img/NB_7.png" alt="函数图像"></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


<blockquote>
<p>使用算法: 对社区留言板言论进行分类</p>
</blockquote>
<p>朴素贝叶斯分类函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span>(<span class="params">vec2Classify, p0Vec, p1Vec, pClass1</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用算法: </span></span><br><span class="line"><span class="string">        # 将乘法转换为加法</span></span><br><span class="line"><span class="string">        乘法: P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)</span></span><br><span class="line"><span class="string">        加法: P(F1|C)*P(F2|C)....P(Fn|C)P(C) -&gt; log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line"><span class="string">    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量</span></span><br><span class="line"><span class="string">    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line"><span class="string">    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line"><span class="string">    :param pClass1: 类别1，侮辱性文件的出现概率</span></span><br><span class="line"><span class="string">    :return: 类别1 or 0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line">    <span class="comment"># 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，</span></span><br><span class="line">    <span class="comment"># 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。</span></span><br><span class="line">    <span class="comment"># 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。</span></span><br><span class="line">    <span class="comment"># 我的理解是: 这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来</span></span><br><span class="line">    p1 = <span class="built_in">sum</span>(vec2Classify * p1Vec) + log(pClass1) <span class="comment"># P(w|c1) * P(c1) ，即贝叶斯准则的分子</span></span><br><span class="line">    p0 = <span class="built_in">sum</span>(vec2Classify * p0Vec) + log(<span class="number">1.0</span> - pClass1) <span class="comment"># P(w|c0) * P(c0) ，即贝叶斯准则的分子·</span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    测试朴素贝叶斯算法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1. 加载数据集</span></span><br><span class="line">    listOPosts, listClasses = loadDataSet()</span><br><span class="line">    <span class="comment"># 2. 创建单词集合</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    <span class="comment"># 3. 计算单词是否出现并创建数据矩阵</span></span><br><span class="line">    trainMat = []</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        <span class="comment"># 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息</span></span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment"># 4. 训练数据</span></span><br><span class="line">    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))</span><br><span class="line">    <span class="comment"># 5. 测试数据</span></span><br><span class="line">    testEntry = [<span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="built_in">print</span> testEntry, <span class="string">&#x27;classified as: &#x27;</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br><span class="line">    testEntry = [<span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="built_in">print</span> testEntry, <span class="string">&#x27;classified as: &#x27;</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br></pre></td></tr></table></div></figure>



        <h3 id="项目案例2-使用朴素贝叶斯过滤垃圾邮件"   >
          <a href="#项目案例2-使用朴素贝叶斯过滤垃圾邮件" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例2-使用朴素贝叶斯过滤垃圾邮件" class="headerlink" title="项目案例2: 使用朴素贝叶斯过滤垃圾邮件"></a>项目案例2: 使用朴素贝叶斯过滤垃圾邮件</h3>
      <p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <p>使用朴素贝叶斯对电子邮件进行分类</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 提供文本文件</span><br><span class="line">准备数据: 将文本文件解析成词条向量</span><br><span class="line">分析数据: 检查词条确保解析的正确性</span><br><span class="line">训练算法: 使用我们之前建立的 trainNB() 函数</span><br><span class="line">测试算法: 使用朴素贝叶斯进行交叉验证</span><br><span class="line">使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>文本文件内容如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hi Peter,</span><br><span class="line"></span><br><span class="line">With Jose out of town, do you want to</span><br><span class="line">meet once in a while to keep things</span><br><span class="line">going and do some interesting stuff?</span><br><span class="line"></span><br><span class="line">Let me know</span><br><span class="line">Eugene</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>使用正则表达式来切分文本</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mySent = <span class="string">&#x27;This book is the best book on Python or M.L. I have ever laid eyes upon.&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>regEx = re.<span class="built_in">compile</span>(<span class="string">&#x27;\\W*&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens = regEx.split(mySent)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens</span><br><span class="line">[<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;best&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;Python&#x27;</span>, <span class="string">&#x27;or&#x27;</span>, <span class="string">&#x27;M.L.&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;have&#x27;</span>, <span class="string">&#x27;ever&#x27;</span>, <span class="string">&#x27;laid&#x27;</span>, <span class="string">&#x27;eyes&#x27;</span>, <span class="string">&#x27;upon&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用朴素贝叶斯进行交叉验证</p>
</blockquote>
<p>文件解析及完整的垃圾邮件测试函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切分文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span>(<span class="params">bigString</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        接收一个大字符串并将其解析为字符串列表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bigString -- 大字符串</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="comment"># 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串</span></span><br><span class="line">    listOfTokens = re.split(<span class="string">r&#x27;\W*&#x27;</span>, bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> <span class="built_in">len</span>(tok) &gt; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对贝叶斯垃圾邮件分类器进行自动化处理。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    docList = []</span><br><span class="line">    classList = []</span><br><span class="line">    fullText = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">26</span>):</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 1 类别</span></span><br><span class="line">        wordList = textParse(<span class="built_in">open</span>(<span class="string">&#x27;data/4.NaiveBayes/email/spam/%d.txt&#x27;</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 0 类别</span></span><br><span class="line">        wordList = textParse(<span class="built_in">open</span>(<span class="string">&#x27;data/4.NaiveBayes/email/ham/%d.txt&#x27;</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 创建词汇表    </span></span><br><span class="line">    vocabList = createVocabList(docList)</span><br><span class="line">    trainingSet = <span class="built_in">range</span>(<span class="number">50</span>)</span><br><span class="line">    testSet = []</span><br><span class="line">    <span class="comment"># 随机取 10 个邮件用来测试</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># random.uniform(x, y) 随机生成一个范围为 x ~ y 的实数</span></span><br><span class="line">        randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, <span class="built_in">len</span>(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat = []</span><br><span class="line">    trainClasses = []</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector = setOfWords2Vec(vocabList, docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the errorCount is: &#x27;</span>, errorCount</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the testSet length is :&#x27;</span>, <span class="built_in">len</span>(testSet)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the error rate is :&#x27;</span>, <span class="built_in">float</span>(errorCount)/<span class="built_in">len</span>(testSet)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
</blockquote>

        <h3 id="项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向"   >
          <a href="#项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="headerlink" title="项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向"></a>项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向</h3>
      <p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-2"   >
          <a href="#项目概述-2" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-2" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。</p>
<p>我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是哪些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。</p>

        <h4 id="开发流程-2"   >
          <a href="#开发流程-2" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-2" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</span><br><span class="line">准备数据: 将文本文件解析成词条向量</span><br><span class="line">分析数据: 检查词条确保解析的正确性</span><br><span class="line">训练算法: 使用我们之前建立的 trainNB0() 函数</span><br><span class="line">测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</span><br><span class="line">使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</p>
</blockquote>
<p>也就是导入 RSS 源，我们使用 python 下载文本，在<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://code.google.com/p/feedparser/" >http://code.google.com/p/feedparser/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>python setup.py install</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>文档词袋模型</p>
<p>我们将每个词的出现与否作为一个特征，这可以被描述为 <b>词集模型(set-of-words model)</b>。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 <b>词袋模型(bag-of-words model)</b>。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。</p>
<p>如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span>(<span class="params">vocaList, inputSet</span>):</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * <span class="built_in">len</span>(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocaList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个包含在所有文档中出现的不重复词的列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    vocabSet=<span class="built_in">set</span>([])    <span class="comment">#创建一个空集</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet=vocabSet|<span class="built_in">set</span>(document)   <span class="comment">#创建两个集合的并集</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(vocabSet)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2VecMN</span>(<span class="params">vocabList,inputSet</span>):</span></span><br><span class="line">    returnVec=[<span class="number">0</span>]*<span class="built_in">len</span>(vocabList)  <span class="comment">#创建一个其中所含元素都为0的向量</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">                returnVec[vocabList.index(word)]+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件解析</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span>(<span class="params">bigString</span>):</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    listOfTokens=re.split(<span class="string">r&#x27;\W*&#x27;</span>,bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> <span class="built_in">len</span>(tok)&gt;<span class="number">2</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span>(<span class="params">trainMatrix, trainCategory</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = <span class="built_in">len</span>(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = <span class="built_in">len</span>(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = <span class="built_in">sum</span>(trainCategory) / <span class="built_in">float</span>(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计 </span></span><br><span class="line">    <span class="comment"># 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += <span class="built_in">sum</span>(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#RSS源分类器及高频词去除函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span>(<span class="params">vocabList,fullText</span>):</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    freqDict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> vocabList:  <span class="comment">#遍历词汇表中的每个词</span></span><br><span class="line">        freqDict[token]=fullText.count(token)  <span class="comment">#统计每个词在文本中出现的次数</span></span><br><span class="line">    sortedFreq=<span class="built_in">sorted</span>(freqDict.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)  <span class="comment">#根据每个词出现的次数从高到底对字典进行排序</span></span><br><span class="line">    <span class="keyword">return</span> sortedFreq[:<span class="number">30</span>]   <span class="comment">#返回出现次数最高的30个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">localWords</span>(<span class="params">feed1,feed0</span>):</span></span><br><span class="line">    <span class="keyword">import</span> feedparser</span><br><span class="line">    docList=[];classList=[];fullText=[]</span><br><span class="line">    minLen=<span class="built_in">min</span>(<span class="built_in">len</span>(feed1[<span class="string">&#x27;entries&#x27;</span>]),<span class="built_in">len</span>(feed0[<span class="string">&#x27;entries&#x27;</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(minLen):</span><br><span class="line">        wordList=textParse(feed1[<span class="string">&#x27;entries&#x27;</span>][i][<span class="string">&#x27;summary&#x27;</span>])   <span class="comment">#每次访问一条RSS源</span></span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        wordList=textParse(feed0[<span class="string">&#x27;entries&#x27;</span>][i][<span class="string">&#x27;summary&#x27;</span>])</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList=createVocabList(docList)</span><br><span class="line">    top30Words=calcMostFreq(vocabList,fullText)</span><br><span class="line">    <span class="keyword">for</span> pairW <span class="keyword">in</span> top30Words:</span><br><span class="line">        <span class="keyword">if</span> pairW[<span class="number">0</span>] <span class="keyword">in</span> vocabList:vocabList.remove(pairW[<span class="number">0</span>])    <span class="comment">#去掉出现次数最高的那些词</span></span><br><span class="line">    trainingSet=<span class="built_in">range</span>(<span class="number">2</span>*minLen);testSet=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        randIndex=<span class="built_in">int</span>(random.uniform(<span class="number">0</span>,<span class="built_in">len</span>(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat=[];trainClasses=[]</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V,p1V,pSpam=trainNBO(array(trainMat),array(trainClasses))</span><br><span class="line">    errorCount=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;the error rate is:&#x27;</span>,<span class="built_in">float</span>(errorCount)/<span class="built_in">len</span>(testSet)</span><br><span class="line">    <span class="keyword">return</span> vocabList,p0V,p1V</span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯分类函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span>(<span class="params">vec2Classify,p0Vec,p1Vec,pClass1</span>):</span></span><br><span class="line">    p1=<span class="built_in">sum</span>(vec2Classify*p1Vec)+log(pClass1)</span><br><span class="line">    p0=<span class="built_in">sum</span>(vec2Classify*p0Vec)+log(<span class="number">1.0</span>-pClass1)</span><br><span class="line">    <span class="keyword">if</span> p1&gt;p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</p>
</blockquote>
<p>函数 localWords() 使用了两个 RSS 源作为参数，RSS 源要在函数外导入，这样做的原因是 RSS 源会随时间而改变，重新加载 RSS 源就会得到新的数据</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">&#x27;bayes&#x27;</span> <span class="keyword">from</span> <span class="string">&#x27;bayes.pyc&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ny=feedparser.parse(<span class="string">&#x27;http://newyork.craigslist.org/stp/index.rss&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sy=feedparser.parse(<span class="string">&#x27;http://sfbay.craigslist.org/stp/index.rss&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br></pre></td></tr></table></div></figure>
<p>为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值</p>
<p>接下来，我们要分析一下数据，显示地域相关的用词</p>
<p>可以先对向量pSF与pNY进行排序，然后按照顺序打印出来，将下面的代码添加到文件中: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#最具表征性的词汇显示函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTopWords</span>(<span class="params">ny,sf</span>):</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    vocabList,p0V,p1V=localWords(ny,sf)</span><br><span class="line">    topNY=[];topSF=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(p0V)):</span><br><span class="line">        <span class="keyword">if</span> p0V[i]&gt;-<span class="number">6.0</span>:topSF.append((vocabList[i],p0V[i]))</span><br><span class="line">        <span class="keyword">if</span> p1V[i]&gt;-<span class="number">6.0</span>:topNY.append((vocabList[i],p1V[i]))</span><br><span class="line">    sortedSF=<span class="built_in">sorted</span>(topSF,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedSF:</span><br><span class="line">        <span class="built_in">print</span> item[<span class="number">0</span>]</span><br><span class="line">    sortedNY=<span class="built_in">sorted</span>(topNY,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedNY:</span><br><span class="line">        <span class="built_in">print</span> item[<span class="number">0</span>]</span><br></pre></td></tr></table></div></figure>

<p>函数 getTopWords() 使用两个 RSS 源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储，与之前返回排名最高的 X 个单词不同，这里可以返回大于某个阈值的所有词，这些元组会按照它们的条件概率进行排序。</p>
<p>保存 bayes.py 文件，在python提示符下输入: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">&#x27;bayes&#x27;</span> <span class="keyword">from</span> <span class="string">&#x27;bayes.pyc&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.getTopWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br><span class="line">SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**</span><br><span class="line">how</span><br><span class="line">last</span><br><span class="line">man</span><br><span class="line">...</span><br><span class="line">veteran</span><br><span class="line">still</span><br><span class="line">ends</span><br><span class="line">late</span><br><span class="line">off</span><br><span class="line">own</span><br><span class="line">know</span><br><span class="line">NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**</span><br><span class="line">someone</span><br><span class="line">meet</span><br><span class="line">...</span><br><span class="line">apparel</span><br><span class="line">recalled</span><br><span class="line">starting</span><br><span class="line">strings</span><br></pre></td></tr></table></div></figure>

<p>当注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能，去掉这几行代码之后，错误率为54%，，而保留这些代码得到的错误率为70%。这里观察到，这些留言中出现次数最多的前30个词涵盖了所有用词的30%，vocabList的大小约为3000个词，也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定高频词中移除结构上的辅助词，该词表称为停用词表。</p>
<p>从最后输出的单词，可以看出程序输出了大量的停用词，可以移除固定的停用词看看结果如何，这样做的话，分类错误率也会降低。</p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuxin" >羊三</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/03/ml_3/">第3章 决策树</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-03</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="/img/DecisionTree_headPage_xy.png" alt="决策树_首页" title="决策树首页"></p>

        <h2 id="决策树-概述"   >
          <a href="#决策树-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-概述" class="headerlink" title="决策树 概述"></a>决策树 概述</h2>
      <p><code>决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。</code></p>
<p><code>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</code></p>
<p><code>决策树学习通常包括 3 个步骤: 特征选择、决策树的生成和决策树的修剪。</code></p>

        <h2 id="决策树-场景"   >
          <a href="#决策树-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-场景" class="headerlink" title="决策树 场景"></a>决策树 场景</h2>
      <p>一个叫做 “二十个问题” 的游戏，游戏的规则很简单: 参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。</p>
<p>一个邮件分类系统，大致工作流程如下: </p>
<p><img src="/img/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="决策树-流程图" title="决策树示例流程图"></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先检测发送邮件域名地址。如果地址为 myEmployer.com, 则将其放在分类 &quot;无聊时需要阅读的邮件&quot;中。</span><br><span class="line">如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 &quot;曲棍球&quot; , 如果包含则将邮件归类到 &quot;需要及时处理的朋友邮件&quot;, </span><br><span class="line">如果不包含则将邮件归类到 &quot;无需阅读的垃圾邮件&quot; 。</span><br></pre></td></tr></table></div></figure>

<p>决策树的定义: </p>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>
<p>用决策树对需要测试的实例进行分类: 从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。</p>

        <h2 id="决策树-原理"   >
          <a href="#决策树-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2>
      
        <h3 id="决策树-须知概念"   >
          <a href="#决策树-须知概念" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3>
      
        <h4 id="信息熵-amp-信息增益"   >
          <a href="#信息熵-amp-信息增益" class="heading-link"><i class="fas fa-link"></i></a><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4>
      <p>熵（entropy）:<br>熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p>
<p>信息论（information theory）中的熵（香农熵）:<br>是一种信息的度量方式，表示信息的混乱程度，也就是说: 信息越有序，信息熵越低。例如: 火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p>
<p>信息增益（information gain）:<br>在划分数据集前后信息发生的变化称为信息增益。</p>

        <h3 id="决策树-工作原理"   >
          <a href="#决策树-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3>
      <p>如何构造一个决策树?<br/><br>我们使用 createBranch() 方法，如下所示: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def createBranch():</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">    检测数据集中的所有数据的分类标签是否相同:</span><br><span class="line">        If so return 类标签</span><br><span class="line">        Else:</span><br><span class="line">            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）</span><br><span class="line">            划分数据集</span><br><span class="line">            创建分支节点</span><br><span class="line">                for 每个划分的子集</span><br><span class="line">                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中</span><br><span class="line">            return 分支节点</span><br></pre></td></tr></table></div></figure>


        <h3 id="决策树-开发流程"   >
          <a href="#决策树-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法。</span><br><span class="line">准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)</span><br><span class="line">分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。</span><br><span class="line">训练算法: 构造树的数据结构。</span><br><span class="line">测试算法: 使用训练好的树计算错误率。</span><br><span class="line">使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</span><br></pre></td></tr></table></div></figure>


        <h3 id="决策树-算法特点"   >
          <a href="#决策树-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-算法特点" class="headerlink" title="决策树 算法特点"></a>决策树 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。</span><br><span class="line">缺点: 容易过拟合。</span><br><span class="line">适用数据类型: 数值型和标称型。</span><br></pre></td></tr></table></div></figure>


        <h2 id="决策树-项目案例"   >
          <a href="#决策树-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#决策树-项目案例" class="headerlink" title="决策树 项目案例"></a>决策树 项目案例</h2>
      
        <h3 id="项目案例1-判定鱼类和非鱼类"   >
          <a href="#项目案例1-判定鱼类和非鱼类" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-判定鱼类和非鱼类" class="headerlink" title="项目案例1: 判定鱼类和非鱼类"></a>项目案例1: 判定鱼类和非鱼类</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>根据以下 2 个特征，将动物分成两类: 鱼类和非鱼类。</p>
<p>特征: </p>
<ol>
<li>不浮出水面是否可以生存</li>
<li>是否有脚蹼</li>
</ol>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任何方法</span><br><span class="line">准备数据: 树构造算法（这里使用的是ID3算法，因此数值型数据必须离散化。）</span><br><span class="line">分析数据: 可以使用任何方法，构造树完成之后，我们可以将树画出来。</span><br><span class="line">训练算法: 构造树结构</span><br><span class="line">测试算法: 使用习得的决策树执行分类</span><br><span class="line">使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p><img src="/img/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png" alt="海洋生物数据"></p>
<p>我们利用 createDataSet() 函数输入数据</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span>():</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;no surfacing&#x27;</span>, <span class="string">&#x27;flippers&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>准备数据: 树构造算法</p>
</blockquote>
<p>此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。</p>
<blockquote>
<p>分析数据: 可以使用任何方法，构造树完成之后，我们可以将树画出来。</p>
</blockquote>
<p><img src="/img/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg" alt="熵的计算公式"></p>
<p>计算给定数据集的香农熵的函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="comment"># 求list的长度，表示计算参与训练的数据量</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)</span><br><span class="line">    <span class="comment"># 计算分类标签label出现的次数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># the the number of unique elements and their occurrence</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签</span></span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于 label 标签的占比，求出 label 标签的香农熵</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        <span class="comment"># 使用所有类标签的发生频率计算类别出现的概率。</span></span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key])/numEntries</span><br><span class="line">        <span class="comment"># 计算香农熵，以 2 为底求对数</span></span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></div></figure>

<p>按照给定特征划分数据集</p>
<p><code>将指定特征的特征值等于 value 的行剩下列作为子数据集。</code></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span>(<span class="params">dataSet, index, value</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)</span></span><br><span class="line"><span class="string">        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集                 待划分的数据集</span></span><br><span class="line"><span class="string">        index 表示每一行的index列        划分数据集的特征</span></span><br><span class="line"><span class="string">        value 表示index列对应的value值   需要返回的特征的值。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet: </span><br><span class="line">        <span class="comment"># index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line">        <span class="comment"># 判断index列的值是否为value</span></span><br><span class="line">        <span class="keyword">if</span> featVec[index] == value:</span><br><span class="line">            <span class="comment"># chop out index used for splitting</span></span><br><span class="line">            <span class="comment"># [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行</span></span><br><span class="line">            reducedFeatVec = featVec[:index]</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            请百度查询一下:  extend和append的区别</span></span><br><span class="line"><span class="string">            music_media.append(object) 向列表中添加一个对象object</span></span><br><span class="line"><span class="string">            music_media.extend(sequence) 把一个序列seq的内容添加到列表中 (跟 += 在list运用类似， music_media += sequence)</span></span><br><span class="line"><span class="string">            1、使用append的时候，是将object看作一个对象，整体打包添加到music_media对象中。</span></span><br><span class="line"><span class="string">            2、使用extend的时候，是将sequence看作一个序列，将这个序列和music_media序列合并，并放在其后面。</span></span><br><span class="line"><span class="string">            music_media = []</span></span><br><span class="line"><span class="string">            music_media.extend([1,2,3])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果: </span></span><br><span class="line"><span class="string">            #[1, 2, 3]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.append([4,5,6])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果: </span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6]]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.extend([7,8,9])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果: </span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6], 7, 8, 9]</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            reducedFeatVec.extend(featVec[index+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment"># [index+1:]表示从跳过 index 的 index+1行，取接下来的数据</span></span><br><span class="line">            <span class="comment"># 收集结果值 index列为value的行【该行需要排除index列】</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></div></figure>

<p>选择最好的数据集划分方式</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;chooseBestFeatureToSplit(选择最好的特征)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestFeature 最优的特征列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 求第一行有多少列的 Feature, 最后一列是label列嘛</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 数据集的原始信息熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 最优的信息增益值, 和最优的Featurn编号</span></span><br><span class="line">    bestInfoGain, bestFeature = <span class="number">0.0</span>, -<span class="number">1</span></span><br><span class="line">    <span class="comment"># iterate over all the features</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        <span class="comment"># create a list of all the examples of this feature</span></span><br><span class="line">        <span class="comment"># 获取对应的feature下的所有数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># get a set of unique values</span></span><br><span class="line">        <span class="comment"># 获取剔重后的集合，使用set对list数据进行去重</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        <span class="comment"># 创建一个临时的信息熵</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 遍历某一列的value集合，计算该列的信息熵 </span></span><br><span class="line">        <span class="comment"># 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算概率</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet)/<span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            <span class="comment"># 计算信息熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值</span></span><br><span class="line">        <span class="comment"># 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;infoGain=&#x27;</span>, infoGain, <span class="string">&#x27;bestFeature=&#x27;</span>, i, baseEntropy, newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问: 上面的 newEntropy 为什么是根据子集计算的呢？</span><br><span class="line">答: 因为我们在根据一个特征计算香农熵的时候，该特征的分类值是相同，这个特征这个分类的香农熵为 0；</span><br><span class="line">这就是为什么计算新的香农熵的时候使用的是子集。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 构造树的数据结构</p>
</blockquote>
<p>创建树的函数代码如下: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span>(<span class="params">dataSet, labels</span>):</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件: 所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件: 使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择最优的列，得到最优列对应的label含义</span></span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取label的名称</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    <span class="comment"># 初始化myTree</span></span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 注: labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改</span></span><br><span class="line">    <span class="comment"># 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示&#x27;no surfacing&#x27; is not in list</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    <span class="comment"># 取出最优列，然后它的branch做分类</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        <span class="comment"># 求出剩余的标签label</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">        <span class="comment"># print &#x27;myTree&#x27;, value, myTree</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用决策树执行分类</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">inputTree, featLabels, testVec</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;classify(给输入的节点，进行分类)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inputTree  决策树模型</span></span><br><span class="line"><span class="string">        featLabels Feature标签对应的名称</span></span><br><span class="line"><span class="string">        testVec    测试输入的数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        classLabel 分类的结果值，需要映射label才能知道名称</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取tree的根节点对于的key值</span></span><br><span class="line">    firstStr = <span class="built_in">list</span>(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 通过key得到根节点对应的value</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;+++&#x27;</span>, firstStr, <span class="string">&#x27;xxx&#x27;</span>, secondDict, <span class="string">&#x27;---&#x27;</span>, key, <span class="string">&#x27;&gt;&gt;&gt;&#x27;</span>, valueOfFeat</span><br><span class="line">    <span class="comment"># 判断分枝是否结束: 判断valueOfFeat是否是dict类型</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(valueOfFeat, <span class="built_in">dict</span>):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</p>
</blockquote>

        <h3 id="项目案例2-使用决策树预测隐形眼镜类型"   >
          <a href="#项目案例2-使用决策树预测隐形眼镜类型" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例2-使用决策树预测隐形眼镜类型" class="headerlink" title="项目案例2: 使用决策树预测隐形眼镜类型"></a>项目案例2: 使用决策树预测隐形眼镜类型</h3>
      <p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <ol>
<li>收集数据: 提供的文本文件。</li>
<li>解析数据: 解析 tab 键分隔的数据行</li>
<li>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</li>
<li>训练算法: 使用 createTree() 函数。</li>
<li>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</li>
<li>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</li>
</ol>
<blockquote>
<p>收集数据: 提供的文本文件</p>
</blockquote>
<p>文本文件数据格式如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">young	myope	no	reduced	no lenses</span><br><span class="line">pre	myope	no	reduced	no lenses</span><br><span class="line">presbyopic	myope	no	reduced	no lenses</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>解析数据: 解析 tab 键分隔的数据行</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lecses = [inst.strip().split(<span class="string">&#x27;\t&#x27;</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLabels = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;prescript&#x27;</span>, <span class="string">&#x27;astigmatic&#x27;</span>, <span class="string">&#x27;tearRate&#x27;</span>]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>treePlotter.createPlot(lensesTree)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 使用 createTree() 函数</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree = trees.createTree(lenses, lensesLabels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree</span><br><span class="line">&#123;<span class="string">&#x27;tearRate&#x27;</span>: &#123;<span class="string">&#x27;reduced&#x27;</span>: <span class="string">&#x27;no lenses&#x27;</span>, <span class="string">&#x27;normal&#x27;</span>: &#123;<span class="string">&#x27;astigmatic&#x27;</span>:&#123;<span class="string">&#x27;yes&#x27;</span>:</span><br><span class="line">&#123;<span class="string">&#x27;prescript&#x27;</span>:&#123;<span class="string">&#x27;hyper&#x27;</span>:&#123;<span class="string">&#x27;age&#x27;</span>:&#123;<span class="string">&#x27;pre&#x27;</span>:<span class="string">&#x27;no lenses&#x27;</span>, <span class="string">&#x27;presbyopic&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;no lenses&#x27;</span>, <span class="string">&#x27;young&#x27;</span>:<span class="string">&#x27;hard&#x27;</span>&#125;&#125;, <span class="string">&#x27;myope&#x27;</span>:<span class="string">&#x27;hard&#x27;</span>&#125;&#125;, <span class="string">&#x27;no&#x27;</span>:&#123;<span class="string">&#x27;age&#x27;</span>:&#123;<span class="string">&#x27;pre&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;soft&#x27;</span>, <span class="string">&#x27;presbyopic&#x27;</span>:&#123;<span class="string">&#x27;prescript&#x27;</span>: &#123;<span class="string">&#x27;hyper&#x27;</span>:<span class="string">&#x27;soft&#x27;</span>, <span class="string">&#x27;myope&#x27;</span>:</span><br><span class="line"><span class="string">&#x27;no lenses&#x27;</span>&#125;&#125;, <span class="string">&#x27;young&#x27;</span>:<span class="string">&#x27;soft&#x27;</span>&#125;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</p>
</blockquote>
<blockquote>
<p>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</p>
</blockquote>
<p>使用 pickle 模块存储决策树</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span>(<span class="params">inputTree, filename</span>):</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span>(<span class="params">filename</span>):</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></div></figure>

<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong>        </li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/02/ml_2/">第2章 k-近邻算法</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-02</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="/img/knn_headPage_xy.png" alt="k-近邻算法_首页" title="k-近邻算法首页"></p>

        <h2 id="KNN-概述"   >
          <a href="#KNN-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#KNN-概述" class="headerlink" title="KNN 概述"></a>KNN 概述</h2>
      <p><code>k-近邻（kNN, k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。</code></p>
<p><strong>一句话总结: 近朱者赤近墨者黑！</strong> </p>
<p><code>k 近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k 近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显式的学习过程。</code></p>
<p><code>k 近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。</code></p>

        <h2 id="KNN-场景"   >
          <a href="#KNN-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#KNN-场景" class="headerlink" title="KNN 场景"></a>KNN 场景</h2>
      <p>电影可以按照题材分类，那么如何区分 <code>动作片</code> 和 <code>爱情片</code> 呢？<br/></p>
<ol>
<li>动作片: 打斗次数更多</li>
<li>爱情片: 亲吻次数更多</li>
</ol>
<p>基于电影中的亲吻、打斗出现的次数，使用 k-近邻算法构造程序，就可以自动划分电影的题材类型。</p>
<p><img src="/img/knn-1-movie.png" alt="电影视频案例" title="电影视频案例"></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">现在根据上面我们得到的样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到 k 个距离最近的电影。</span><br><span class="line">假定 k=3，则三个最靠近的电影依次是， He&#x27;s Not Really into Dudes 、 Beautiful Woman 和 California Man。</span><br><span class="line">knn 算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。</span><br></pre></td></tr></table></div></figure>


        <h2 id="KNN-原理"   >
          <a href="#KNN-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#KNN-原理" class="headerlink" title="KNN 原理"></a>KNN 原理</h2>
      <blockquote>
<p>KNN 工作原理</p>
</blockquote>
<ol>
<li>假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。</li>
<li>输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。<ol>
<li>计算新数据与样本数据集中每条数据的距离。</li>
<li>对求得的所有距离进行排序（从小到大，越小表示越相似）。</li>
<li>取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。</li>
</ol>
</li>
<li>求 k 个数据中出现次数最多的分类标签作为新数据的分类。</li>
</ol>
<blockquote>
<p>KNN 通俗理解</p>
</blockquote>
<p>给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。</p>
<blockquote>
<p>KNN 开发流程</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 任何方法</span><br><span class="line">准备数据: 距离计算所需要的数值，最好是结构化的数据格式</span><br><span class="line">分析数据: 任何方法</span><br><span class="line">训练算法: 此步骤不适用于 k-近邻算法</span><br><span class="line">测试算法: 计算错误率</span><br><span class="line">使用算法: 输入样本数据和结构化的输出结果，然后运行 k-近邻算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>KNN 算法特点</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 精度高、对异常值不敏感、无数据输入假定</span><br><span class="line">缺点: 计算复杂度高、空间复杂度高</span><br><span class="line">适用数据范围: 数值型和标称型</span><br></pre></td></tr></table></div></figure>


        <h2 id="KNN-项目案例"   >
          <a href="#KNN-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#KNN-项目案例" class="headerlink" title="KNN 项目案例"></a>KNN 项目案例</h2>
      
        <h3 id="项目案例1-优化约会网站的配对效果"   >
          <a href="#项目案例1-优化约会网站的配对效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-优化约会网站的配对效果" class="headerlink" title="项目案例1: 优化约会网站的配对效果"></a>项目案例1: 优化约会网站的配对效果</h3>
      <p><a href="/src/py2.x/ml/2.KNN/kNN.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>海伦使用约会网站寻找约会对象。经过一段时间之后，她发现曾交往过三种类型的人:</p>
<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人</li>
</ul>
<p>她希望: </p>
<ol>
<li>工作日与魅力一般的人约会</li>
<li>周末与极具魅力的人约会</li>
<li>不喜欢的人则直接排除掉</li>
</ol>
<p>现在她收集到了一些约会网站未曾记录的数据信息，这更有助于匹配对象的归类。</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 提供文本文件</span><br><span class="line">准备数据: 使用 Python 解析文本文件</span><br><span class="line">分析数据: 使用 Matplotlib 画二维散点图</span><br><span class="line">训练算法: 此步骤不适用于 k-近邻算法</span><br><span class="line">测试算法: 使用海伦提供的部分数据作为测试样本。</span><br><span class="line">        测试样本和非测试样本的区别在于: </span><br><span class="line">            测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。</span><br><span class="line">使用算法: 产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>海伦把这些约会对象的数据存放在文本文件 <a href="/data/2.KNN/datingTestSet2.txt">datingTestSet2.txt</a> 中，总共有 1000 行。海伦约会的对象主要包含以下 3 种特征: </p>
<ul>
<li>每年获得的飞行常客里程数</li>
<li>玩视频游戏所耗时间百分比</li>
<li>每周消费的冰淇淋公升数</li>
</ul>
<p>文本文件数据格式如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">40920	8.326976	0.953952	3</span><br><span class="line">14488	7.153469	1.673904	2</span><br><span class="line">26052	1.441871	0.805124	1</span><br><span class="line">75136	13.147394	0.428964	1</span><br><span class="line">38344	1.669788	0.134296	1</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>准备数据: 使用 Python 解析文本文件</p>
</blockquote>
<p>将文本记录转换为 NumPy 的解析程序</p>
 <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span>(<span class="params">filename</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        导入训练数据</span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">        filename: 数据文件路径</span></span><br><span class="line"><span class="string">    return: </span></span><br><span class="line"><span class="string">        数据矩阵 returnMat 和对应的类别 classLabelVector</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    fr = <span class="built_in">open</span>(filename)</span><br><span class="line">    <span class="comment"># 获得文件中的数据行的行数</span></span><br><span class="line">    numberOfLines = <span class="built_in">len</span>(fr.readlines())</span><br><span class="line">    <span class="comment"># 生成对应的空矩阵</span></span><br><span class="line">    <span class="comment"># 例如: zeros(2，3)就是生成一个 2*3的矩阵，各个位置上全是 0 </span></span><br><span class="line">    returnMat = zeros((numberOfLines, <span class="number">3</span>))  <span class="comment"># prepare matrix to return</span></span><br><span class="line">    classLabelVector = []  <span class="comment"># prepare labels return</span></span><br><span class="line">    fr = <span class="built_in">open</span>(filename)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># str.strip([chars]) --返回已移除字符串头尾指定字符所生成的新字符串</span></span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment"># 以 &#x27;\t&#x27; 切割字符串</span></span><br><span class="line">        listFromLine = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># 每列的属性数据</span></span><br><span class="line">        returnMat[index, :] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 每列的类别数据，就是 label 标签数据</span></span><br><span class="line">        classLabelVector.append(<span class="built_in">int</span>(listFromLine[-<span class="number">1</span>]))</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回数据矩阵returnMat和对应的类别classLabelVector</span></span><br><span class="line">    <span class="keyword">return</span> returnMat, classLabelVector</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 使用 Matplotlib 画二维散点图</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(datingDataMat[:, <span class="number">0</span>], datingDataMat[:, <span class="number">1</span>], <span class="number">15.0</span>*array(datingLabels), <span class="number">15.0</span>*array(datingLabels))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></div></figure>

<p>下图中采用矩阵的第一和第二列属性得到很好的展示效果，清晰地标识了三个不同的样本分类区域，具有不同爱好的人其类别区域也不同。</p>
<p><img src="/img/knn_matplotlib_2.png" alt="Matplotlib 散点图"></p>
<ul>
<li>归一化数据 （归一化是一个让权重变为统一的过程，更多细节请参考:  <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/question/19951858" >https://www.zhihu.com/question/19951858</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> ）</li>
</ul>
<div class="table-container"><table>
<thead>
<tr>
<th>序号</th>
<th align="center">玩视频游戏所耗时间百分比</th>
<th align="right">每年获得的飞行常客里程数</th>
<th align="right">每周消费的冰淇淋公升数</th>
<th align="right">样本分类</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td align="center">0.8</td>
<td align="right">400</td>
<td align="right">0.5</td>
<td align="right">1</td>
</tr>
<tr>
<td>2</td>
<td align="center">12</td>
<td align="right">134 000</td>
<td align="right">0.9</td>
<td align="right">3</td>
</tr>
<tr>
<td>3</td>
<td align="center">0</td>
<td align="right">20 000</td>
<td align="right">1.1</td>
<td align="right">2</td>
</tr>
<tr>
<td>4</td>
<td align="center">67</td>
<td align="right">32 000</td>
<td align="right">0.1</td>
<td align="right">2</td>
</tr>
</tbody></table></div>
<p>样本3和样本4的距离:<br>$$\sqrt{(0-67)^2 + (20000-32000)^2 + (1.1-0.1)^2 }$$</p>
<p>归一化特征值，消除特征之间量级不同导致的影响</p>
<p>**归一化定义: ** 我是这样认为的，归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。 方法有如下: </p>
<ol>
<li><p>线性函数转换，表达式如下: 　　</p>
<p> y=(x-MinValue)/(MaxValue-MinValue)　　</p>
<p> 说明: x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。　　</p>
</li>
<li><p>对数函数转换，表达式如下: 　　</p>
<p> y=log10(x)　　</p>
<p> 说明: 以10为底的对数函数转换。</p>
<p> 如图: </p>
<p> <img src="/img/knn_1.png" alt="对数函数图像"></p>
</li>
<li><p>反余切函数转换，表达式如下: </p>
<p> y=arctan(x)*2/PI　</p>
<p> 如图: </p>
<p> <img src="/img/arctan_arccot.gif" alt="反余切函数图像"></p>
</li>
<li><p>式(1)将输入值换算为[-1,1]区间的值，在输出层用式(2)换算回初始值，其中和分别表示训练样本集中负荷的最大值和最小值。　</p>
</li>
</ol>
<p>在统计学中，归一化的具体作用是归纳统一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1–+1之间是统计的坐标分布。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        归一化特征值，消除特征之间量级不同导致的影响</span></span><br><span class="line"><span class="string">    parameter:</span></span><br><span class="line"><span class="string">        dataSet: 数据集</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        归一化后的数据集 normDataSet. ranges和minVals即最小值与范围，并没有用到</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    归一化公式: </span></span><br><span class="line"><span class="string">        Y = (X-Xmin)/(Xmax-Xmin)</span></span><br><span class="line"><span class="string">        其中的 min 和 max 分别是数据集中的最小特征值和最大特征值。该函数可以自动将数字特征值转化为0到1的区间。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算每种属性的最大值、最小值、范围</span></span><br><span class="line">    minVals = dataSet.<span class="built_in">min</span>(<span class="number">0</span>)</span><br><span class="line">    maxVals = dataSet.<span class="built_in">max</span>(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 极差</span></span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    normDataSet = zeros(shape(dataSet))</span><br><span class="line">    m = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 生成与最小值之差组成的矩阵</span></span><br><span class="line">    normDataSet = dataSet - tile(minVals, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 将最小值之差除以范围组成矩阵</span></span><br><span class="line">    normDataSet = normDataSet / tile(ranges, (m, <span class="number">1</span>))  <span class="comment"># element wise divide</span></span><br><span class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 此步骤不适用于 k-近邻算法</p>
</blockquote>
<p>因为测试数据每一次都要与全量的训练数据进行比较，所以这个过程是没有必要的。</p>
<p>kNN 算法伪代码: </p>
<pre><code>对于每一个在数据集中的数据点: 
    计算目标的数据点（需要分类的数据点）与该数据点的距离
    将距离排序: 从小到大
    选取前K个最短距离
    选取这K个中最多的分类类别
    返回该类别来作为目标数据点的预测值
</code></pre>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span>(<span class="params">inX, dataSet, labels, k</span>):</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#距离度量 度量公式为欧氏距离</span></span><br><span class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) – dataSet</span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将距离排序: 从小到大</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    <span class="comment">#选取前K个最短距离， 选取这K个中最多的分类类别</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k): </span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span> </span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></div></figure>


<blockquote>
<p>测试算法: 使用海伦提供的部分数据作为测试样本。如果预测分类与实际类别不同，则标记为一个错误。</p>
</blockquote>
<p>kNN 分类器针对约会网站的测试代码</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对约会网站的测试方法</span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        错误数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 设置测试数据的的一个比例（训练数据集比例=1-hoRatio）</span></span><br><span class="line">    hoRatio = <span class="number">0.1</span>  <span class="comment"># 测试范围,一部分测试一部分作为样本</span></span><br><span class="line">    <span class="comment"># 从文件中加载数据</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">&#x27;data/2.KNN/datingTestSet2.txt&#x27;</span>)  <span class="comment"># load data setfrom file</span></span><br><span class="line">    <span class="comment"># 归一化数据</span></span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    <span class="comment"># m 表示数据的行数，即矩阵的第一维</span></span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 设置测试的样本数量， numTestVecs:m表示训练样本的数量</span></span><br><span class="line">    numTestVecs = <span class="built_in">int</span>(m * hoRatio)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;numTestVecs=&#x27;</span>, numTestVecs</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTestVecs):</span><br><span class="line">        <span class="comment"># 对数据测试</span></span><br><span class="line">        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], <span class="number">3</span>)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;the classifier came back with: %d, the real answer is: %d&quot;</span> % (classifierResult, datingLabels[i])</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != datingLabels[i]): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;the total error rate is: %f&quot;</span> % (errorCount / <span class="built_in">float</span>(numTestVecs))</span><br><span class="line">    <span class="built_in">print</span> errorCount</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。</p>
</blockquote>
<p>约会网站预测函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyPerson</span>():</span></span><br><span class="line">    resultList = [<span class="string">&#x27;not at all&#x27;</span>, <span class="string">&#x27;in small doses&#x27;</span>, <span class="string">&#x27;in large doses&#x27;</span>]</span><br><span class="line">    percentTats = <span class="built_in">float</span>(raw_input(<span class="string">&quot;percentage of time spent playing video games ?&quot;</span>))</span><br><span class="line">    ffMiles = <span class="built_in">float</span>(raw_input(<span class="string">&quot;frequent filer miles earned per year?&quot;</span>))</span><br><span class="line">    iceCream = <span class="built_in">float</span>(raw_input(<span class="string">&quot;liters of ice cream consumed per year?&quot;</span>))</span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">&#x27;datingTestSet2.txt&#x27;</span>)</span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    inArr = array([ffMiles, percentTats, iceCream])</span><br><span class="line">    classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels, <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;You will probably like this person: &quot;</span>, resultList[classifierResult - <span class="number">1</span>]</span><br></pre></td></tr></table></div></figure>

<p>实际运行效果如下: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classifyPerson()</span><br><span class="line">percentage of time spent playing video games?<span class="number">10</span></span><br><span class="line">frequent flier miles earned per year?<span class="number">10000</span></span><br><span class="line">liters of ice cream consumed per year?<span class="number">0.5</span></span><br><span class="line">You will probably like this person: <span class="keyword">in</span> small doses</span><br></pre></td></tr></table></div></figure>




        <h3 id="项目案例2-手写数字识别系统"   >
          <a href="#项目案例2-手写数字识别系统" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例2-手写数字识别系统" class="headerlink" title="项目案例2: 手写数字识别系统"></a>项目案例2: 手写数字识别系统</h3>
      <p><a href="/src/py2.x/ml/2.KNN/kNN.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>构造一个能识别数字 0 到 9 的基于 KNN 分类器的手写数字识别系统。</p>
<p>需要识别的数字是存储在文本文件中的具有相同的色彩和大小: 宽高是 32 像素 * 32 像素的黑白图像。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 提供文本文件。</span><br><span class="line">准备数据: 编写函数 img2vector(), 将图像格式转换为分类器使用的向量格式</span><br><span class="line">分析数据: 在 Python 命令提示符中检查数据，确保它符合要求</span><br><span class="line">训练算法: 此步骤不适用于 KNN</span><br><span class="line">测试算法: 编写函数使用提供的部分数据集作为测试样本，测试样本与非测试样本的</span><br><span class="line">         区别在于测试样本是已经完成分类的数据，如果预测分类与实际类别不同，</span><br><span class="line">         则标记为一个错误</span><br><span class="line">使用算法: 本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取</span><br><span class="line">         数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>目录 <a href="/data/2.KNN/trainingDigits">trainingDigits</a> 中包含了大约 2000 个例子，每个例子内容如下图所示，每个数字大约有 200 个样本；目录 <a href="/data/2.KNN/testDigits">testDigits</a> 中包含了大约 900 个测试数据。</p>
<p><img src="/img/knn_2_handWriting.png" alt="手写数字数据集的例子"></p>
<blockquote>
<p>准备数据: 编写函数 img2vector(), 将图像文本数据转换为分类器使用的向量</p>
</blockquote>
<p>将图像文本数据转换为向量</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span>(<span class="params">filename</span>):</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = <span class="built_in">open</span>(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = <span class="built_in">int</span>(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 在 Python 命令提示符中检查数据，确保它符合要求</p>
</blockquote>
<p>在 Python 命令行中输入下列命令测试 img2vector 函数，然后与文本编辑器打开的文件进行比较: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector = kNN.img2vector(<span class="string">&#x27;testDigits/0_13.txt&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector[<span class="number">0</span>,<span class="number">0</span>:<span class="number">32</span>]</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector[<span class="number">0</span>,<span class="number">32</span>:<span class="number">64</span>]</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>训练算法: 此步骤不适用于 KNN</p>
</blockquote>
<p>因为测试数据每一次都要与全量的训练数据进行比较，所以这个过程是没有必要的。</p>
<blockquote>
<p>测试算法: 编写函数使用提供的部分数据集作为测试样本，如果预测分类与实际类别不同，则标记为一个错误</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span>():</span></span><br><span class="line">    <span class="comment"># 1. 导入训练数据</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(<span class="string">&#x27;data/2.KNN/trainingDigits&#x27;</span>)  <span class="comment"># load the training set</span></span><br><span class="line">    m = <span class="built_in">len</span>(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="comment"># hwLabels存储0～9对应的index位置， trainingMat存放的每个位置对应的图片向量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = <span class="built_in">int</span>(fileStr.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        <span class="comment"># 将 32*32的矩阵-&gt;1*1024的矩阵</span></span><br><span class="line">        trainingMat[i, :] = img2vector(<span class="string">&#x27;data/2.KNN/trainingDigits/%s&#x27;</span> % fileNameStr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 导入测试数据</span></span><br><span class="line">    testFileList = listdir(<span class="string">&#x27;data/2.KNN/testDigits&#x27;</span>)  <span class="comment"># iterate through the test set</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = <span class="built_in">len</span>(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = <span class="built_in">int</span>(fileStr.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">&#x27;data/2.KNN/testDigits/%s&#x27;</span> % fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;the classifier came back with: %d, the real answer is: %d&quot;</span> % (classifierResult, classNumStr)</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;\nthe total number of errors is: %d&quot;</span> % errorCount</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;\nthe total error rate is: %f&quot;</span> % (errorCount / <span class="built_in">float</span>(mTest))</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统。</p>
</blockquote>

        <h2 id="KNN-小结"   >
          <a href="#KNN-小结" class="heading-link"><i class="fas fa-link"></i></a><a href="#KNN-小结" class="headerlink" title="KNN 小结"></a>KNN 小结</h2>
      <p>KNN 是什么？定义:  监督学习？ 非监督学习？</p>
<p>KNN 是一个简单的无显示学习过程，非泛化学习的监督学习模型。在分类和回归中均有应用。</p>

        <h3 id="基本原理"   >
          <a href="#基本原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3>
      <p>简单来说:  通过距离度量来计算查询点（query point）与每个训练数据点的距离，然后选出与查询点（query point）相近的K个最邻点（K nearest neighbors），使用分类决策来选出对应的标签来作为该查询点的标签。</p>

        <h3 id="KNN-三要素"   >
          <a href="#KNN-三要素" class="heading-link"><i class="fas fa-link"></i></a><a href="#KNN-三要素" class="headerlink" title="KNN 三要素"></a>KNN 三要素</h3>
      <blockquote>
<p>K, K的取值</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>对查询点标签影响显著（效果拔群）。k值小的时候 近似误差小，估计误差大。 k值大 近似误差大，估计误差小。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果选择较大的 k 值，就相当于用较大的邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。 k 值的增大就意味着整体的模型变得简单。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>太大太小都不太好，可以用交叉验证（cross validation）来选取适合的k值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>近似误差和估计误差，请看这里: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.zhihu.com/question/60793482" >https://www.zhihu.com/question/60793482</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>距离度量 Metric/Distance Measure </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>距离度量 通常为 欧式距离（Euclidean distance），还可以是 Minkowski 距离 或者 曼哈顿距离。也可以是 地理空间中的一些距离公式。（更多细节可以参看 sklearn 中 valid_metric 部分）</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>分类决策 （decision rule）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>分类决策 在 分类问题中 通常为通过少数服从多数 来选取票数最多的标签，在回归问题中通常为 K个最邻点的标签的平均值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

        <h3 id="算法-（sklearn-上有三种）"   >
          <a href="#算法-（sklearn-上有三种）" class="heading-link"><i class="fas fa-link"></i></a><a href="#算法-（sklearn-上有三种）" class="headerlink" title="算法: （sklearn 上有三种）"></a>算法: （sklearn 上有三种）</h3>
      <blockquote>
<p>Brute Force 暴力计算/线性扫描 </p>
</blockquote>
<blockquote>
<p>KD Tree 使用二叉树根据数据维度来平分参数空间。</p>
</blockquote>
<blockquote>
<p>Ball Tree 使用一系列的超球体来平分训练数据集。</p>
</blockquote>
<blockquote>
<p>树结构的算法都有建树和查询两个过程。Brute Force 没有建树的过程。</p>
</blockquote>
<blockquote>
<p>算法特点:    </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>优点:  High Accuracy， No Assumption on data， not sensitive to outliers</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>缺点: 时间和空间复杂度 高</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>适用范围:  continuous values and nominal values</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>相似同源产物:  </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>radius neighbors 根据制定的半径来找寻邻点</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>影响算法因素: </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>N 数据集样本数量(number of samples)， D 数据维度 (number of features)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>总消耗: </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force:  O[DN^2] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>此处考虑的是最蠢的方法: 把所有训练的点之间的距离都算一遍。当然有更快的实现方式, 比如 O(ND + kN)  和  O(NDK) , 最快的是 O[DN] 。感兴趣的可以阅读这个链接:  <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity" >k-NN computational complexity</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>KD Tree: O[DN log(N)] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Ball Tree: O[DN log(N)] 跟 KD Tree 处于相同的数量级，虽然建树时间会比 KD Tree 久一点，但是在高结构的数据，甚至是高纬度的数据中，查询速度有很大的提升。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>查询所需消耗:</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force:  O[DN] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>KD Tree: 当维度比较小的时候， 比如 D&lt;20,  O[Dlog(N)] 。相反，将会趋向于 O[DN] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Ball Tree: O[Dlog(N)] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>当数据集比较小的时候，比如 N&lt;30的时候，Brute Force 更有优势。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Intrinsic Dimensionality(本征维数) 和 Sparsity（稀疏度）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>数据的 intrinsic dimensionality 是指数据所在的流形的维数 d &lt; D , 在参数空间可以是线性或非线性的。稀疏度指的是数据填充参数空间的程度(这与“稀疏”矩阵中使用的概念不同, 数据矩阵可能没有零项, 但是从这个意义上来讲,它的结构 仍然是 “稀疏” 的)。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force 的查询时间不受影响。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>对于 KD Tree 和 Ball Tree的查询时间, 较小本征维数且更稀疏的数据集的查询时间更快。KD Tree 的改善由于通过坐标轴来平分参数空间的自身特性 没有Ball Tree 显著。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>k的取值 (k 个邻点)</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force 的查询时间基本不受影响。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>但是对于 KD Tree 和 Ball Tree , k越大，查询时间越慢。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>k 在N的占比较大的时候，使用 Brute Force 比较好。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Number of Query Points （查询点数量， 即测试数据的数量）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>查询点较少的时候用Brute Force。查询点较多的时候可以使用树结构算法。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>关于 sklearn 中模型的一些额外干货: </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果KD Tree，Ball Tree 和Brute Force 应用场景傻傻分不清楚，可以直接使用 含有algorithm=’auto’的模组。 algorithm=’auto’ 自动为您选择最优算法。<br>有 regressor 和 classifier 可以来选择。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>metric/distance measure 可以选择。 另外距离 可以通过weight 来加权。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>leaf size 对KD Tree 和 Ball Tree 的影响</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>建树时间: leaf size 比较大的时候，建树时间也就快点。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>查询时间:  leaf size 太大太小都不太好。如果leaf size 趋向于 N（训练数据的样本数量），算法其实就是 brute force了。如果leaf size 太小了，趋向于1，那查询的时候 遍历树的时间就会大大增加。leaf size 建议的数值是 30，也就是默认值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>内存:  leaf size 变大，存树结构的内存变小。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Nearest Centroid Classifier</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>分类决策是哪个标签的质心与测试点最近，就选哪个标签。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>该模型假设在所有维度中方差相同。 是一个很好的base line。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>进阶版:  Nearest Shrunken Centroid </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>可以通过shrink_threshold来设置。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>作用:  可以移除某些影响分类的特征，例如移除噪音特征的影响</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuxin" >羊三</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/01/ml_1/">第1章 机器学习基础</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-01</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%A6%96%E9%A1%B5.jpg" alt="机器学习基础_首页"></p>

        <h2 id="机器学习-概述"   >
          <a href="#机器学习-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-概述" class="headerlink" title="机器学习 概述"></a>机器学习 概述</h2>
      <p><code>机器学习(Machine Learning,ML)</code> 是使用计算机来彰显数据背后的真实含义，它为了把无序的数据转换成有用的信息。是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。<br>它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。</p>
<ol>
<li>海量的数据</li>
<li>获取有用的信息</li>
</ol>

        <h2 id="机器学习-研究意义"   >
          <a href="#机器学习-研究意义" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-研究意义" class="headerlink" title="机器学习 研究意义"></a>机器学习 研究意义</h2>
      <p>机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。 “机器学习是对能通过经验自动改进的计算机算法的研究”。 “机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p>机器学习已经有了十分广泛的应用，例如: 数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。</p>

        <h2 id="机器学习-场景"   >
          <a href="#机器学习-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-场景" class="headerlink" title="机器学习 场景"></a>机器学习 场景</h2>
      <ul>
<li><p>例如: 识别动物猫</p>
<ul>
<li>模式识别（官方标准）: 人们通过大量的经验，得到结论，从而判断它就是猫。</li>
<li>机器学习（数据学习）: 人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。</li>
<li>深度学习（深入数据）: 人们通过深入了解它，发现它会’喵喵’的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域: 语音识别、图像识别）</li>
</ul>
</li>
<li><p>模式识别（pattern recognition）: 模式识别是最古老的（作为一个术语而言，可以说是很过时的）。</p>
<ul>
<li>我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。</li>
<li>通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。</li>
</ul>
</li>
<li><p>机器学习（machine learning）: 机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。</p>
<ul>
<li>在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。</li>
<li>“机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。</li>
<li>机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。</li>
</ul>
</li>
<li><p>深度学习（deep learning）: 深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。</p>
<ul>
<li>深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</li>
</ul>
</li>
<li><p>参考地址:  </p>
<ul>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.csdn.net/article/2015-03-24/2824301" >深度学习 vs 机器学习 vs 模式识别</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q" >深度学习 百科资料</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</li>
</ul>
<blockquote>
<p>机器学习已应用于多个领域，远远超出大多数人的想象，横跨: 计算机科学、工程技术和统计学等多个学科。</p>
</blockquote>
<ul>
<li>搜索引擎: 根据你的搜索点击，优化你下次的搜索结果,是机器学习来帮助搜索引擎判断哪个结果更适合你（也判断哪个广告更适合你）。</li>
<li>垃圾邮件: 会自动的过滤垃圾广告邮件到垃圾箱内。</li>
<li>超市优惠券: 你会发现，你在购买小孩子尿布的时候，售货员会赠送你一张优惠券可以兑换6罐啤酒。</li>
<li>邮局邮寄: 手写软件自动识别寄送贺卡的地址。</li>
<li>申请贷款: 通过你最近的金融活动信息进行综合评定，决定你是否合格。</li>
</ul>

        <h2 id="机器学习-组成"   >
          <a href="#机器学习-组成" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-组成" class="headerlink" title="机器学习 组成"></a>机器学习 组成</h2>
      
        <h3 id="主要任务"   >
          <a href="#主要任务" class="heading-link"><i class="fas fa-link"></i></a><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h3>
      <ul>
<li>分类（classification）: 将实例数据划分到合适的类别中。<ul>
<li>应用实例: 判断网站是否被黑客入侵（二分类 ），手写数字的自动识别（多分类）</li>
</ul>
</li>
<li>回归（regression）: 主要用于预测数值型数据。<ul>
<li>应用实例: 股票价格波动的预测，房屋价格的预测等。</li>
</ul>
</li>
</ul>

        <h3 id="监督学习（supervised-learning）"   >
          <a href="#监督学习（supervised-learning）" class="heading-link"><i class="fas fa-link"></i></a><a href="#监督学习（supervised-learning）" class="headerlink" title="监督学习（supervised learning）"></a>监督学习（supervised learning）</h3>
      <ul>
<li>必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括: 分类和回归)</li>
<li>样本集: 训练数据 + 测试数据<ul>
<li>训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)</li>
<li>特征通常是训练样本集的列，它们是独立测量得到的。</li>
<li>目标变量: 目标变量是机器学习预测算法的测试结果。<ul>
<li>在分类算法中目标变量的类型通常是标称型(如: 真与假)，而在回归算法中通常是连续型(如: 1~100)。</li>
</ul>
</li>
</ul>
</li>
<li>监督学习需要注意的问题: <ul>
<li>偏置方差权衡</li>
<li>功能的复杂性和数量的训练数据</li>
<li>输入空间的维数</li>
<li>噪声中的输出值</li>
</ul>
</li>
<li><code>知识表示</code>: <ul>
<li>可以采用规则集的形式【例如: 数学成绩大于90分为优秀】</li>
<li>可以采用概率分布的形式【例如: 通过统计分布发现，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】</li>
<li>可以使用训练样本集中的一个实例【例如: 通过样本集合，我们训练出一个模型实例，得出 年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】</li>
</ul>
</li>
</ul>

        <h3 id="非监督学习（unsupervised-learning）"   >
          <a href="#非监督学习（unsupervised-learning）" class="heading-link"><i class="fas fa-link"></i></a><a href="#非监督学习（unsupervised-learning）" class="headerlink" title="非监督学习（unsupervised learning）"></a>非监督学习（unsupervised learning）</h3>
      <ul>
<li>在机器学习，无监督学习的问题是，在未加标签的数据中，试图找到隐藏的结构。因为提供给学习者的实例是未标记的，因此没有错误或报酬信号来评估潜在的解决方案。</li>
<li>无监督学习是密切相关的统计数据密度估计的问题。然而无监督学习还包括寻求，总结和解释数据的主要特点等诸多技术。在无监督学习使用的许多方法是基于用于处理数据的数据挖掘方法。</li>
<li>数据没有类别信息，也不会给定目标值。</li>
<li>非监督学习包括的类型: <ul>
<li>聚类: 在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。</li>
<li>密度估计: 通过样本分布的紧密程度，来估计与分组的相似性。</li>
<li>此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。
        <h3 id="强化学习"   >
          <a href="#强化学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3>
      这个算法可以训练程序做出某一决定。程序在某一情况下尝试所有的可能行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。 属于这一类算法的有马尔可夫决策过程。
        <h3 id="训练过程"   >
          <a href="#训练过程" class="heading-link"><i class="fas fa-link"></i></a><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3>
      </li>
</ul>
</li>
</ul>
<p><img src="/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.jpg" alt="机器学习训练过程图"></p>

        <h3 id="算法汇总"   >
          <a href="#算法汇总" class="heading-link"><i class="fas fa-link"></i></a><a href="#算法汇总" class="headerlink" title="算法汇总"></a>算法汇总</h3>
      <p><img src="/img/ml_algorithm.jpg" alt="算法汇总"></p>

        <h2 id="机器学习-使用"   >
          <a href="#机器学习-使用" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-使用" class="headerlink" title="机器学习 使用"></a>机器学习 使用</h2>
      <blockquote>
<p>选择算法需要考虑的两个问题</p>
</blockquote>
<ol>
<li>算法场景<ul>
<li>预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法</li>
<li>给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。</li>
</ul>
</li>
<li>需要收集或分析的数据是什么</li>
</ol>
<blockquote>
<p>举例</p>
</blockquote>
<p><img src="/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95.jpg" alt="选择算法图"></p>
<blockquote>
<p>机器学习 开发流程</p>
</blockquote>
<ol>
<li>收集数据: 收集样本数据</li>
<li>准备数据: 注意数据的格式</li>
<li>分析数据: 为了确保数据集中没有垃圾数据；<ul>
<li>如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；</li>
<li>另外该步骤需要人工干预，会降低自动化系统的价值。</li>
</ul>
</li>
<li>训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤</li>
<li>测试算法: [机器学习算法核心]评估算法效果</li>
<li>使用算法: 将机器学习算法转为应用程序</li>
</ol>

        <h2 id="机器学习-数学基础"   >
          <a href="#机器学习-数学基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-数学基础" class="headerlink" title="机器学习 数学基础"></a>机器学习 数学基础</h2>
      <ul>
<li>微积分</li>
<li>统计学/概率论</li>
<li>线性代数
        <h2 id="机器学习-工具"   >
          <a href="#机器学习-工具" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习-工具" class="headerlink" title="机器学习 工具"></a>机器学习 工具</h2>
      </li>
</ul>

        <h3 id="Python语言"   >
          <a href="#Python语言" class="heading-link"><i class="fas fa-link"></i></a><a href="#Python语言" class="headerlink" title="Python语言"></a>Python语言</h3>
      <ol>
<li>可执行伪代码</li>
<li>Python比较流行: 使用广泛、代码范例多、丰富模块库，开发周期短</li>
<li>Python语言的特色: 清晰简练、易于理解</li>
<li>Python语言的缺点: 唯一不足的是性能问题</li>
<li>Python相关的库<ul>
<li>科学函数库: <code>SciPy</code>、<code>NumPy</code>(底层语言: C和Fortran)</li>
<li>绘图工具库: <code>Matplotlib</code></li>
<li>数据分析库 <code>Pandas</code>
        <h3 id="数学工具"   >
          <a href="#数学工具" class="heading-link"><i class="fas fa-link"></i></a><a href="#数学工具" class="headerlink" title="数学工具"></a>数学工具</h3>
      </li>
</ul>
</li>
</ol>
<ul>
<li>Matlab
        <h2 id="附-机器学习专业术语"   >
          <a href="#附-机器学习专业术语" class="heading-link"><i class="fas fa-link"></i></a><a href="#附-机器学习专业术语" class="headerlink" title="附: 机器学习专业术语"></a>附: 机器学习专业术语</h2>
      </li>
<li>模型（model）: 计算机层面的认知</li>
<li>学习算法（learning algorithm），从数据中产生模型的方法</li>
<li>数据集（data set）: 一组记录的合集</li>
<li>示例（instance）: 对于某个对象的描述</li>
<li>样本（sample）: 也叫示例</li>
<li>属性（attribute）: 对象的某方面表现或特征</li>
<li>特征（feature）: 同属性</li>
<li>属性值（attribute value）: 属性上的取值</li>
<li>属性空间（attribute space）: 属性张成的空间</li>
<li>样本空间/输入空间（samplespace）: 同属性空间</li>
<li>特征向量（feature vector）: 在属性空间里每个点对应一个坐标向量，把一个示例称作特征向量</li>
<li>维数（dimensionality）: 描述样本参数的个数（也就是空间是几维的）</li>
<li>学习（learning）/训练（training）: 从数据中学得模型</li>
<li>训练数据（training data）: 训练过程中用到的数据</li>
<li>训练样本（training sample）:训练用到的每个样本</li>
<li>训练集（training set）: 训练样本组成的集合</li>
<li>假设（hypothesis）: 学习模型对应了关于数据的某种潜在规则</li>
<li>真相（ground-truth）:真正存在的潜在规律</li>
<li>学习器（learner）: 模型的另一种叫法，把学习算法在给定数据和参数空间的实例化</li>
<li>预测（prediction）: 判断一个东西的属性</li>
<li>标记（label）: 关于示例的结果信息，比如我是一个“好人”。</li>
<li>样例（example）: 拥有标记的示例</li>
<li>标记空间/输出空间（label space）: 所有标记的集合</li>
<li>分类（classification）: 预测是离散值，比如把人分为好人和坏人之类的学习任务</li>
<li>回归（regression）: 预测值是连续值，比如你的好人程度达到了0.9，0.6之类的</li>
<li>二分类（binary classification）: 只涉及两个类别的分类任务</li>
<li>正类（positive class）: 二分类里的一个</li>
<li>反类（negative class）: 二分类里的另外一个</li>
<li>多分类（multi-class classification）: 涉及多个类别的分类</li>
<li>测试（testing）: 学习到模型之后对样本进行预测的过程</li>
<li>测试样本（testing sample）: 被预测的样本</li>
<li>聚类（clustering）: 把训练集中的对象分为若干组</li>
<li>簇（cluster）: 每一个组叫簇</li>
<li>监督学习（supervised learning）: 典范–分类和回归</li>
<li>无监督学习（unsupervised learning）: 典范–聚类</li>
<li>未见示例（unseen instance）: “新样本“，没训练过的样本</li>
<li>泛化（generalization）能力: 学得的模型适用于新样本的能力</li>
<li>分布（distribution）: 样本空间的全体样本服从的一种规律</li>
<li>独立同分布（independent and identically distributed，简称i,i,d.）:获得的每个样本都是独立地从这个分布上采样获得的。</li>
</ul>

        <h2 id="机器学习基础补充"   >
          <a href="#机器学习基础补充" class="heading-link"><i class="fas fa-link"></i></a><a href="#机器学习基础补充" class="headerlink" title="机器学习基础补充"></a>机器学习基础补充</h2>
      
        <h3 id="数据集的划分"   >
          <a href="#数据集的划分" class="heading-link"><i class="fas fa-link"></i></a><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h3>
      <ul>
<li>训练集（Training set） —— 学习样本数据集，通过匹配一些参数来建立一个模型，主要用来训练模型。类比考研前做的解题大全。</li>
<li>验证集（validation set） —— 对学习出来的模型，调整模型的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。类比 考研之前做的模拟考试。</li>
<li>测试集（Test set） —— 测试训练好的模型的分辨能力。类比 考研。这次真的是一考定终身。</li>
</ul>

        <h3 id="模型拟合程度"   >
          <a href="#模型拟合程度" class="heading-link"><i class="fas fa-link"></i></a><a href="#模型拟合程度" class="headerlink" title="模型拟合程度"></a>模型拟合程度</h3>
      <ul>
<li>欠拟合（Underfitting）: 模型没有很好地捕捉到数据特征，不能够很好地拟合数据，对训练样本的一般性质尚未学好。类比，光看书不做题觉得自己什么都会了，上了考场才知道自己啥都不会。</li>
<li>过拟合（Overfitting）: 模型把训练样本学习“太好了”，可能把一些训练样本自身的特性当做了所有潜在样本都有的一般性质，导致泛化能力下降。类比，做课后题全都做对了，超纲题也都认为是考试必考题目，上了考场还是啥都不会。 </li>
</ul>
<p>通俗来说，欠拟合和过拟合都可以用一句话来说，欠拟合就是: “你太天真了！”，过拟合就是: “你想太多了！”。</p>

        <h3 id="常见的模型指标"   >
          <a href="#常见的模型指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#常见的模型指标" class="headerlink" title="常见的模型指标"></a>常见的模型指标</h3>
      <ul>
<li>正确率 —— 提取出的正确信息条数 / 提取出的信息条数</li>
<li>召回率 —— 提取出的正确信息条数 / 样本中的信息条数</li>
<li>F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）</li>
</ul>
<p>举个例子如下: </p>
<p>举个例子如下:<br>某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只<br>虾， 100 只乌龟。那么这些指标分别如下:<br>正确率 = 700 / (700 + 200 + 100) = 70%<br>召回率 = 700 / 1400 = 50%<br>F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%</p>

        <h3 id="模型"   >
          <a href="#模型" class="heading-link"><i class="fas fa-link"></i></a><a href="#模型" class="headerlink" title="模型"></a>模型</h3>
      <ul>
<li>分类问题 —— 说白了就是将一些未知类别的数据分到现在已知的类别中去。比如，根据你的一些信息，判断你是高富帅，还是穷屌丝。评判分类效果好坏的三个指标就是上面介绍的三个指标: 正确率，召回率，F值。</li>
<li>回归问题 —— 对数值型连续随机变量进行预测和建模的监督学习算法。回归往往会通过计算 误差（Error）来确定模型的精确性。</li>
<li>聚类问题 —— 聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。聚类问题的标准一般基于距离: 簇内距离（Intra-cluster Distance） 和 簇间距离（Inter-cluster Distance） 。簇内距离是越小越好，也就是簇内的元素越相似越好；而簇间距离越大越好，也就是说簇间（不同簇）元素越不相同越好。一般的，衡量聚类问题会给出一个结合簇内距离和簇间距离的公式。</li>
</ul>
<p>下面这个图可以比较直观地展示出来: </p>
<p><img src="/img/ml_add_1.jpg"></p>

        <h3 id="特征工程的一些小东西"   >
          <a href="#特征工程的一些小东西" class="heading-link"><i class="fas fa-link"></i></a><a href="#特征工程的一些小东西" class="headerlink" title="特征工程的一些小东西"></a>特征工程的一些小东西</h3>
      <ul>
<li><p>特征选择 —— 也叫特征子集选择（FSS，Feature Subset Selection）。是指从已有的 M 个特征（Feature）中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。</p>
</li>
<li><p>特征提取 —— 特征提取是计算机视觉和图像处理中的一个概念。它指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取的结果是把图像上的点分为不同的子集，这些子集往往属于孤立的点，连续的曲线或者连续的区域。</p>
</li>
</ul>
<p>下面给出一个特征工程的图: </p>
<p><img src="/img/ml_add_2.jpg"></p>

        <h3 id="其他"   >
          <a href="#其他" class="heading-link"><i class="fas fa-link"></i></a><a href="#其他" class="headerlink" title="其他"></a>其他</h3>
      <ul>
<li>Learning rate —— 学习率，通俗地理解，可以理解为步长，步子大了，很容易错过最佳结果。就是本来目标尽在咫尺，可是因为我迈的步子很大，却一下子走过了。步子小了呢，就是同样的距离，我却要走很多很多步，这样导致训练的耗时费力还不讨好。</li>
<li>一个总结的知识点很棒的链接 : <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25197792" >https://zhuanlan.zhihu.com/p/25197792</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~lihuisong" >1988</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/06/14/hello-world/">Hello World</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Welcome to <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/" >Hexo</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>! This is your very first post. Check <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/docs/" >documentation</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> for more info. If you get any problems when using Hexo, you can find the answer in <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html" >troubleshooting</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> or you can ask me on <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues" >GitHub</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>.</p>

        <h2 id="Quick-Start"   >
          <a href="#Quick-Start" class="heading-link"><i class="fas fa-link"></i></a><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2>
      
        <h3 id="Create-a-new-post"   >
          <a href="#Create-a-new-post" class="heading-link"><i class="fas fa-link"></i></a><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3>
      <figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></div></figure>

<p>More info: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html" >Writing</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="Run-server"   >
          <a href="#Run-server" class="heading-link"><i class="fas fa-link"></i></a><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3>
      <figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></div></figure>

<p>More info: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/docs/server.html" >Server</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="Generate-static-files"   >
          <a href="#Generate-static-files" class="heading-link"><i class="fas fa-link"></i></a><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3>
      <figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></div></figure>

<p>More info: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html" >Generating</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="Deploy-to-remote-sites"   >
          <a href="#Deploy-to-remote-sites" class="heading-link"><i class="fas fa-link"></i></a><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3>
      <figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></div></figure>

<p>More info: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html" >Deployment</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><a class="extend prev" rel="prev" href="/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>