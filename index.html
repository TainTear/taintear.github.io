<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="TainTear&#39;s Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="TainTear&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="TainTear">
<meta name="twitter:card" content="summary"><title>TainTear's Blog</title><link ref="canonical" href="http://example.com/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">Categories</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">TainTear's Blog</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/16/ml_16/">第16章 推荐系统</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-16</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="背景与挖掘目标"   >
          <a href="#背景与挖掘目标" class="heading-link"><i class="fas fa-link"></i></a><a href="#背景与挖掘目标" class="headerlink" title="背景与挖掘目标"></a>背景与挖掘目标</h2>
      <p>随着互联网的快速发展，用户很难快速从海量信息中寻找到自己感兴趣的信息。因此诞生了: 搜索引擎+推荐系统</p>
<p>本章节-推荐系统: </p>
<ol>
<li>帮助用户发现其感兴趣和可能感兴趣的信息。</li>
<li>让网站价值信息脱颖而出，得到广大用户的认可。</li>
<li>提高用户对网站的忠诚度和关注度，建立稳固用户群体。</li>
</ol>

        <h2 id="分析方法与过程"   >
          <a href="#分析方法与过程" class="heading-link"><i class="fas fa-link"></i></a><a href="#分析方法与过程" class="headerlink" title="分析方法与过程"></a>分析方法与过程</h2>
      <p>本案例的目标是对用户进行推荐，即以一定的方式将用户与物品（本次指网页）之间建立联系。</p>
<p>由于用户访问网站的数据记录很多，如果不对数据进行分类处理，对所有的记录直接采用推荐系统进行推荐，这样会存在一下问题。</p>
<ol>
<li>数据量太大意味着物品数与用户数很多，在模型构建用户与物品稀疏矩阵时，出现设备内存空间不够的情况，并且模型计算需要消耗大量的时间。</li>
<li>用户区别很大，不同的用户关注的信息不一样，因此，即使能够得到推荐结果，其效果也会不好。</li>
</ol>
<p>为了避免出现上述问题，需要进行分类处理与分析。</p>
<p>正常的情况下，需要对用户的兴趣爱好以及需求进行分类。<br>因为在用户访问记录中，没有记录用户访问页面时间的长短，因此不容易判断用户兴趣爱好。<br>因此，本文根据用户浏览的网页信息进行分析处理，主要采用以下方法处理: 以用户浏览网页的类型进行分类，然后对每个类型中的内容进行推荐。</p>
<p>分析过程如下: </p>
<ul>
<li>从系统中获取用户访问网站的原始记录。</li>
<li>对数据进行多维分析，包括用户访问内容，流失用户分析以及用户分类等分析。</li>
<li>对数据进行预处理，包含数据去重、数据变换和数据分类鞥处理过程。</li>
<li>以用户访问html后缀的页面为关键条件，对数据进行处理。</li>
<li>对比多种推荐算法进行推荐，通过模型评价，得到比较好的智能推荐模型。通过模型对样本数据进行预测，获得推荐结果。</li>
</ul>

        <h2 id="主流推荐算法"   >
          <a href="#主流推荐算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#主流推荐算法" class="headerlink" title="主流推荐算法"></a>主流推荐算法</h2>
      <div class="table-container"><table>
<thead>
<tr>
<th>推荐方法</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>基于内容推荐</td>
<td></td>
</tr>
<tr>
<td>协同过滤推荐</td>
<td></td>
</tr>
<tr>
<td>基于规则推荐</td>
<td></td>
</tr>
<tr>
<td>基于效用推荐</td>
<td></td>
</tr>
<tr>
<td>基于知识推荐</td>
<td></td>
</tr>
<tr>
<td>组合推荐</td>
<td></td>
</tr>
</tbody></table></div>
<p><img src="img/%E6%8E%A8%E8%8D%90%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94.png" alt="推荐方法对比"></p>

        <h3 id="基于知识推荐"   >
          <a href="#基于知识推荐" class="heading-link"><i class="fas fa-link"></i></a><a href="#基于知识推荐" class="headerlink" title="基于知识推荐"></a>基于知识推荐</h3>
      <p>基于知识的推荐（Knowledge-based Recommendation）在某种程度是可以看成是一种推理（Inference）技术，它不是建立在用户需要和偏好基础上推荐的。基于知识的方法因它们所用的功能知识不同而有明显区别。效用知识（Functional Knowledge）是一种关于一个项目如何满足某一特定用户的知识，因此能解释需要和推荐的关系，所以用户资料可以是任何能支持推理的知识结构，它可以是用户已经规范化的查询，也可以是一个更详细的用户需要的表示。</p>
<p><img src="img/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E7%9A%84%E6%8E%A8%E8%8D%90.jpg" alt="基于知识的推荐"></p>

        <h3 id="协同过滤推荐"   >
          <a href="#协同过滤推荐" class="heading-link"><i class="fas fa-link"></i></a><a href="#协同过滤推荐" class="headerlink" title="协同过滤推荐"></a>协同过滤推荐</h3>
      <ul>
<li>memory-based推荐<ul>
<li>Item-based方法</li>
<li>User-based方法</li>
<li>Memory-based推荐方法通过执行最近邻搜索，把每一个Item或者User看成一个向量，计算其他所有Item或者User与它的相似度。有了Item或者User之间的两两相似度之后，就可以进行预测与推荐了。 </li>
</ul>
</li>
<li>model-based推荐<ul>
<li>Model-based推荐最常见的方法为Matrix factorization.</li>
<li>矩阵分解通过把原始的评分矩阵R分解为两个矩阵相乘，并且只考虑有评分的值，训练时不考虑missing项的值。R矩阵分解成为U与V两个矩阵后，评分矩阵R中missing的值就可以通过U矩阵中的某列和V矩阵的某行相乘得到</li>
<li>矩阵分解的目标函数: U矩阵与V矩阵的可以通过梯度下降(gradient descent)算法求得，通过交替更新u与v多次迭代收敛之后可求出U与V。 </li>
<li>矩阵分解背后的核心思想，找到两个矩阵，它们相乘之后得到的那个矩阵的值，与评分矩阵R中有值的位置中的值尽可能接近。这样一来，分解出来的两个矩阵相乘就尽可能还原了评分矩阵R，因为有值的地方，值都相差得尽可能地小，那么missing的值通过这样的方式计算得到，比较符合趋势。 </li>
</ul>
</li>
<li>协同过滤中主要存在如下两个问题: 稀疏性与冷启动问题。已有的方案通常会通过引入多个不同的数据源或者辅助信息(Side information)来解决这些问题，用户的Side information可以是用户的基本个人信息、用户画像信息等，而Item的Side information可以是物品的content信息等。</li>
</ul>

        <h2 id="效果评估"   >
          <a href="#效果评估" class="heading-link"><i class="fas fa-link"></i></a><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h2>
      <ol>
<li>召回率和准确率 【人为统计分析】</li>
<li>F值(P-R曲线) 【偏重: 非均衡问题】</li>
<li>ROC和AUC  【偏重: 不同结果的对比】</li>
</ol>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
<blockquote>
<p>摘录的原文地址: </p>
</blockquote>
<ul>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.36dsj.com/archives/9519" >推荐系统中常用算法 以及优点缺点对比</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://zhidao.baidu.com/question/2013524494179442228.html" >推荐算法的基于知识推荐</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.iteye.com/news/32100" >推荐系统中基于深度学习的混合协同过滤模型</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/15/ml_15/">第15章 大数据与MapReduce</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-15</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="img/mr_headPage.jpg" alt="大数据与MapReduce首页" title="大数据与MapReduce首页"></p>

        <h2 id="大数据-概述"   >
          <a href="#大数据-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#大数据-概述" class="headerlink" title="大数据 概述"></a>大数据 概述</h2>
      <p><code>大数据: 收集到的数据已经远远超出了我们的处理能力。</code></p>

        <h2 id="大数据-场景"   >
          <a href="#大数据-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#大数据-场景" class="headerlink" title="大数据 场景"></a>大数据 场景</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">假如你为一家网络购物商店工作，很多用户访问该网站，其中有些人会购买商品，有些人则随意浏览后就离开。</span><br><span class="line">对于你来说，可能很想识别那些有购物意愿的用户。</span><br><span class="line">那么问题就来了，数据集可能会非常大，在单机上训练要运行好几天。</span><br><span class="line">接下来: 我们讲讲 MapRedece 如何来解决这样的问题</span><br></pre></td></tr></table></div></figure>



        <h2 id="MapRedece"   >
          <a href="#MapRedece" class="heading-link"><i class="fas fa-link"></i></a><a href="#MapRedece" class="headerlink" title="MapRedece"></a>MapRedece</h2>
      
        <h3 id="Hadoop-概述"   >
          <a href="#Hadoop-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#Hadoop-概述" class="headerlink" title="Hadoop 概述"></a>Hadoop 概述</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 是 MapRedece 框架的一个免费开源实现。</span><br><span class="line">MapReduce: 分布式的计算框架，可以将单个计算作业分配给多台计算机执行。</span><br></pre></td></tr></table></div></figure>


        <h3 id="MapRedece-原理"   >
          <a href="#MapRedece-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#MapRedece-原理" class="headerlink" title="MapRedece 原理"></a>MapRedece 原理</h3>
      <blockquote>
<p>MapRedece 工作原理</p>
</blockquote>
<ul>
<li>主节点控制 MapReduce 的作业流程</li>
<li>MapReduce 的作业可以分成map任务和reduce任务</li>
<li>map 任务之间不做数据交流，reduce 任务也一样</li>
<li>在 map 和 reduce 阶段中间，有一个 sort 和 combine 阶段</li>
<li>数据被重复存放在不同的机器上，以防止某个机器失效</li>
<li>mapper 和 reducer 传输的数据形式为 key/value对</li>
</ul>
<p><img src="img/mr_1_cluster.jpg" alt="MapReduce框架的示意图" title="MapReduce框架的示意图"></p>
<blockquote>
<p>MapRedece 特点</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 使程序以并行的方式执行，可在短时间内完成大量工作。</span><br><span class="line">缺点: 算法必须经过重写，需要对系统工程有一定的理解。</span><br><span class="line">适用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="Hadoop-流-Python-调用"   >
          <a href="#Hadoop-流-Python-调用" class="heading-link"><i class="fas fa-link"></i></a><a href="#Hadoop-流-Python-调用" class="headerlink" title="Hadoop 流(Python 调用)"></a>Hadoop 流(Python 调用)</h3>
      <blockquote>
<p>理论简介</p>
</blockquote>
<p>例如: Hadoop流可以像Linux命令一样执行</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat inputFile.txt | python mapper.py | sort | python reducer.py &gt; outputFile.txt</span><br></pre></td></tr></table></div></figure>

<p>类似的Hadoop流就可以在多台机器上分布式执行，用户可以通过Linux命令来测试Python语言编写的MapReduce脚本。</p>
<blockquote>
<p>实战脚本</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 测试 Mapper</span><br><span class="line"># Linux</span><br><span class="line">cat data/15.BigData_MapReduce/inputFile.txt | python src/python/15.BigData_MapReduce/mrMeanMapper.py</span><br><span class="line"># Window</span><br><span class="line"># python src/python/15.BigData_MapReduce/mrMeanMapper.py &lt; data/15.BigData_MapReduce/inputFile.txt</span><br><span class="line"></span><br><span class="line"># 测试 Reducer</span><br><span class="line"># Linux</span><br><span class="line">cat data/15.BigData_MapReduce/inputFile.txt | python src/python/15.BigData_MapReduce/mrMeanMapper.py | python src/python/15.BigData_MapReduce/mrMeanReducer.py</span><br><span class="line"># Window</span><br><span class="line"># python src/python/15.BigData_MapReduce/mrMeanMapper.py &lt; data/15.BigData_MapReduce/inputFile.txt | python src/python/15.BigData_MapReduce/mrMeanReducer.py</span><br></pre></td></tr></table></div></figure>


        <h3 id="MapReduce-机器学习"   >
          <a href="#MapReduce-机器学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#MapReduce-机器学习" class="headerlink" title="MapReduce 机器学习"></a>MapReduce 机器学习</h3>
      
        <h4 id="Mahout-in-Action"   >
          <a href="#Mahout-in-Action" class="heading-link"><i class="fas fa-link"></i></a><a href="#Mahout-in-Action" class="headerlink" title="Mahout in Action"></a>Mahout in Action</h4>
      <ol>
<li>简单贝叶斯: 它属于为数不多的可以很自然的使用MapReduce的算法。通过统计在某个类别下某特征的概率。</li>
<li>k-近邻算法: 高维数据下（如文本、图像和视频）流行的近邻查找方法是局部敏感哈希算法。</li>
<li>支持向量机(SVM): 使用随机梯度下降算法求解，如Pegasos算法。</li>
<li>奇异值分解: Lanczos算法是一个有效的求解近似特征值的算法。</li>
<li>k-均值聚类: canopy算法初始化k个簇，然后再运行K-均值求解结果。</li>
</ol>

        <h3 id="使用-mrjob-库将-MapReduce-自动化"   >
          <a href="#使用-mrjob-库将-MapReduce-自动化" class="heading-link"><i class="fas fa-link"></i></a><a href="#使用-mrjob-库将-MapReduce-自动化" class="headerlink" title="使用 mrjob 库将 MapReduce 自动化"></a>使用 mrjob 库将 MapReduce 自动化</h3>
      <blockquote>
<p>理论简介</p>
</blockquote>
<ul>
<li>MapReduce 作业流自动化的框架: Cascading 和 Oozie.</li>
<li>mrjob 是一个不错的学习工具，与2010年底实现了开源，来之于 Yelp(一个餐厅点评网站).</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python src/python/15.BigData_MapReduce/mrMean.py &lt; data/15.BigData_MapReduce/inputFile.txt &gt; data/15.BigData_MapReduce/myOut.txt</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>实战脚本</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 测试 mrjob的案例</span><br><span class="line"># 先测试一下mapper方法</span><br><span class="line"># python src/python/15.BigData_MapReduce/mrMean.py --mapper &lt; data/15.BigData_MapReduce/inputFile.txt</span><br><span class="line"># 运行整个程序，移除 --mapper 就行</span><br><span class="line">python src/python/15.BigData_MapReduce/mrMean.py &lt; data/15.BigData_MapReduce/inputFile.txt</span><br></pre></td></tr></table></div></figure>


        <h3 id="项目案例-分布式-SVM-的-Pegasos-算法"   >
          <a href="#项目案例-分布式-SVM-的-Pegasos-算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-分布式-SVM-的-Pegasos-算法" class="headerlink" title="项目案例: 分布式 SVM 的 Pegasos 算法"></a>项目案例: 分布式 SVM 的 Pegasos 算法</h3>
      <p>Pegasos是指原始估计梯度求解器(Peimal Estimated sub-GrAdient Solver)</p>

        <h4 id="Pegasos-工作原理"   >
          <a href="#Pegasos-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pegasos-工作原理" class="headerlink" title="Pegasos 工作原理"></a>Pegasos 工作原理</h4>
      <ol>
<li>从训练集中随机挑选一些样本点添加到待处理列表中</li>
<li>按序判断每个样本点是否被正确分类<ul>
<li>如果是则忽略</li>
<li>如果不是则将其加入到待更新集合。</li>
</ul>
</li>
<li>批处理完毕后，权重向量按照这些错分的样本进行更新。</li>
</ol>
<p>上述算法伪代码如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">将 回归系数w 初始化为0</span><br><span class="line">对每次批处理</span><br><span class="line">    随机选择 k 个样本点(向量)</span><br><span class="line">    对每个向量</span><br><span class="line">        如果该向量被错分: </span><br><span class="line">            更新权重向量 w</span><br><span class="line">    累加对 w 的更新</span><br></pre></td></tr></table></div></figure>


        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 数据按文本格式存放。</span><br><span class="line">准备数据: 输入数据已经是可用的格式，所以不需任何准备工作。如果你需要解析一个大规模的数据集，建议使用 map 作业来完成，从而达到并行处理的目的。</span><br><span class="line">分析数据: 无。</span><br><span class="line">训练算法: 与普通的 SVM 一样，在分类器训练上仍需花费大量的时间。</span><br><span class="line">测试算法: 在二维空间上可视化之后，观察超平面，判断算法是否有效。</span><br><span class="line">使用算法: 本例不会展示一个完整的应用，但会展示如何在大数据集上训练SVM。该算法其中一个应用场景就是本文分类，通常在文本分类里可能有大量的文档和成千上万的特征。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据</p>
</blockquote>
<p>文本文件数据格式如下: </p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.365032</span>        <span class="number">2.465645</span>        -<span class="number">1</span></span><br><span class="line">-<span class="number">2.494175</span>       -<span class="number">0.292380</span>       -<span class="number">1</span></span><br><span class="line">-<span class="number">3.039364</span>       -<span class="number">0.123108</span>       -<span class="number">1</span></span><br><span class="line"><span class="number">1.348150</span>        <span class="number">0.255696</span>        <span class="number">1</span></span><br><span class="line"><span class="number">2.768494</span>        <span class="number">1.234954</span>        <span class="number">1</span></span><br><span class="line"><span class="number">1.232328</span>        -<span class="number">0.601198</span>       <span class="number">1</span></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># dataMat.append([float(lineArr[0]), float(lineArr[1]), float(lineArr[2])])</span></span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 无</p>
</blockquote>
<blockquote>
<p>训练算法</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchPegasos</span>(<span class="params">dataSet, labels, lam, T, k</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;batchPegasos()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat    特征集合</span></span><br><span class="line"><span class="string">        labels     分类结果集合</span></span><br><span class="line"><span class="string">        lam        固定值</span></span><br><span class="line"><span class="string">        T          迭代次数</span></span><br><span class="line"><span class="string">        k          待处理列表大小</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        w          回归系数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    w = zeros(n)  <span class="comment"># 回归系数</span></span><br><span class="line">    dataIndex = <span class="built_in">range</span>(m)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T+<span class="number">1</span>):</span><br><span class="line">        wDelta = mat(zeros(n))  <span class="comment"># 重置 wDelta</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 它是学习率，代表了权重调整幅度的大小。（也可以理解为随机梯度的步长，使它不断减小，便于拟合）</span></span><br><span class="line">        <span class="comment"># 输入T和K分别设定了迭代次数和待处理列表的大小。在T次迭代过程中，每次需要重新计算eta</span></span><br><span class="line">        eta = <span class="number">1.0</span>/(lam*t)</span><br><span class="line">        random.shuffle(dataIndex)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):      <span class="comment"># 全部的训练集  内循环中执行批处理，将分类错误的值全部做累加后更新权重向量</span></span><br><span class="line">            i = dataIndex[j]</span><br><span class="line">            p = predict(w, dataSet[i, :])              <span class="comment"># mapper 代码</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果预测正确，并且预测结果的绝对值&gt;=1，因为最大间隔为1, 认为没问题。</span></span><br><span class="line">            <span class="comment"># 否则算是预测错误, 通过预测错误的结果，来累计更新w.</span></span><br><span class="line">            <span class="keyword">if</span> labels[i]*p &lt; <span class="number">1</span>:                        <span class="comment"># mapper 代码</span></span><br><span class="line">                wDelta += labels[i]*dataSet[i, :].A    <span class="comment"># 累积变化</span></span><br><span class="line">        <span class="comment"># w通过不断的随机梯度的方式来优化</span></span><br><span class="line">        w = (<span class="number">1.0</span> - <span class="number">1</span>/t)*w + (eta/k)*wDelta             <span class="comment"># 在每个 T上应用更改</span></span><br><span class="line">        <span class="comment"># print &#x27;-----&#x27;, w</span></span><br><span class="line">    <span class="comment"># print &#x27;++++++&#x27;, w</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></div></figure>

<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/pegasos.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/pegasos.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/pegasos.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>运行方式: <code>python /opt/git/MachineLearning/src/python/15.BigData_MapReduce/mrSVM.py &lt; data/15.BigData_MapReduce/inputFile.txt</code><br><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py" >MR版本的代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/14/ml_14/">第14章 利用SVD简化数据</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/svd_headPage.jpg" alt="利用SVD简化数据首页" title="利用SVD简化数据首页"></p>

        <h2 id="SVD-概述"   >
          <a href="#SVD-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVD-概述" class="headerlink" title="SVD 概述"></a>SVD 概述</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">奇异值分解（SVD, Singular Value Decomposition）:</span><br><span class="line">    提取信息的一种方法，可以把 SVD 看成是从噪声数据中抽取相关特征。从生物信息学到金融学，SVD 是提取信息的强大工具。</span><br></pre></td></tr></table></div></figure>


        <h2 id="SVD-场景"   >
          <a href="#SVD-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVD-场景" class="headerlink" title="SVD 场景"></a>SVD 场景</h2>
      <blockquote>
<p>信息检索-隐性语义检索（Latent Semantic Indexing, LSI）或 隐形语义分析（Latent Semantic Analysis, LSA）</p>
</blockquote>
<p>隐性语义索引: 矩阵 = 文档 + 词语</p>
<ul>
<li>是最早的 SVD 应用之一，我们称利用 SVD 的方法为隐性语义索引（LSI）或隐性语义分析（LSA）。</li>
</ul>
<p><img src="img/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-LSI%E4%B8%BE%E4%BE%8B.png" alt="LSA举例"></p>
<blockquote>
<p>推荐系统</p>
</blockquote>
<ol>
<li>利用 SVD 从数据中构建一个主题空间。</li>
<li>再在该空间下计算其相似度。(从高维-低维空间的转化，在低维空间来计算相似度，SVD 提升了推荐系统的效率。)</li>
</ol>
<p><img src="img/SVD_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E4%B8%BB%E9%A2%98%E7%A9%BA%E9%97%B4%E6%A1%88%E4%BE%8B1.jpg" alt="主题空间案例1"></p>
<ul>
<li>上图右边标注的为一组共同特征，表示美式 BBQ 空间；另一组在上图右边未标注的为日式食品 空间。</li>
</ul>
<blockquote>
<p>图像压缩</p>
</blockquote>
<p>例如: <code>32*32=1024 =&gt; 32*2+2*1+32*2=130</code>(2*1表示去掉了除对角线的0), 几乎获得了10倍的压缩比。</p>
<p><img src="img/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-SVD%E5%85%AC%E5%BC%8F.jpg" alt="SVD公式"></p>

        <h2 id="SVD-原理"   >
          <a href="#SVD-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVD-原理" class="headerlink" title="SVD 原理"></a>SVD 原理</h2>
      
        <h3 id="SVD-工作原理"   >
          <a href="#SVD-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVD-工作原理" class="headerlink" title="SVD 工作原理"></a>SVD 工作原理</h3>
      <blockquote>
<p>矩阵分解</p>
</blockquote>
<ul>
<li>矩阵分解是将数据矩阵分解为多个独立部分的过程。</li>
<li>矩阵分解可以将原始矩阵表示成新的易于处理的形式，这种新形式是两个或多个矩阵的乘积。（类似代数中的因数分解）</li>
<li>举例: 如何将12分解成两个数的乘积？（1，12）、（2，6）、（3，4）都是合理的答案。</li>
</ul>
<blockquote>
<p>SVD 是矩阵分解的一种类型，也是矩阵分解最常见的技术</p>
</blockquote>
<ul>
<li>SVD 将原始的数据集矩阵 Data 分解成三个矩阵 U、$$\sum$$、V</li>
<li>举例: 如果原始矩阵 $$Data_{m \ast n}$$ 是m行n列，<ul>
<li>$$U_{m \ast k}$$ 表示m行k列</li>
<li>$$\sum_{k \ast k}$$ 表示k行k列</li>
<li>$$V_{k \ast n}$$ 表示k行n列。</li>
</ul>
</li>
</ul>
<p>$$Data_{m \ast n} = U_{m \ast k} \sum_{k \ast k} V_{k \ast n}$$</p>
<p><img src="img/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-SVD%E5%85%AC%E5%BC%8F.jpg" alt="SVD公式"></p>
<p>具体的案例: （大家可以试着推导一下: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://wenku.baidu.com/view/b7641217866fb84ae45c8d17.html" >https://wenku.baidu.com/view/b7641217866fb84ae45c8d17.html</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> ）</p>
<p><img src="img/SVD%E5%85%AC%E5%BC%8F%E7%9A%84%E6%B5%8B%E8%AF%95%E6%A1%88%E4%BE%8B.jpg" alt="SVD公式"></p>
<ul>
<li>上述分解中会构建出一个矩阵 $$\sum$$ ，该矩阵只有对角元素，其他元素均为0(近似于0)。另一个惯例就是，$$\sum$$ 的对角元素是从大到小排列的。这些对角元素称为奇异值。</li>
<li>奇异值与特征值(PCA 数据中重要特征)是有关系的。这里的奇异值就是矩阵 $$Data \ast Data^T$$ 特征值的平方根。</li>
<li>普遍的事实: 在某个奇异值的数目(r 个=&gt;奇异值的平方和累加到总值的90%以上)之后，其他的奇异值都置为0(近似于0)。这意味着数据集中仅有 r 个重要特征，而其余特征则都是噪声或冗余特征。</li>
</ul>

        <h3 id="SVD-算法特点"   >
          <a href="#SVD-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVD-算法特点" class="headerlink" title="SVD 算法特点"></a>SVD 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 简化数据，去除噪声，优化算法的结果</span><br><span class="line">缺点: 数据的转换可能难以理解</span><br><span class="line">使用的数据类型: 数值型数据</span><br></pre></td></tr></table></div></figure>


        <h2 id="推荐系统"   >
          <a href="#推荐系统" class="heading-link"><i class="fas fa-link"></i></a><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2>
      
        <h3 id="推荐系统-概述"   >
          <a href="#推荐系统-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#推荐系统-概述" class="headerlink" title="推荐系统 概述"></a>推荐系统 概述</h3>
      <p><code>推荐系统是利用电子商务网站向客户提供商品信息和建议，帮助用户决定应该购买什么产品，模拟销售人员帮助客户完成购买过程。</code></p>

        <h3 id="推荐系统-场景"   >
          <a href="#推荐系统-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#推荐系统-场景" class="headerlink" title="推荐系统 场景"></a>推荐系统 场景</h3>
      <ol>
<li>Amazon 会根据顾客的购买历史向他们推荐物品</li>
<li>Netflix 会向其用户推荐电影</li>
<li>新闻网站会对用户推荐新闻频道</li>
</ol>

        <h3 id="推荐系统-要点"   >
          <a href="#推荐系统-要点" class="heading-link"><i class="fas fa-link"></i></a><a href="#推荐系统-要点" class="headerlink" title="推荐系统 要点"></a>推荐系统 要点</h3>
      <blockquote>
<p>基于协同过滤(collaborative filtering) 的推荐引擎</p>
</blockquote>
<ul>
<li>利用Python 实现 SVD(Numpy 有一个称为 linalg 的线性代数工具箱)</li>
<li>协同过滤: 是通过将用户和其他用户的数据进行对比来实现推荐的。</li>
<li>当知道了两个用户或两个物品之间的相似度，我们就可以利用已有的数据来预测未知用户的喜好。</li>
</ul>
<blockquote>
<p>基于物品的相似度和基于用户的相似度: 物品比较少则选择物品相似度，用户比较少则选择用户相似度。【矩阵还是小一点好计算】</p>
</blockquote>
<ul>
<li>基于物品的相似度: 计算物品之间的距离。【耗时会随物品数量的增加而增加】</li>
<li>由于物品A和物品C 相似度(相关度)很高，所以给买A的人推荐C。</li>
</ul>
<p><img src="img/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9B%B8%E4%BC%BC%E5%BA%A6.png" alt="SVD公式"></p>
<ul>
<li>基于用户的相似度: 计算用户之间的距离。【耗时会随用户数量的增加而增加】</li>
<li>由于用户A和用户C 相似度(相关度)很高，所以A和C是兴趣相投的人，对于C买的物品就会推荐给A。</li>
</ul>
<p><img src="img/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9B%B8%E4%BC%BC%E5%BA%A6.png" alt="SVD公式"></p>
<blockquote>
<p>相似度计算</p>
</blockquote>
<ul>
<li>inA, inB 对应的是 列向量</li>
</ul>
<ol>
<li>欧氏距离: 指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。二维或三维中的欧氏距离就是两点之间的实际距离。<ul>
<li>相似度= 1/(1+欧式距离)</li>
<li><code>相似度= 1.0/(1.0 + la.norm(inA - inB))</code></li>
<li>物品对越相似，它们的相似度值就越大。</li>
</ul>
</li>
<li>皮尔逊相关系数: 度量的是两个向量之间的相似度。<ul>
<li>相似度= 0.5 + 0.5*corrcoef() 【皮尔逊相关系数的取值范围从 -1 到 +1，通过函数0.5 + 0.5*corrcoef()这个函数计算，把值归一化到0到1之间】</li>
<li><code>相似度= 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]</code></li>
<li>相对欧氏距离的优势: 它对用户评级的量级并不敏感。</li>
</ul>
</li>
<li>余弦相似度: 计算的是两个向量夹角的余弦值。<ul>
<li>余弦值 = (A·B)/(||A||·||B||) 【余弦值的取值范围也在-1到+1之间】</li>
<li>相似度= 0.5 + 0.5*余弦值</li>
<li><code>相似度= 0.5 + 0.5*( float(inA.T*inB) / la.norm(inA)*la.norm(inB))</code></li>
<li>如果夹角为90度，则相似度为0；如果两个向量的方向相同，则相似度为1.0。</li>
</ul>
</li>
</ol>
<blockquote>
<p>推荐系统的评价</p>
</blockquote>
<ul>
<li>采用交叉测试的方法。【拆分数据为训练集和测试集】</li>
<li>推荐引擎评价的指标:  最小均方根误差(Root mean squared error, RMSE)，也称标准误差(Standard error)，就是计算均方误差的平均值然后取其平方根。<ul>
<li>如果RMSE=1, 表示相差1个星级；如果RMSE=2.5, 表示相差2.5个星级。</li>
</ul>
</li>
</ul>

        <h3 id="推荐系统-原理"   >
          <a href="#推荐系统-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#推荐系统-原理" class="headerlink" title="推荐系统 原理"></a>推荐系统 原理</h3>
      <ul>
<li>推荐系统的工作过程: 给定一个用户，系统会为此用户返回N个最好的推荐菜。</li>
<li>实现流程大致如下: <ol>
<li>寻找用户没有评级的菜肴，即在用户-物品矩阵中的0值。</li>
<li>在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数。这就是说: 我们认为用户可能会对物品的打分（这就是相似度计算的初衷）。</li>
<li>对这些物品的评分从高到低进行排序，返回前N个物品。</li>
</ol>
</li>
</ul>

        <h3 id="项目案例-餐馆菜肴推荐系统"   >
          <a href="#项目案例-餐馆菜肴推荐系统" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-餐馆菜肴推荐系统" class="headerlink" title="项目案例: 餐馆菜肴推荐系统"></a>项目案例: 餐馆菜肴推荐系统</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p><code>假如一个人在家决定外出吃饭，但是他并不知道该到哪儿去吃饭，该点什么菜。推荐系统可以帮他做到这两点。</code></p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <blockquote>
<p>收集 并 准备数据</p>
</blockquote>
<p><img src="img/%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5.jpg" alt="SVD 矩阵"></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadExData3</span>():</span></span><br><span class="line">    <span class="comment"># 利用SVD提高推荐效果，菜肴矩阵</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    行: 代表人</span></span><br><span class="line"><span class="string">    列: 代表菜肴名词</span></span><br><span class="line"><span class="string">    值: 代表人对菜肴的评分，0表示未评分</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span>[[<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">4</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 这里不做过多的讨论(当然此处可以对比不同距离之间的差别)</p>
</blockquote>
<blockquote>
<p>训练算法: 通过调用 recommend() 函数进行推荐</p>
</blockquote>
<p>recommend() 会调用 基于物品相似度 或者是 基于SVD，得到推荐的物品评分。</p>
<ul>
<li>1.基于物品相似度</li>
</ul>
<p><img src="img/%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9B%B8%E4%BC%BC%E5%BA%A6.jpg" alt="基于物品相似度"></p>
<p><img src="img/%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F.jpg" alt="欧式距离的计算方式"></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于物品相似度的推荐引擎</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standEst</span>(<span class="params">dataMat, user, simMeas, item</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;standEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat         训练数据集</span></span><br><span class="line"><span class="string">        user            用户编号</span></span><br><span class="line"><span class="string">        simMeas         相似度计算方法</span></span><br><span class="line"><span class="string">        item            未评分的物品编号</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ratSimTotal/simTotal     评分（0～5之间的值）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 得到数据集中的物品数目</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 初始化两个评分值</span></span><br><span class="line">    simTotal = <span class="number">0.0</span></span><br><span class="line">    ratSimTotal = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 遍历行中的每个物品（对用户评过分的物品进行遍历，并将它与其他物品进行比较）</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        userRating = dataMat[user, j]</span><br><span class="line">        <span class="comment"># 如果某个物品的评分值为0，则跳过这个物品</span></span><br><span class="line">        <span class="keyword">if</span> userRating == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 寻找两个用户都评级的物品</span></span><br><span class="line">        <span class="comment"># 变量 overLap 给出的是两个物品当中已经被评分的那个元素的索引ID</span></span><br><span class="line">        <span class="comment"># logical_and 计算x1和x2元素的真值。</span></span><br><span class="line">        overLap = nonzero(logical_and(dataMat[:, item].A &gt; <span class="number">0</span>, dataMat[:, j].A &gt; <span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 如果相似度为0，则两着没有任何重合元素，终止本次循环</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(overLap) == <span class="number">0</span>:</span><br><span class="line">            similarity = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 如果存在重合的物品，则基于这些重合物重新计算相似度。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            similarity = simMeas(dataMat[overLap, item], dataMat[overLap, j])</span><br><span class="line">        <span class="comment"># print &#x27;the %d and %d similarity is : %f&#x27;(iten,j,similarity)</span></span><br><span class="line">        <span class="comment"># 相似度会不断累加，每次计算时还考虑相似度和当前用户评分的乘积</span></span><br><span class="line">        <span class="comment"># similarity  用户相似度，   userRating 用户评分</span></span><br><span class="line">        simTotal += similarity</span><br><span class="line">        ratSimTotal += similarity * userRating</span><br><span class="line">    <span class="keyword">if</span> simTotal == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 通过除以所有的评分总和，对上述相似度评分的乘积进行归一化，使得最后评分在0~5之间，这些评分用来对预测值进行排序</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ratSimTotal/simTotal</span><br></pre></td></tr></table></div></figure>

<ul>
<li>2.基于SVD(参考地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.codeweblog.com/svd-%E7%AC%94%E8%AE%B0/" >http://www.codeweblog.com/svd-%E7%AC%94%E8%AE%B0/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>)</li>
</ul>
<p><img src="img/%E5%9F%BA%E4%BA%8ESVD.png" alt="基于SVD.png"></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于SVD的评分估计</span></span><br><span class="line"><span class="comment"># 在recommend() 中，这个函数用于替换对standEst()的调用，该函数对给定用户给定物品构建了一个评分估计值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svdEst</span>(<span class="params">dataMat, user, simMeas, item</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;svdEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat         训练数据集</span></span><br><span class="line"><span class="string">        user            用户编号</span></span><br><span class="line"><span class="string">        simMeas         相似度计算方法</span></span><br><span class="line"><span class="string">        item            未评分的物品编号</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ratSimTotal/simTotal     评分（0～5之间的值）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 物品数目</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对数据集进行SVD分解</span></span><br><span class="line">    simTotal = <span class="number">0.0</span></span><br><span class="line">    ratSimTotal = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 奇异值分解</span></span><br><span class="line">    <span class="comment"># 在SVD分解之后，我们只利用包含了90%能量值的奇异值，这些奇异值会以NumPy数组的形式得以保存</span></span><br><span class="line">    U, Sigma, VT = la.svd(dataMat)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 分析 Sigma 的长度取值</span></span><br><span class="line">    <span class="comment"># analyse_data(Sigma, 20)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果要进行矩阵运算，就必须要用这些奇异值构建出一个对角矩阵</span></span><br><span class="line">    Sig4 = mat(eye(<span class="number">4</span>) * Sigma[: <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用U矩阵将物品转换到低维空间中，构建转换后的物品(物品+4个主要的“隐形”特征)</span></span><br><span class="line">    <span class="comment"># 公式1(目的是: 降维-改变形状，也改变大小)  xformedItems = dataMat.T * U[:, :4] * Sig4.I</span></span><br><span class="line">    <span class="comment"># 公式2(目的是: 压缩-不改变形状，改变大小)      reconMat = U[:, :4] * Sig4.I * VT[:4, :]</span></span><br><span class="line">        <span class="comment"># 其中: imgCompress() 是详细的案例</span></span><br><span class="line">    <span class="comment"># 最近看到一篇文章描述，感觉挺有道理的，我就顺便补充一下注释: https://blog.csdn.net/qq_36523839/article/details/82347332</span></span><br><span class="line">    xformedItems = dataMat.T * U[:, :<span class="number">4</span>] * Sig4.I</span><br><span class="line">    <span class="comment"># 对于给定的用户，for循环在用户对应行的元素上进行遍历，</span></span><br><span class="line">    <span class="comment"># 这和standEst()函数中的for循环的目的一样，只不过这里的相似度计算时在低维空间下进行的。</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        userRating = dataMat[user, j]</span><br><span class="line">        <span class="keyword">if</span> userRating == <span class="number">0</span> <span class="keyword">or</span> j == item:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 相似度的计算方法也会作为一个参数传递给该函数</span></span><br><span class="line">        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)</span><br><span class="line">        <span class="comment"># for 循环中加入了一条print语句，以便了解相似度计算的进展情况。如果觉得累赘，可以去掉</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;the %d and %d similarity is: %f&#x27;</span> % (item, j, similarity)</span><br><span class="line">        <span class="comment"># 对相似度不断累加求和</span></span><br><span class="line">        simTotal += similarity</span><br><span class="line">        <span class="comment"># 对相似度及对应评分值的乘积求和</span></span><br><span class="line">        ratSimTotal += similarity * userRating</span><br><span class="line">    <span class="keyword">if</span> simTotal == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 计算估计评分</span></span><br><span class="line">        <span class="keyword">return</span> ratSimTotal/simTotal</span><br></pre></td></tr></table></div></figure>

<p>排序获取最后的推荐结果</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># recommend()函数，就是推荐引擎，它默认调用standEst()函数，产生了最高的N个推荐结果。</span></span><br><span class="line"><span class="comment"># 如果不指定N的大小，则默认值为3。该函数另外的参数还包括相似度计算方法和估计方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommend</span>(<span class="params">dataMat, user, N=<span class="number">3</span>, simMeas=cosSim, estMethod=standEst</span>):</span></span><br><span class="line">    <span class="comment"># 寻找未评级的物品</span></span><br><span class="line">    <span class="comment"># 对给定的用户建立一个未评分的物品列表</span></span><br><span class="line">    unratedItems = nonzero(dataMat[user, :].A == <span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 如果不存在未评分物品，那么就退出函数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(unratedItems) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;you rated everything&#x27;</span></span><br><span class="line">    <span class="comment"># 物品的编号和评分值</span></span><br><span class="line">    itemScores = []</span><br><span class="line">    <span class="comment"># 在未评分物品上进行循环</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> unratedItems:</span><br><span class="line">        estimatedScore = estMethod(dataMat, user, simMeas, item)</span><br><span class="line">        <span class="comment"># 寻找前N个未评级物品，调用standEst()来产生该物品的预测得分，该物品的编号和估计值会放在一个元素列表itemScores中</span></span><br><span class="line">        itemScores.append((item, estimatedScore))</span><br><span class="line">        <span class="comment"># 按照估计得分，对该列表进行排序并返回。列表逆排序，第一个值就是最大值</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(itemScores, key=<span class="keyword">lambda</span> jj: jj[<span class="number">1</span>], reverse=<span class="literal">True</span>)[: N]</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试 和 项目调用，可直接参考我们的代码</p>
</blockquote>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="要点补充"   >
          <a href="#要点补充" class="heading-link"><i class="fas fa-link"></i></a><a href="#要点补充" class="headerlink" title="要点补充"></a>要点补充</h4>
      <blockquote>
<p>基于内容(content-based)的推荐</p>
</blockquote>
<ol>
<li>通过各种标签来标记菜肴</li>
<li>将这些属性作为相似度计算所需要的数据</li>
<li>这就是: 基于内容的推荐。</li>
</ol>
<blockquote>
<p>构建推荐引擎面临的挑战</p>
</blockquote>
<p>问题</p>
<ul>
<li>1）在大规模的数据集上，SVD分解会降低程序的速度</li>
<li>2）存在其他很多规模扩展性的挑战性问题，比如矩阵的表示方法和计算相似度得分消耗资源。</li>
<li>3）如何在缺乏数据时给出好的推荐-称为冷启动【简单说: 用户不会喜欢一个无效的物品，而用户不喜欢的物品又无效】</li>
</ul>
<p>建议</p>
<ul>
<li>1）在大型系统中，SVD分解(可以在程序调入时运行一次)每天运行一次或者其频率更低，并且还要离线运行。</li>
<li>2）在实际中，另一个普遍的做法就是离线计算并保存相似度得分。(物品相似度可能被用户重复的调用)</li>
<li>3）冷启动问题，解决方案就是将推荐看成是搜索问题，通过各种标签／属性特征进行<code>基于内容的推荐</code>。</li>
</ul>

        <h3 id="项目案例-基于-SVD-的图像压缩"   >
          <a href="#项目案例-基于-SVD-的图像压缩" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-基于-SVD-的图像压缩" class="headerlink" title="项目案例: 基于 SVD 的图像压缩"></a>项目案例: 基于 SVD 的图像压缩</h3>
      <blockquote>
<p>收集 并 准备数据</p>
</blockquote>
<p>将文本数据转化为矩阵</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载并转换数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgLoadData</span>(<span class="params">filename</span>):</span></span><br><span class="line">    myl = []</span><br><span class="line">    <span class="comment"># 打开文本文件，并从文件以数组方式读入字符</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(filename).readlines():</span><br><span class="line">        newRow = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            newRow.append(<span class="built_in">int</span>(line[i]))</span><br><span class="line">        myl.append(newRow)</span><br><span class="line">    <span class="comment"># 矩阵调入后，就可以在屏幕上输出该矩阵</span></span><br><span class="line">    myMat = mat(myl)</span><br><span class="line">    <span class="keyword">return</span> myMat</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 分析 Sigma 的长度个数</p>
</blockquote>
<p>通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并去除噪声。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analyse_data</span>(<span class="params">Sigma, loopNum=<span class="number">20</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;analyse_data(分析 Sigma 的长度取值)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Sigma         Sigma的值</span></span><br><span class="line"><span class="string">        loopNum       循环次数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 总方差的集合（总能量值）</span></span><br><span class="line">    Sig2 = Sigma**<span class="number">2</span></span><br><span class="line">    SigmaSum = <span class="built_in">sum</span>(Sig2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(loopNum):</span><br><span class="line">        SigmaI = <span class="built_in">sum</span>(Sig2[:i+<span class="number">1</span>])</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        根据自己的业务情况，就行处理，设置对应的 Singma 次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;主成分: %s, 方差占比: %s%%&#x27;</span> % (<span class="built_in">format</span>(i+<span class="number">1</span>, <span class="string">&#x27;2.0f&#x27;</span>), <span class="built_in">format</span>(SigmaI/SigmaSum*<span class="number">100</span>, <span class="string">&#x27;4.2f&#x27;</span>))</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 对比使用 SVD 前后的数据差异对比，对于存储大家可以试着写写</p>
</blockquote>
<p>例如: <code>32*32=1024 =&gt; 32*2+2*1+32*2=130</code>(2*1表示去掉了除对角线的0), 几乎获得了10倍的压缩比。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printMat</span>(<span class="params">inMat, thresh=<span class="number">0.8</span></span>):</span></span><br><span class="line">    <span class="comment"># 由于矩阵保护了浮点数，因此定义浅色和深色，遍历所有矩阵元素，当元素大于阀值时打印1，否则打印0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">float</span>(inMat[i, k]) &gt; thresh:</span><br><span class="line">                <span class="built_in">print</span> <span class="number">1</span>,</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span> <span class="number">0</span>,</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现图像压缩，允许基于任意给定的奇异值数目来重构图像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgCompress</span>(<span class="params">numSV=<span class="number">3</span>, thresh=<span class="number">0.8</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;imgCompress( )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        numSV       Sigma长度   </span></span><br><span class="line"><span class="string">        thresh      判断的阈值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 构建一个列表</span></span><br><span class="line">    myMat = imgLoadData(<span class="string">&#x27;data/14.SVD/0_5.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;****original matrix****&quot;</span></span><br><span class="line">    <span class="comment"># 对原始图像进行SVD分解并重构图像e</span></span><br><span class="line">    printMat(myMat, thresh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过Sigma 重新构成SigRecom来实现</span></span><br><span class="line">    <span class="comment"># Sigma是一个对角矩阵，因此需要建立一个全0矩阵，然后将前面的那些奇异值填充到对角线上。</span></span><br><span class="line">    U, Sigma, VT = la.svd(myMat)</span><br><span class="line">    <span class="comment"># SigRecon = mat(zeros((numSV, numSV)))</span></span><br><span class="line">    <span class="comment"># for k in range(numSV):</span></span><br><span class="line">    <span class="comment">#     SigRecon[k, k] = Sigma[k]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分析插入的 Sigma 长度</span></span><br><span class="line">    analyse_data(Sigma, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    SigRecon = mat(eye(numSV) * Sigma[: numSV])</span><br><span class="line">    reconMat = U[:, :numSV] * SigRecon * VT[:numSV, :]</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;****reconstructed matrix using %d singular values *****&quot;</span> % numSV</span><br><span class="line">    printMat(reconMat, thresh)</span><br></pre></td></tr></table></div></figure>

<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~lihuisong" >1988</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/13/ml_13/">第13章 利用 PCA 来简化数据</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="img/%E5%88%A9%E7%94%A8PCA%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE_%E9%A6%96%E9%A1%B5.jpg" alt="利用PCA简化数据_首页"></p>

        <h2 id="降维技术"   >
          <a href="#降维技术" class="heading-link"><i class="fas fa-link"></i></a><a href="#降维技术" class="headerlink" title="降维技术"></a>降维技术</h2>
      <blockquote>
<p>场景</p>
</blockquote>
<ul>
<li>我们正通过电视观看体育比赛，在电视的显示器上有一个球。</li>
<li>显示器大概包含了100万像素点，而球则可能是由较少的像素点组成，例如说一千个像素点。</li>
<li>人们实时的将显示器上的百万像素转换成为一个三维图像，该图像就给出运动场上球的位置。</li>
<li>在这个过程中，人们已经将百万像素点的数据，降至为三维。这个过程就称为<code>降维(dimensionality reduction)</code></li>
</ul>
<blockquote>
<p>数据显示 并非大规模特征下的唯一难题，对数据进行简化还有如下一系列的原因: </p>
</blockquote>
<ul>
<li><ol>
<li>使得数据集更容易使用</li>
</ol>
</li>
<li><ol start="2">
<li>降低很多算法的计算开销</li>
</ol>
</li>
<li><ol start="3">
<li>去除噪音</li>
</ol>
</li>
<li><ol start="4">
<li>使得结果易懂</li>
</ol>
</li>
</ul>
<blockquote>
<p>适用范围: </p>
</blockquote>
<ul>
<li>在已标注与未标注的数据上都有降维技术。</li>
<li>这里我们将主要关注未标注数据上的降维技术，将技术同样也可以应用于已标注的数据。</li>
</ul>
<blockquote>
<p>在以下3种降维技术中， PCA的应用目前最为广泛，因此本章主要关注PCA。</p>
</blockquote>
<ul>
<li><ol>
<li>主成分分析(Principal Component Analysis, PCA)</li>
</ol>
<ul>
<li><code>通俗理解: 就是找出一个最主要的特征，然后进行分析。</code></li>
<li><code>例如:  考察一个人的智力情况，就直接看数学成绩就行(存在: 数学、语文、英语成绩)</code></li>
</ul>
</li>
<li><ol start="2">
<li>因子分析(Factor Analysis)</li>
</ol>
<ul>
<li><code>通俗理解: 将多个实测变量转换为少数几个综合指标。它反映一种降维的思想，通过降维将相关性高的变量聚在一起,从而减少需要分析的变量的数量,而减少问题分析的复杂性</code></li>
<li><code>例如:  考察一个人的整体情况，就直接组合3样成绩(隐变量)，看平均成绩就行(存在: 数学、语文、英语成绩)</code></li>
<li>应用的领域: 社会科学、金融和其他领域</li>
<li>在因子分析中，我们<ul>
<li>假设观察数据的成分中有一些观察不到的隐变量(latent variable)。</li>
<li>假设观察数据是这些隐变量和某些噪音的线性组合。</li>
<li>那么隐变量的数据可能比观察数据的数目少，也就说通过找到隐变量就可以实现数据的降维。</li>
</ul>
</li>
</ul>
</li>
<li><ol start="3">
<li>独立成分分析(Independ Component Analysis, ICA)</li>
</ol>
<ul>
<li><code>通俗理解: ICA 认为观测信号是若干个独立信号的线性组合，ICA 要做的是一个解混过程。</code></li>
<li><code>例如: 我们去ktv唱歌，想辨别唱的是什么歌曲？ICA 是观察发现是原唱唱的一首歌【2个独立的声音（原唱／主唱）】。</code></li>
<li>ICA 是假设数据是从 N 个数据源混合组成的，这一点和因子分析有些类似，这些数据源之间在统计上是相互独立的，而在 PCA 中只假设数据是不 相关（线性关系）的。</li>
<li>同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。</li>
</ul>
</li>
</ul>

        <h2 id="PCA"   >
          <a href="#PCA" class="heading-link"><i class="fas fa-link"></i></a><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2>
      
        <h3 id="PCA-概述"   >
          <a href="#PCA-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#PCA-概述" class="headerlink" title="PCA 概述"></a>PCA 概述</h3>
      <p>主成分分析(Principal Component Analysis, PCA): <code>通俗理解: 就是找出一个最主要的特征，然后进行分析。</code></p>

        <h3 id="PCA-场景"   >
          <a href="#PCA-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#PCA-场景" class="headerlink" title="PCA 场景"></a>PCA 场景</h3>
      <p><code>例如:  考察一个人的智力情况，就直接看数学成绩就行(存在: 数学、语文、英语成绩)</code></p>

        <h3 id="PCA-原理"   >
          <a href="#PCA-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#PCA-原理" class="headerlink" title="PCA 原理"></a>PCA 原理</h3>
      <blockquote>
<p>PCA 工作原理</p>
</blockquote>
<ol>
<li>找出第一个主成分的方向，也就是数据 <code>方差最大</code> 的方向。</li>
<li>找出第二个主成分的方向，也就是数据 <code>方差次大</code> 的方向，并且该方向与第一个主成分方向 <code>正交(orthogonal 如果是二维空间就叫垂直)</code>。</li>
<li>通过这种方式计算出所有的主成分方向。</li>
<li>通过数据集的协方差矩阵及其特征值分析，我们就可以得到这些主成分的值。</li>
<li>一旦得到了协方差矩阵的特征值和特征向量，我们就可以保留最大的 N 个特征。这些特征向量也给出了 N 个最重要特征的真实结构，我们就可以通过将数据乘上这 N 个特征向量 从而将它转换到新的空间上。</li>
</ol>
<p>为什么正交？</p>
<ol>
<li>正交是为了数据有效性损失最小</li>
<li>正交的一个原因是特征值的特征向量是正交的</li>
</ol>
<p>例如下图: </p>
<p><img src="img/%E5%BA%94%E7%94%A8PCA%E9%99%8D%E7%BB%B4.png" alt="应用PCA降维"></p>
<blockquote>
<p>PCA 优缺点</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 降低数据的复杂性，识别最重要的多个特征。</span><br><span class="line">缺点: 不一定需要，且可能损失有用信息。</span><br><span class="line">适用数据类型: 数值型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="项目案例-对半导体数据进行降维处理"   >
          <a href="#项目案例-对半导体数据进行降维处理" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-对半导体数据进行降维处理" class="headerlink" title="项目案例: 对半导体数据进行降维处理"></a>项目案例: 对半导体数据进行降维处理</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">半导体是在一些极为先进的工厂中制造出来的。设备的生命早期有限，并且花费极其巨大。</span><br><span class="line">虽然通过早期测试和频繁测试来发现有瑕疵的产品，但仍有一些存在瑕疵的产品通过测试。</span><br><span class="line">如果我们通过机器学习技术用于发现瑕疵产品，那么它就会为制造商节省大量的资金。</span><br><span class="line"></span><br><span class="line">具体来讲，它拥有590个特征。我们看看能否对这些特征进行降维处理。</span><br><span class="line"></span><br><span class="line">对于数据的缺失值的问题，我们有一些处理方法(参考第5章)</span><br><span class="line">目前该章节处理的方案是: 将缺失值NaN(Not a Number缩写)，全部用平均值来替代(如果用0来处理的策略就太差劲了)。</span><br></pre></td></tr></table></div></figure>


        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>文件名: secom.data</p>
<p>文本文件数据格式如下: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3030.93 2564 2187.7333 1411.1265 1.3602 100 97.6133 0.1242 1.5005 0.0162 -0.0034 0.9455 202.4396 0 7.9558 414.871 10.0433 0.968 192.3963 12.519 1.4026 -5419 2916.5 -4043.75 751 0.8955 1.773 3.049 64.2333 2.0222 0.1632 3.5191 83.3971 9.5126 50.617 64.2588 49.383 66.3141 86.9555 117.5132 61.29 4.515 70 352.7173 10.1841 130.3691 723.3092 1.3072 141.2282 1 624.3145 218.3174 0 4.592 4.841 2834 0.9317 0.9484 4.7057 -1.7264 350.9264 10.6231 108.6427 16.1445 21.7264 29.5367 693.7724 0.9226 148.6009 1 608.17 84.0793 NaN NaN 0 0.0126 -0.0206 0.0141 -0.0307 -0.0083 -0.0026 -0.0567 -0.0044 7.2163 0.132 NaN 2.3895 0.969 1747.6049 0.1841 8671.9301 -0.3274 -0.0055 -0.0001 0.0001 0.0003 -0.2786 0 0.3974 -0.0251 0.0002 0.0002 0.135 -0.0042 0.0003 0.0056 0 -0.2468 0.3196 NaN NaN NaN NaN 0.946 0 748.6115 0.9908 58.4306 0.6002 0.9804 6.3788 15.88 2.639 15.94 15.93 0.8656 3.353 0.4098 3.188 -0.0473 0.7243 0.996 2.2967 1000.7263 39.2373 123 111.3 75.2 46.2 350.671 0.3948 0 6.78 0.0034 0.0898 0.085 0.0358 0.0328 12.2566 0 4.271 10.284 0.4734 0.0167 11.8901 0.41 0.0506 NaN NaN 1017 967 1066 368 0.09 0.048 0.095 2 0.9 0.069 0.046 0.725 0.1139 0.3183 0.5888 0.3184 0.9499 0.3979 0.16 0 0 20.95 0.333 12.49 16.713 0.0803 5.72 0 11.19 65.363 0 0 0 0 0 0 0.292 5.38 20.1 0.296 10.62 10.3 5.38 4.04 16.23 0.2951 8.64 0 10.3 97.314 0 0.0772 0.0599 0.07 0.0547 0.0704 0.052 0.0301 0.1135 3.4789 0.001 NaN 0.0707 0.0211 175.2173 0.0315 1940.3994 0 0.0744 0.0546 0 0 0 0 0 0 0 0 0 0.0027 0.004 0 0 0 0 NaN NaN NaN NaN 0.0188 0 219.9453 0.0011 2.8374 0.0189 0.005 0.4269 0 0 0 0 0 0 0 0 0 0 0 0.0472 40.855 4.5152 30.9815 33.9606 22.9057 15.9525 110.2144 0.131 0 2.5883 0.001 0.0319 0.0197 0.012 0.0109 3.9321 0 1.5123 3.5811 0.1337 0.0055 3.8447 0.1077 0.0167 NaN NaN 418.1363 398.3185 496.1582 158.333 0.0373 0.0202 0.0462 0.6083 0.3032 0.02 0.0174 0.2827 0.0434 0.1342 0.2419 0.1343 0.367 0.1431 0.061 0 0 0 6.2698 0.1181 3.8208 5.3737 0.0254 1.6252 0 3.2461 18.0118 0 0 0 0 0 0 0.0752 1.5989 6.5893 0.0913 3.0911 8.4654 1.5989 1.2293 5.3406 0.0867 2.8551 0 2.9971 31.8843 NaN NaN 0 0.0215 0.0274 0.0315 0.0238 0.0206 0.0238 0.0144 0.0491 1.2708 0.0004 NaN 0.0229 0.0065 55.2039 0.0105 560.2658 0 0.017 0.0148 0.0124 0.0114 0 0 0 0 0 0 0 0.001 0.0013 0 0 0 0 NaN NaN NaN NaN 0.0055 0 61.5932 0.0003 0.9967 0.0082 0.0017 0.1437 0 0 0 0 0 0 0 0 0 0 0 0.0151 14.2396 1.4392 5.6188 3.6721 2.9329 2.1118 24.8504 29.0271 0 6.9458 2.738 5.9846 525.0965 0 3.4641 6.0544 0 53.684 2.4788 4.7141 1.7275 6.18 3.275 3.6084 18.7673 33.1562 26.3617 49.0013 10.0503 2.7073 3.1158 3.1136 44.5055 42.2737 1.3071 0.8693 1.1975 0.6288 0.9163 0.6448 1.4324 0.4576 0.1362 0 0 0 5.9396 3.2698 9.5805 2.3106 6.1463 4.0502 0 1.7924 29.9394 0 0 0 0 0 0 6.2052 311.6377 5.7277 2.7864 9.7752 63.7987 24.7625 13.6778 2.3394 31.9893 5.8142 0 1.6936 115.7408 0 613.3069 291.4842 494.6996 178.1759 843.1138 0 53.1098 0 48.2091 0.7578 NaN 2.957 2.1739 10.0261 17.1202 22.3756 0 0 0 0 0 0 0 0 0 0 0 0 64.6707 0 0 0 0 0 NaN NaN NaN NaN 1.9864 0 29.3804 0.1094 4.856 3.1406 0.5064 6.6926 0 0 0 0 0 0 0 0 0 0 0 2.057 4.0825 11.5074 0.1096 0.0078 0.0026 7.116 1.0616 395.57 75.752 0.4234 12.93 0.78 0.1827 5.7349 0.3363 39.8842 3.2687 1.0297 1.0344 0.4385 0.1039 42.3877 NaN NaN NaN NaN NaN NaN NaN NaN 533.85 2.1113 8.95 0.3157 3.0624 0.1026 1.6765 14.9509 NaN NaN NaN NaN 0.5005 0.0118 0.0035 2.363 NaN NaN NaN NaN</span><br><span class="line">3095.78 2465.14 2230.4222 1463.6606 0.8294 100 102.3433 0.1247 1.4966 -0.0005 -0.0148 0.9627 200.547 0 10.1548 414.7347 9.2599 0.9701 191.2872 12.4608 1.3825 -5441.5 2604.25 -3498.75 -1640.25 1.2973 2.0143 7.39 68.4222 2.2667 0.2102 3.4171 84.9052 9.7997 50.6596 64.2828 49.3404 64.9193 87.5241 118.1188 78.25 2.773 70 352.2445 10.0373 133.1727 724.8264 1.2887 145.8445 1 631.2618 205.1695 0 4.59 4.842 2853 0.9324 0.9479 4.682 0.8073 352.0073 10.3092 113.98 10.9036 19.1927 27.6301 697.1964 1.1598 154.3709 1 620.3582 82.3494 NaN NaN 0 -0.0039 -0.0198 0.0004 -0.044 -0.0358 -0.012 -0.0377 0.0017 6.8043 0.1358 NaN 2.3754 0.9894 1931.6464 0.1874 8407.0299 0.1455 -0.0015 0 -0.0005 0.0001 0.5854 0 -0.9353 -0.0158 -0.0004 -0.0004 -0.0752 -0.0045 0.0002 0.0015 0 0.0772 -0.0903 NaN NaN NaN NaN 0.9425 0 731.2517 0.9902 58.668 0.5958 0.9731 6.5061 15.88 2.541 15.91 15.88 0.8703 2.771 0.4138 3.272 -0.0946 0.8122 0.9985 2.2932 998.1081 37.9213 98 80.3 81 56.2 219.7679 0.2301 0 5.7 0.0049 0.1356 0.06 0.0547 0.0204 12.3319 0 6.285 13.077 0.5666 0.0144 11.8428 0.35 0.0437 NaN NaN 568 59 297 3277 0.112 0.115 0.124 2.2 1.1 0.079 0.561 1.0498 0.1917 0.4115 0.6582 0.4115 1.0181 0.2315 0.325 0 0 17.99 0.439 10.14 16.358 0.0892 6.92 0 9.05 82.986 0 0 0 0 0 0 0.222 3.74 19.59 0.316 11.65 8.02 3.74 3.659 15.078 0.358 8.96 0 8.02 134.25 0 0.0566 0.0488 0.1651 0.1578 0.0468 0.0987 0.0734 0.0747 3.9578 0.005 NaN 0.0761 0.0014 128.4285 0.0238 1988 0 0.0203 0.0236 0 0 0 0 0 0 0 0 0 0.0064 0.0036 0 0 0 0 NaN NaN NaN NaN 0.0154 0 193.0287 0.0007 3.8999 0.0187 0.0086 0.5749 0 0 0 0 0 0 0 0 0 0 0 0.0411 29.743 3.6327 29.0598 28.9862 22.3163 17.4008 83.5542 0.0767 0 1.8459 0.0012 0.044 0.0171 0.0154 0.0069 3.9011 0 2.1016 3.9483 0.1662 0.0049 3.7836 0.1 0.0139 NaN NaN 233.9865 26.5879 139.2082 1529.7622 0.0502 0.0561 0.0591 0.8151 0.3464 0.0291 0.1822 0.3814 0.0715 0.1667 0.263 0.1667 0.3752 0.0856 0.1214 0 0 0 5.6522 0.1417 2.9939 5.2445 0.0264 1.8045 0 2.7661 23.623 0 0 0 0 0 0 0.0778 1.1506 5.9247 0.0878 3.3604 7.7421 1.1506 1.1265 5.0108 0.1013 2.4278 0 2.489 41.708 NaN NaN 0 0.0142 0.023 0.0768 0.0729 0.0143 0.0513 0.0399 0.0365 1.2474 0.0017 NaN 0.0248 0.0005 46.3453 0.0069 677.1873 0 0.0053 0.0059 0.0081 0.0033 0 0 0 0 0 0 0 0.0022 0.0013 0 0 0 0 NaN NaN NaN NaN 0.0049 0 65.0999 0.0002 1.1655 0.0068 0.0027 0.1921 0 0 0 0 0 0 0 0 0 0 0 0.012 10.5837 1.0323 4.3465 2.5939 3.2858 2.5197 15.015 27.7464 0 5.5695 3.93 9.0604 0 368.9713 2.1196 6.1491 0 61.8918 3.1531 6.1188 1.4857 6.1911 2.8088 3.1595 10.4383 2.2655 8.4887 199.7866 8.6336 5.7093 1.6779 3.2153 48.5294 37.5793 16.4174 1.2364 1.9562 0.8123 1.0239 0.834 1.5683 0.2645 0.2751 0 0 0 5.1072 4.3737 7.6142 2.2568 6.9233 4.7448 0 1.4336 40.4475 0 0 0 0 0 0 4.7415 463.2883 5.5652 3.0652 10.2211 73.5536 19.4865 13.243 2.1627 30.8643 5.8042 0 1.2928 163.0249 0 0 246.7762 0 359.0444 130.635 820.79 194.4371 0 58.1666 3.6822 NaN 3.2029 0.1441 6.6487 12.6788 23.6469 0 0 0 0 0 0 0 0 0 0 0 0 141.4365 0 0 0 0 0 NaN NaN NaN NaN 1.6292 0 26.397 0.0673 6.6475 3.131 0.8832 8.837 0 0 0 0 0 0 0 0 0 0 0 1.791 2.9799 9.5796 0.1096 0.0078 0.0026 7.116 1.3526 408.798 74.64 0.7193 16 1.33 0.2829 7.1196 0.4989 53.1836 3.9139 1.7819 0.9634 0.1745 0.0375 18.1087 NaN NaN NaN NaN NaN NaN NaN NaN 535.0164 2.4335 5.92 0.2653 2.0111 0.0772 1.1065 10.9003 0.0096 0.0201 0.006 208.2045 0.5019 0.0223 0.0055 4.4447 0.0096 0.0201 0.006 208.2045</span><br><span class="line">2932.61 2559.94 2186.4111 1698.0172 1.5102 100 95.4878 0.1241 1.4436 0.0041 0.0013 0.9615 202.0179 0 9.5157 416.7075 9.3144 0.9674 192.7035 12.5404 1.4123 -5447.75 2701.75 -4047 -1916.5 1.3122 2.0295 7.5788 67.1333 2.3333 0.1734 3.5986 84.7569 8.659 50.153 64.1114 49.847 65.8389 84.7327 118.6128 14.37 5.434 70 364.3782 9.8783 131.8027 734.7924 1.2992 141.0845 1 637.2655 185.7574 0 4.486 4.748 2936 0.9139 0.9447 4.5873 23.8245 364.5364 10.1685 115.6273 11.3019 16.1755 24.2829 710.5095 0.8694 145.8 1 625.9636 84.7681 140.6972 485.2665 0 -0.0078 -0.0326 -0.0052 0.0213 -0.0054 -0.1134 -0.0182 0.0287 7.1041 0.1362 NaN 2.4532 0.988 1685.8514 0.1497 9317.1698 0.0553 0.0006 -0.0013 0 0.0002 -0.1343 0 -0.1427 0.1218 0.0006 -0.0001 0.0134 -0.0026 -0.0016 -0.0006 0.0013 -0.0301 -0.0728 NaN NaN NaN 0.4684 0.9231 0 718.5777 0.9899 58.4808 0.6015 0.9772 6.4527 15.9 2.882 15.94 15.95 0.8798 3.094 0.4777 3.272 -0.1892 0.8194 0.9978 2.2592 998.444 42.0579 89 126.4 96.5 45.1001 306.038 0.3263 0 8.33 0.0038 0.0754 0.0483 0.0619 0.0221 8.266 0 4.819 8.443 0.4909 0.0177 8.2054 0.47 0.0497 NaN NaN 562 788 759 2100 0.187 0.117 0.068 2.1 1.4 0.123 0.319 1.0824 0.0369 0.3141 0.5753 0.3141 0.9677 0.2706 0.326 0 0 17.78 0.745 13.31 22.912 0.1959 9.21 0 17.87 60.11 0 0 0 0 0 0 0.139 5.09 19.75 0.949 9.71 16.73 5.09 11.059 22.624 0.1164 13.3 0 16.73 79.618 0 0.0339 0.0494 0.0696 0.0406 0.0401 0.084 0.0349 0.0718 2.4266 0.0014 NaN 0.0963 0.0152 182.4956 0.0284 839.6006 0 0.0192 0.017 0 0 0 0 0 0 0 0 0 0.0062 0.004 0 0 0 0 NaN NaN NaN 0.1729 0.0273 0 104.4042 0.0007 4.1446 0.0733 0.0063 0.4166 0 0 0 0 0 0 0 0 0 0 0 0.0487 29.621 3.9133 23.551 41.3837 32.6256 15.7716 97.3868 0.1117 0 2.5274 0.0012 0.0249 0.0152 0.0157 0.0075 2.8705 0 1.5306 2.5493 0.1479 0.0059 2.8046 0.1185 0.0167 NaN NaN 251.4536 329.6406 325.0672 902.4576 0.08 0.0583 0.0326 0.6964 0.4031 0.0416 0.1041 0.3846 0.0151 0.1288 0.2268 0.1288 0.3677 0.1175 0.1261 0 0 0 5.7247 0.2682 3.8541 6.1797 0.0546 2.568 0 4.6067 16.0104 0 0 0 0 0 0 0.0243 1.5481 5.9453 0.2777 3.16 8.9855 1.5481 2.9844 6.2277 0.0353 3.7663 0 5.6983 24.7959 13.5664 15.4488 0 0.0105 0.0208 0.0327 0.0171 0.0116 0.0428 0.0154 0.0383 0.7786 0.0005 NaN 0.0302 0.0046 58.0575 0.0092 283.6616 0 0.0054 0.0043 0.003 0.0037 0 0 0 0 0 0 0 0.0021 0.0015 0 0 0 0 NaN NaN NaN 0.0221 0.01 0 28.7334 0.0003 1.2356 0.019 0.002 0.1375 0 0 0 0 0 0 0 0 0 0 0 0.019 11.4871 1.1798 4.0782 4.3102 3.7696 2.0627 18.0233 21.6062 0 8.7236 3.0609 5.2231 0 0 2.2943 4.0917 0 50.6425 2.0261 5.2707 1.8268 4.2581 3.7479 3.522 10.3162 29.1663 18.7546 109.5747 14.2503 5.765 0.8972 3.1281 60 70.9161 8.8647 1.2771 0.4264 0.6263 0.8973 0.6301 1.4698 0.3194 0.2748 0 0 0 4.8795 7.5418 10.0984 3.1182 15.079 6.528 0 2.8042 32.3594 0 0 0 0 0 0 3.0301 21.3645 5.4178 9.3327 8.3977 148.0287 31.4674 45.5423 3.1842 13.3923 9.1221 0 2.6727 93.9245 0 434.2674 151.7665 0 190.3869 746.915 74.0741 191.7582 250.1742 34.1573 1.0281 NaN 3.9238 1.5357 10.8251 18.9849 9.0113 0 0 0 0 0 0 0 0 0 0 0 0 240.7767 244.2748 0 0 0 0 NaN NaN NaN 36.9067 2.9626 0 14.5293 0.0751 7.087 12.1831 0.6451 6.4568 0 0 0 0 0 0 0 0 0 0 0 2.1538 2.9667 9.3046 0.1096 0.0078 0.0026 7.116 0.7942 411.136 74.654 0.1832 16.16 0.85 0.0857 7.1619 0.3752 23.0713 3.9306 1.1386 1.5021 0.3718 0.1233 24.7524 267.064 0.9032 1.1 0.6219 0.4122 0.2562 0.4119 68.8489 535.0245 2.0293 11.21 0.1882 4.0923 0.064 2.0952 9.2721 0.0584 0.0484 0.0148 82.8602 0.4958 0.0157 0.0039 3.1745 0.0584 0.0484 0.0148 82.8602</span><br><span class="line">2988.72 2479.9 2199.0333 909.7926 1.3204 100 104.2367 0.1217 1.4882 -0.0124 -0.0033 0.9629 201.8482 0 9.6052 422.2894 9.6924 0.9687 192.1557 12.4782 1.4011 -5468.25 2648.25 -4515 -1657.25 1.3137 2.0038 7.3145 62.9333 2.6444 0.2071 3.3813 84.9105 8.6789 50.51 64.1125 49.49 65.1951 86.6867 117.0442 76.9 1.279 70 363.0273 9.9305 131.8027 733.8778 1.3027 142.5427 1 637.3727 189.9079 0 4.486 4.748 2936 0.9139 0.9447 4.5873 24.3791 361.4582 10.2112 116.1818 13.5597 15.6209 23.4736 710.4043 0.9761 147.6545 1 625.2945 70.2289 160.321 464.9735 0 -0.0555 -0.0461 -0.04 0.04 0.0676 -0.1051 0.0028 0.0277 7.5925 0.1302 NaN 2.4004 0.9904 1752.0968 0.1958 8205.7 0.0697 -0.0003 -0.0021 -0.0001 0.0002 0.0411 0 0.0177 -0.0195 -0.0002 0 -0.0699 -0.0059 0.0003 0.0003 0.0021 -0.0483 -0.118 NaN NaN NaN 0.4647 0.9564 0 709.0867 0.9906 58.6635 0.6016 0.9761 6.4935 15.55 3.132 15.61 15.59 1.366 2.48 0.5176 3.119 0.2838 0.7244 0.9961 2.3802 980.451 41.1025 127 118 123.7 47.8 162.432 0.1915 0 5.51 0.003 0.114 0.0393 0.0613 0.019 13.2651 0 9.073 15.241 1.3029 0.015 11.9738 0.35 0.0699 NaN NaN 859 355 3433 3004 0.068 0.108 0.1 1.7 0.9 0.086 0.241 0.9386 0.0356 0.2618 0.4391 0.2618 0.8567 0.2452 0.39 0 0 16.22 0.693 14.67 22.562 0.1786 5.69 0 18.2 52.571 0 0 0 0 0 0 0.139 5.92 23.6 1.264 10.63 13.56 5.92 11.382 24.32 0.3458 9.56 0 21.97 104.95 0 0.1248 0.0463 0.1223 0.0354 0.0708 0.0754 0.0643 0.0932 5.5398 0.0023 NaN 0.0764 0.0015 152.0885 0.0573 820.3999 0 0.0152 0.0149 0 0 0 0 0 0 0 0 0 0.0067 0.004 0 0 0 0 NaN NaN NaN 0.0191 0.0234 0 94.0954 0.001 3.2119 0.0406 0.0072 0.4212 0 0 0 0 0 0 0 0 0 0 0 0.0513 31.83 3.1959 33.896 37.8477 44.3906 16.9347 50.3631 0.0581 0 2.1775 0.0007 0.0417 0.0115 0.0172 0.0063 4.2154 0 2.896 4.0526 0.3882 0.0049 3.9403 0.0916 0.0245 NaN NaN 415.5048 157.0889 1572.6896 1377.4276 0.0285 0.0445 0.0465 0.6305 0.3046 0.0286 0.0824 0.3483 0.0128 0.1004 0.1701 0.1004 0.3465 0.0973 0.1675 0 0 0 5.444 0.2004 4.19 6.3329 0.0479 1.7339 0 4.966 15.7375 0 0 0 0 0 0 0.0243 1.7317 6.6262 0.3512 3.2699 9.402 1.7317 3.0672 6.6839 0.0928 3.0229 0 6.3292 29.0339 8.4026 4.8851 0 0.0407 0.0198 0.0531 0.0167 0.0224 0.0422 0.0273 0.0484 1.8222 0.0006 NaN 0.0252 0.0004 45.7058 0.0188 309.8492 0 0.0046 0.0049 0.0028 0.0034 0 0 0 0 0 0 0 0.0024 0.0014 0 0 0 0 NaN NaN NaN 0.0038 0.0068 0 32.4228 0.0003 1.1135 0.0132 0.0023 0.1348 0 0 0 0 0 0 0 0 0 0 0 0.0155 13.3972 1.1907 5.6363 3.9482 4.9881 2.1737 17.8537 14.5054 0 5.286 2.4643 7.6602 317.7362 0 1.9689 6.5718 0 94.4594 3.6091 13.442 1.5441 6.2313 2.8049 4.9898 15.7089 13.4051 76.0354 181.2641 5.176 5.3899 1.3671 2.7013 34.0336 41.5236 7.1274 1.1054 0.4097 0.5183 0.6849 0.529 1.3141 0.2829 0.3332 0 0 0 4.468 6.9785 11.1303 3.0744 13.7105 3.9918 0 2.8555 27.6824 0 0 0 0 0 0 3.0301 24.2831 6.5291 12.3786 9.1494 100.0021 37.8979 48.4887 3.4234 35.4323 6.4746 0 3.5135 149.4399 0 225.0169 100.4883 305.75 88.5553 104.666 71.7583 0 336.766 72.9635 1.767 NaN 3.1817 0.1488 8.6804 29.2542 9.9979 0 0 711.6418 0 0 0 0 0 0 0 0 0 113.5593 0 0 0 0 0 NaN NaN NaN 4.12 2.4416 0 13.2699 0.0977 5.4751 6.7553 0.7404 6.4865 0 0 0 0 0 0 0 0 0 0 0 2.1565 3.2465 7.7754 0.1096 0.0078 0.0026 7.116 1.165 372.822 72.442 1.8804 131.68 39.33 0.6812 56.9303 17.4781 161.4081 35.3198 54.2917 1.1613 0.7288 0.271 62.7572 268.228 0.6511 7.32 0.163 3.5611 0.067 2.729 25.0363 530.5682 2.0253 9.33 0.1738 2.8971 0.0525 1.7585 8.5831 0.0202 0.0149 0.0044 73.8432 0.499 0.0103 0.0025 2.0544 0.0202 0.0149 0.0044 73.8432</span><br><span class="line">3032.24 2502.87 2233.3667 1326.52 1.5334 100 100.3967 0.1235 1.5031 -0.0031 -0.0072 0.9569 201.9424 0 10.5661 420.5925 10.3387 0.9735 191.6037 12.4735 1.3888 -5476.25 2635.25 -3987.5 117 1.2887 1.9912 7.2748 62.8333 3.1556 0.2696 3.2728 86.3269 8.7677 50.248 64.1511 49.752 66.1542 86.1468 121.4364 76.39 2.209 70 353.34 10.4091 176.3136 789.7523 1.0341 138.0882 1 667.7418 233.5491 0 4.624 4.894 2865 0.9298 0.9449 4.6414 -12.2945 355.0809 9.7948 144.0191 21.9782 32.2945 44.1498 745.6025 0.9256 146.6636 1 645.7636 65.8417 NaN NaN 0 -0.0534 0.0183 -0.0167 -0.0449 0.0034 -0.0178 -0.0123 -0.0048 7.5017 0.1342 NaN 2.453 0.9902 1828.3846 0.1829 9014.46 0.0448 -0.0077 -0.0001 -0.0001 -0.0001 0.2189 0 -0.6704 -0.0167 0.0004 -0.0003 0.0696 -0.0045 0.0002 0.0078 0 -0.0799 -0.2038 NaN NaN NaN NaN 0.9424 0 796.595 0.9908 58.3858 0.5913 0.9628 6.3551 15.75 3.148 15.73 15.71 0.946 3.027 0.5328 3.299 -0.5677 0.778 1.001 2.3715 993.1274 38.1448 119 143.2 123.1 48.8 296.303 0.3744 0 3.64 0.0041 0.0634 0.0451 0.0623 0.024 14.2354 0 9.005 12.506 0.4434 0.0126 13.9047 0.43 0.0538 NaN NaN 699 283 1747 1443 0.147 0.04 0.113 3.9 0.8 0.101 0.499 0.576 0.0631 0.3053 0.583 0.3053 0.8285 0.1308 0.922 0 0 15.24 0.282 10.85 37.715 0.1189 3.98 0 25.54 72.149 0 0 0 0 0 0 0.25 5.52 15.76 0.519 10.71 19.77 5.52 8.446 33.832 0.3951 9.09 0 19.77 92.307 0 0.0915 0.0506 0.0769 0.1079 0.0797 0.1047 0.0924 0.1015 4.1338 0.003 NaN 0.0802 0.0004 69.151 0.197 1406.4004 0 0.0227 0.0272 0 0 0 0 0 0 0 0 0 0.0067 0.0031 0 0 0 0 NaN NaN NaN NaN 0.024 0 149.2172 0.0006 2.5775 0.0177 0.0214 0.4051 0 0 0 0 0 0 0 0 0 0 0 0.0488 19.862 3.6163 34.125 55.9626 53.0876 17.4864 88.7672 0.1092 0 1.0929 0.0013 0.0257 0.0116 0.0163 0.008 4.4239 0 3.2376 3.6536 0.1293 0.004 4.3474 0.1275 0.0181 NaN NaN 319.1252 128.0296 799.5884 628.3083 0.0755 0.0181 0.0476 1.35 0.2698 0.032 0.1541 0.2155 0.031 0.1354 0.2194 0.1354 0.3072 0.0582 0.3574 0 0 0 4.8956 0.0766 2.913 11.0583 0.0327 1.1229 0 7.3296 23.116 0 0 0 0 0 0 0.0822 1.6216 4.7279 0.1773 3.155 9.7777 1.6216 2.5923 10.5352 0.1301 3.0939 0 6.3767 32.0537 NaN NaN 0 0.0246 0.0221 0.0329 0.0522 0.0256 0.0545 0.0476 0.0463 1.553 0.001 NaN 0.0286 0.0001 21.0312 0.0573 494.7368 0 0.0063 0.0077 0.0052 0.0027 0 0 0 0 0 0 0 0.0025 0.0012 0 0 0 0 NaN NaN NaN NaN 0.0089 0 57.2692 0.0002 0.8495 0.0065 0.0077 0.1356 0 0 0 0 0 0 0 0 0 0 0 0.0165 7.1493 1.1704 5.3823 4.7226 4.9184 2.185 22.3369 24.4142 0 3.6256 3.3208 4.2178 0 866.0295 2.5046 7.0492 0 85.2255 2.9734 4.2892 1.2943 7.257 3.4473 3.8754 12.7642 10.739 43.8119 0 11.4064 2.0088 1.5533 6.2069 25.3521 37.4691 15.247 0.6672 0.7198 0.6076 0.9088 0.6136 1.2524 0.1518 0.7592 0 0 0 4.3131 2.7092 6.1538 4.7756 11.4945 2.8822 0 3.8248 30.8924 0 0 0 0 0 0 5.3863 44.898 4.4384 5.2987 7.4365 89.9529 17.0927 19.1303 4.5375 42.6838 6.1979 0 3.0615 140.1953 0 171.4486 276.881 461.8619 240.1781 0 587.3773 748.1781 0 55.1057 2.2358 NaN 3.2712 0.0372 3.7821 107.6905 15.6016 0 293.1396 0 0 0 0 0 0 0 0 0 0 148.0663 0 0 0 0 0 NaN NaN NaN NaN 2.5512 0 18.7319 0.0616 4.4146 2.9954 2.2181 6.3745 0 0 0 0 0 0 0 0 0 0 0 2.0579 1.9999 9.4805 0.1096 0.0078 0.0026 7.116 1.4636 399.914 79.156 1.0388 19.63 1.98 0.4287 9.7608 0.8311 70.9706 4.9086 2.5014 0.9778 0.2156 0.0461 22.05 NaN NaN NaN NaN NaN NaN NaN NaN 532.0155 2.0275 8.83 0.2224 3.1776 0.0706 1.6597 10.9698 NaN NaN NaN NaN 0.48 0.4766 0.1045 99.3032 0.0202 0.0149 0.0044 73.8432</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 将value为NaN的替换为均值</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replaceNanWithMean</span>():</span></span><br><span class="line">    datMat = loadDataSet(<span class="string">&#x27;data/13.PCA/secom.data&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    numFeat = shape(datMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeat):</span><br><span class="line">        <span class="comment"># 对value不为NaN的求均值</span></span><br><span class="line">        <span class="comment"># .A 返回矩阵基于的数组</span></span><br><span class="line">        meanVal = mean(datMat[nonzero(~isnan(datMat[:, i].A))[<span class="number">0</span>], i])</span><br><span class="line">        <span class="comment"># 将value为NaN的值赋值为均值</span></span><br><span class="line">        datMat[nonzero(isnan(datMat[:, i].A))[<span class="number">0</span>],i] = meanVal</span><br><span class="line">    <span class="keyword">return</span> datMat</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 统计分析 N 的阈值</p>
</blockquote>
<p><img src="img/PCA%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE%E8%BF%87%E7%A8%8B.jpg" alt="PCA分析数据过程"></p>
<blockquote>
<p>PCA 数据降维</p>
</blockquote>
<p>在等式 Av=入v 中，v 是特征向量， 入是特征值。<br/><br>表示 如果特征向量 v 被某个矩阵 A 左乘，那么它就等于某个标量 入 乘以 v.<br/><br>幸运的是:  Numpy 中有寻找特征向量和特征值的模块 linalg，它有 eig() 方法，该方法用于求解特征向量和特征值。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span>(<span class="params">dataMat, topNfeat=<span class="number">9999999</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;pca</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat   原数据集矩阵</span></span><br><span class="line"><span class="string">        topNfeat  应用的N个特征</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        lowDDataMat  降维后数据集</span></span><br><span class="line"><span class="string">        reconMat     新的数据集空间</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每一列的均值</span></span><br><span class="line">    meanVals = mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># print &#x27;meanVals&#x27;, meanVals</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个向量同时都减去 均值</span></span><br><span class="line">    meanRemoved = dataMat - meanVals</span><br><span class="line">    <span class="comment"># print &#x27;meanRemoved=&#x27;, meanRemoved</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># cov协方差=[(x1-x均值)*(y1-y均值)+(x2-x均值)*(y2-y均值)+...+(xn-x均值)*(yn-y均值)+]/(n-1)</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    方差: （一维）度量两个随机变量关系的统计量</span></span><br><span class="line"><span class="string">    协方差:  （二维）度量各个维度偏离其均值的程度</span></span><br><span class="line"><span class="string">    协方差矩阵: （多维）度量各个维度偏离其均值的程度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    当 cov(X, Y)&gt;0时，表明X与Y正相关；(X越大，Y也越大；X越小Y，也越小。这种情况，我们称为“正相关”。)</span></span><br><span class="line"><span class="string">    当 cov(X, Y)&lt;0时，表明X与Y负相关；</span></span><br><span class="line"><span class="string">    当 cov(X, Y)=0时，表明X与Y不相关。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    covMat = cov(meanRemoved, rowvar=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># eigVals为特征值， eigVects为特征向量</span></span><br><span class="line">    eigVals, eigVects = linalg.eig(mat(covMat))</span><br><span class="line">    <span class="comment"># print &#x27;eigVals=&#x27;, eigVals</span></span><br><span class="line">    <span class="comment"># print &#x27;eigVects=&#x27;, eigVects</span></span><br><span class="line">    <span class="comment"># 对特征值，进行从小到大的排序，返回从小到大的index序号</span></span><br><span class="line">    <span class="comment"># 特征值的逆序就可以得到topNfeat个最大的特征向量</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; x = np.array([3, 1, 2])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; np.argsort(x)</span></span><br><span class="line"><span class="string">    array([1, 2, 0])  # index,1 = 1; index,2 = 2; index,0 = 3</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y = np.argsort(x)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y[::-1]</span></span><br><span class="line"><span class="string">    array([0, 2, 1])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y[:-3:-1]</span></span><br><span class="line"><span class="string">    array([0, 2])  # 取出 -1, -2</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y[:-6:-1]</span></span><br><span class="line"><span class="string">    array([0, 2, 1])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    eigValInd = argsort(eigVals)</span><br><span class="line">    <span class="comment"># print &#x27;eigValInd1=&#x27;, eigValInd</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># -1表示倒序，返回topN的特征值[-1 到 -(topNfeat+1) 但是不包括-(topNfeat+1)本身的倒叙]</span></span><br><span class="line">    eigValInd = eigValInd[:-(topNfeat+<span class="number">1</span>):-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># print &#x27;eigValInd2=&#x27;, eigValInd</span></span><br><span class="line">    <span class="comment"># 重组 eigVects 最大到最小</span></span><br><span class="line">    redEigVects = eigVects[:, eigValInd]</span><br><span class="line">    <span class="comment"># print &#x27;redEigVects=&#x27;, redEigVects.T</span></span><br><span class="line">    <span class="comment"># 将数据转换到新空间</span></span><br><span class="line">    <span class="comment"># --- (1567, 590) (590, 20)</span></span><br><span class="line">    <span class="comment"># print &quot;---&quot;, shape(meanRemoved), shape(redEigVects)</span></span><br><span class="line">    lowDDataMat = meanRemoved * redEigVects</span><br><span class="line">    reconMat = (lowDDataMat * redEigVects.T) + meanVals</span><br><span class="line">    <span class="comment"># print &#x27;lowDDataMat=&#x27;, lowDDataMat</span></span><br><span class="line">    <span class="comment"># print &#x27;reconMat=&#x27;, reconMat</span></span><br><span class="line">    <span class="keyword">return</span> lowDDataMat, reconMat</span><br></pre></td></tr></table></div></figure>

<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/13.PCA/pca.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/13.PCA/pca.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/13.PCA/pca.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="要点补充"   >
          <a href="#要点补充" class="heading-link"><i class="fas fa-link"></i></a><a href="#要点补充" class="headerlink" title="要点补充"></a>要点补充</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">降维技术使得数据变的更易使用，并且它们往往能够去除数据中的噪音，使得其他机器学习任务更加精确。</span><br><span class="line">降维往往作为预处理步骤，在数据应用到其他算法之前清洗数据。</span><br><span class="line">比较流行的降维技术:  独立成分分析、因子分析 和 主成分分析， 其中又以主成分分析应用最广泛。</span><br><span class="line"></span><br><span class="line">本章中的PCA将所有的数据集都调入了内存，如果无法做到，就需要其他的方法来寻找其特征值。</span><br><span class="line">如果使用在线PCA分析的方法，你可以参考一篇优秀的论文 &quot;Incremental Eigenanalysis for Classification&quot;。 </span><br><span class="line">下一章要讨论的奇异值分解方法也可以用于特征值分析。</span><br></pre></td></tr></table></div></figure>

<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~lihuisong" >1988</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/12/ml_12/">第12章 使用FP-growth算法来高效发现频繁项集</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-12</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="img/apachecn_fp_growth_homepage.png"></p>

        <h2 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a><a href="#前言" class="headerlink" title="前言"></a>前言</h2>
      <p>在 <a href="">第11章</a> 时我们已经介绍了用 <code>Apriori</code> 算法发现 <code>频繁项集</code> 与 <code>关联规则</code>。<br>本章将继续关注发现 <code>频繁项集</code> 这一任务，并使用 <code>FP-growth</code> 算法更有效的挖掘 <code>频繁项集</code>。</p>

        <h2 id="FP-growth-算法简介"   >
          <a href="#FP-growth-算法简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#FP-growth-算法简介" class="headerlink" title="FP-growth 算法简介"></a>FP-growth 算法简介</h2>
      <ul>
<li>一种非常好的发现频繁项集算法。</li>
<li>基于Apriori算法构建,但是数据结构不同，使用叫做 <code>FP树</code> 的数据结构结构来存储集合。下面我们会介绍这种数据结构。</li>
</ul>

        <h2 id="FP-growth-算法步骤"   >
          <a href="#FP-growth-算法步骤" class="heading-link"><i class="fas fa-link"></i></a><a href="#FP-growth-算法步骤" class="headerlink" title="FP-growth 算法步骤"></a>FP-growth 算法步骤</h2>
      <ul>
<li>基于数据构建FP树   </li>
<li>从FP树种挖掘频繁项集 </li>
</ul>

        <h2 id="FP树-介绍"   >
          <a href="#FP树-介绍" class="heading-link"><i class="fas fa-link"></i></a><a href="#FP树-介绍" class="headerlink" title="FP树 介绍"></a>FP树 介绍</h2>
      <ul>
<li>FP树的节点结构如下:</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">treeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nameValue, numOccur, parentNode</span>):</span></span><br><span class="line">        self.name = nameValue     <span class="comment"># 节点名称</span></span><br><span class="line">        self.count = numOccur     <span class="comment"># 节点出现次数</span></span><br><span class="line">        self.nodeLink = <span class="literal">None</span>      <span class="comment"># 不同项集的相同项通过nodeLink连接在一起</span></span><br><span class="line">        <span class="comment"># needs to be updated</span></span><br><span class="line">        self.parent = parentNode  <span class="comment"># 指向父节点</span></span><br><span class="line">        self.children = &#123;&#125;        <span class="comment"># 存储叶子节点</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="FP-growth-原理"   >
          <a href="#FP-growth-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#FP-growth-原理" class="headerlink" title="FP-growth 原理"></a>FP-growth 原理</h2>
      <p>基于数据构建FP树</p>
<p>步骤1:</p>
<ol>
<li>遍历所有的数据集合，计算所有项的支持度。</li>
<li>丢弃非频繁的项。</li>
<li>基于 支持度 降序排序所有的项。<br><img src="img/%E6%AD%A5%E9%AA%A41-3.png"></li>
<li>所有数据集合按照得到的顺序重新整理。</li>
<li>重新整理完成后，丢弃每个集合末尾非频繁的项。<br><img src="img/%E6%AD%A5%E9%AA%A44-5.png"></li>
</ol>
<p>步骤2:<br>   6. 读取每个集合插入FP树中，同时用一个头部链表数据结构维护不同集合的相同项。<br>      <img src="img/%E6%AD%A5%E9%AA%A46-1.png"><br>      最终得到下面这样一棵FP树<br>      <img src="img/%E6%AD%A5%E9%AA%A46-2.png"></p>
<p>从FP树中挖掘出频繁项集</p>
<p>步骤3:</p>
<ol>
<li>对头部链表进行降序排序</li>
<li>对头部链表节点从小到大遍历，得到条件模式基，同时获得一个频繁项集。<br>  <img src="img/%E6%AD%A5%E9%AA%A46-2.png"><br>  如上图，从头部链表 t 节点开始遍历，t 节点加入到频繁项集。找到以 t 节点为结尾的路径如下:<br>  <img src="img/%E6%AD%A5%E9%AA%A47-1.png"><br>  去掉FP树中的t节点，得到条件模式基&lt;左边路径, 右边是值&gt;[z,x,y,s,t]:2，[z,x,y,r,t]:1 。条件模式基的值取决于末尾节点 t ，因为 t 的出现次数最小，一个频繁项集的支持度由支持度最小的项决定。所以 t 节点的条件模式基的值可以理解为对于以 t 节点为末尾的前缀路径出现次数。</li>
<li>条件模式基继续构造条件 FP树， 得到频繁项集，和之前的频繁项组合起来，这是一个递归遍历头部链表生成FP树的过程，递归截止条件是生成的FP树的头部链表为空。<br>  根据步骤 2 得到的条件模式基 [z,x,y,s,t]:2，[z,x,y,r,t]:1 作为数据集继续构造出一棵FP树，计算支持度，去除非频繁项，集合按照支持度降序排序，重复上面构造FP树的步骤。最后得到下面 t-条件FP树 :<br>  <img src="img/%E6%AD%A5%E9%AA%A47-2.png"><br>  然后根据 t-条件FP树 的头部链表进行遍历，从 y 开始。得到频繁项集 ty 。然后又得到 y 的条件模式基，构造出 ty的条件FP树，即 ty-条件FP树。继续遍历ty-条件FP树的头部链表，得到频繁项集 tyx，然后又得到频繁项集 tyxz. 然后得到构造tyxz-条件FP树的头部链表是空的，终止遍历。我们得到的频繁项集有 t-&gt;ty-&gt;tyz-&gt;tyzx，这只是一小部分。</li>
</ol>
<ul>
<li>条件模式基:头部链表中的某一点的前缀路径组合就是条件模式基，条件模式基的值取决于末尾节点的值。</li>
<li>条件FP树:以条件模式基为数据集构造的FP树叫做条件FP树。</li>
</ul>
<p>FP-growth 算法优缺点:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">* 优点:  1. 因为 FP-growth 算法只需要对数据集遍历两次，所以速度更快。</span><br><span class="line">        2. FP树将集合按照支持度降序排序，不同路径如果有相同前缀路径共用存储空间，使得数据得到了压缩。</span><br><span class="line">        3. 不需要生成候选集。</span><br><span class="line">        4. 比Apriori更快。</span><br><span class="line">* 缺点:  1. FP-Tree第二次遍历会存储很多中间过程的值，会占用很多内存。</span><br><span class="line">        2. 构建FP-Tree是比较昂贵的。</span><br><span class="line">* 适用数据类型: 标称型数据(离散型数据)。</span><br></pre></td></tr></table></div></figure>



        <h2 id="FP-growth-代码讲解"   >
          <a href="#FP-growth-代码讲解" class="heading-link"><i class="fas fa-link"></i></a><a href="#FP-growth-代码讲解" class="headerlink" title="FP-growth 代码讲解"></a>FP-growth 代码讲解</h2>
      <p>完整代码地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>main 方法大致步骤:</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    simpDat = loadSimpDat()                       <span class="comment">#加载数据集。</span></span><br><span class="line">    initSet = createInitSet(simpDat)              <span class="comment">#对数据集进行整理，相同集合进行合并。</span></span><br><span class="line">    myFPtree, myHeaderTab = createTree(initSet, <span class="number">3</span>)<span class="comment">#创建FP树。</span></span><br><span class="line">    freqItemList = []</span><br><span class="line">    mineTree(myFPtree, myHeaderTab, <span class="number">3</span>, <span class="built_in">set</span>([]), freqItemList) <span class="comment">#递归的从FP树中挖掘出频繁项集。</span></span><br><span class="line">    <span class="built_in">print</span> freqItemList</span><br></pre></td></tr></table></div></figure>
<p>大家看懂原理，再仔细跟踪一下代码。基本就没有问题了。</p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/mikechengwei" >mikechengwei</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/11/ml_11/">第11章 使用 Apriori 算法进行关联分析</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-11</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="img/apachecn_apriori_homepage.jpg"></p>

        <h2 id="关联分析"   >
          <a href="#关联分析" class="heading-link"><i class="fas fa-link"></i></a><a href="#关联分析" class="headerlink" title="关联分析"></a>关联分析</h2>
      <p>关联分析是一种在大规模数据集中寻找有趣关系的任务。<br>这些关系可以有两种形式: </p>
<ul>
<li>频繁项集（frequent item sets）: 经常出现在一块的物品的集合。</li>
<li>关联规则（associational rules）: 暗示两种物品之间可能存在很强的关系。</li>
</ul>

        <h2 id="相关术语"   >
          <a href="#相关术语" class="heading-link"><i class="fas fa-link"></i></a><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h2>
      <ul>
<li><p>关联分析（关联规则学习): 从大规模数据集中寻找物品间的隐含关系被称作 <code>关联分析(associati analysis)</code> 或者 <code>关联规则学习（association rule learning）</code> 。<br>下面是用一个 <code>杂货店</code> 例子来说明这两个概念，如下图所示:<br><img src="img/apachecn_apriori_association_demo_1.jpg" alt="关联分析示例1"></p>
</li>
<li><p>频繁项集: {葡萄酒, 尿布, 豆奶} 就是一个频繁项集的例子。</p>
</li>
<li><p>关联规则: 尿布 -&gt; 葡萄酒 就是一个关联规则。这意味着如果顾客买了尿布，那么他很可能会买葡萄酒。</p>
</li>
</ul>
<p>那么 <code>频繁</code> 的定义是什么呢？怎么样才算频繁呢？<br>度量它们的方法有很多种，这里我们来简单的介绍下支持度和可信度。</p>
<ul>
<li>支持度: 数据集中包含该项集的记录所占的比例。例如上图中，{豆奶} 的支持度为 4/5。{豆奶, 尿布} 的支持度为 3/5。</li>
<li>可信度: 针对一条诸如 {尿布} -&gt; {葡萄酒} 这样具体的关联规则来定义的。这条规则的 <code>可信度</code> 被定义为 <code>支持度(&#123;尿布, 葡萄酒&#125;)/支持度(&#123;尿布&#125;)</code>，从图中可以看出 支持度({尿布, 葡萄酒}) = 3/5，支持度({尿布}) = 4/5，所以 {尿布} -&gt; {葡萄酒} 的可信度 = 3/5 / 4/5 = 3/4 = 0.75。</li>
</ul>
<p><code>支持度</code> 和 <code>可信度</code> 是用来量化 <code>关联分析</code> 是否成功的一个方法。<br>假设想找到支持度大于 0.8 的所有项集，应该如何去做呢？<br>一个办法是生成一个物品所有可能组合的清单，然后对每一种组合统计它出现的频繁程度，但是当物品成千上万时，上述做法就非常非常慢了。<br>我们需要详细分析下这种情况并讨论下 Apriori 原理，该原理会减少关联规则学习时所需的计算量。</p>

        <h2 id="Apriori-原理"   >
          <a href="#Apriori-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#Apriori-原理" class="headerlink" title="Apriori 原理"></a>Apriori 原理</h2>
      <p>假设我们一共有 4 个商品: 商品0, 商品1, 商品2, 商品3。<br>所有可能的情况如下:<br><img src="img/apachecn_apriori_goods_all_1.jpg" alt="4种商品的所有组合"><br>如果我们计算所有组合的支持度，也需要计算 15 次。即 2^N - 1 = 2^4 - 1 = 15。<br>随着物品的增加，计算的次数呈指数的形式增长 …<br>为了降低计算次数和时间，研究人员发现了一种所谓的 Apriori 原理，即某个项集是频繁的，那么它的所有子集也是频繁的。<br>例如，如果 {0, 1} 是频繁的，那么 {0}, {1} 也是频繁的。<br>该原理直观上没有什么帮助，但是如果反过来看就有用了，也就是说如果一个项集是 <code>非频繁项集</code>，那么它的所有超集也是非频繁项集，如下图所示:  </p>
<p><img src="img/%E9%9D%9E%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86.png" alt="非频繁项集"></p>
<p>在图中我们可以看到，已知灰色部分 {2,3} 是 <code>非频繁项集</code>，那么利用上面的知识，我们就可以知道 {0,2,3} {1,2,3} {0,1,2,3} 都是 <code>非频繁的</code>。<br>也就是说，计算出 {2,3} 的支持度，知道它是 <code>非频繁</code> 的之后，就不需要再计算 {0,2,3} {1,2,3} {0,1,2,3} 的支持度，因为我们知道这些集合不会满足我们的要求。<br>使用该原理就可以避免项集数目的指数增长，从而在合理的时间内计算出频繁项集。</p>
<p>Apriori 算法优缺点</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* 优点: 易编码实现</span><br><span class="line">* 缺点: 在大数据集上可能较慢</span><br><span class="line">* 适用数据类型: 数值型 或者 标称型数据。</span><br></pre></td></tr></table></div></figure>

<p>Apriori 算法流程步骤: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">* 收集数据: 使用任意方法。</span><br><span class="line">* 准备数据: 任何数据类型都可以，因为我们只保存集合。</span><br><span class="line">* 分析数据: 使用任意方法。</span><br><span class="line">* 训练数据: 使用Apiori算法来找到频繁项集。</span><br><span class="line">* 测试算法: 不需要测试过程。</span><br><span class="line">* 使用算法: 用于发现频繁项集以及物品之间的关联规则。</span><br></pre></td></tr></table></div></figure>


        <h2 id="Apriori-算法的使用"   >
          <a href="#Apriori-算法的使用" class="heading-link"><i class="fas fa-link"></i></a><a href="#Apriori-算法的使用" class="headerlink" title="Apriori 算法的使用"></a>Apriori 算法的使用</h2>
      <p>前面提到，关联分析的目标包括两项: 发现 <code>频繁项集</code> 和发现 <code>关联规则</code>。<br>首先需要找到 <code>频繁项集</code>，然后才能发现 <code>关联规则</code>。<br><code>Apriori</code> 算法是发现 <code>频繁项集</code> 的一种方法。<br>Apriori 算法的两个输入参数分别是最小支持度和数据集。<br>该算法首先会生成所有单个物品的项集列表。<br>接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度要求的集合会被去掉。<br>燃尽后对生下来的集合进行组合以声场包含两个元素的项集。<br>接下来再重新扫描交易记录，去掉不满足最小支持度的项集。<br>该过程重复进行直到所有项集被去掉。</p>

        <h3 id="生成候选项集"   >
          <a href="#生成候选项集" class="heading-link"><i class="fas fa-link"></i></a><a href="#生成候选项集" class="headerlink" title="生成候选项集"></a>生成候选项集</h3>
      <p>下面会创建一个用于构建初始集合的函数，也会创建一个通过扫描数据集以寻找交易记录子集的函数，<br>数据扫描的伪代码如下: </p>
<ul>
<li>对数据集中的每条交易记录 tran</li>
<li>对每个候选项集 can<ul>
<li>检查一下 can 是否是 tran 的子集: 如果是则增加 can 的计数值</li>
</ul>
</li>
<li>对每个候选项集<ul>
<li>如果其支持度不低于最小值，则保留该项集</li>
<li>返回所有频繁项集列表<br>以下是一些辅助函数。</li>
</ul>
</li>
</ul>

        <h4 id="加载数据集"   >
          <a href="#加载数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>():</span></span><br><span class="line">    <span class="keyword">return</span> [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br></pre></td></tr></table></div></figure>


        <h4 id="创建集合-C1。即对-dataSet-进行去重，排序，放入-list-中，然后转换所有的元素为-frozenset"   >
          <a href="#创建集合-C1。即对-dataSet-进行去重，排序，放入-list-中，然后转换所有的元素为-frozenset" class="heading-link"><i class="fas fa-link"></i></a><a href="#创建集合-C1。即对-dataSet-进行去重，排序，放入-list-中，然后转换所有的元素为-frozenset" class="headerlink" title="创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset"></a>创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createC1</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;createC1（创建集合 C1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 原始数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        frozenset 返回一个 frozenset 格式的 list</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    C1 = []</span><br><span class="line">    <span class="keyword">for</span> transaction <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> transaction:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                <span class="comment"># 遍历所有的元素，如果不在 C1 出现过，那么就 append</span></span><br><span class="line">                C1.append([item])</span><br><span class="line">    <span class="comment"># 对数组进行 `从小到大` 的排序</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;sort 前=&#x27;</span>, C1</span><br><span class="line">    C1.sort()</span><br><span class="line">    <span class="comment"># frozenset 表示冻结的 set 集合，元素无改变；可以把它当字典的 key 来使用</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;sort 后=&#x27;</span>, C1</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;frozenset=&#x27;</span>, <span class="built_in">map</span>(<span class="built_in">frozenset</span>, C1)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">map</span>(<span class="built_in">frozenset</span>, C1)</span><br></pre></td></tr></table></div></figure>


        <h4 id="计算候选数据集-CK-在数据集-D-中的支持度，并返回支持度大于最小支持度（minSupport）的数据"   >
          <a href="#计算候选数据集-CK-在数据集-D-中的支持度，并返回支持度大于最小支持度（minSupport）的数据" class="heading-link"><i class="fas fa-link"></i></a><a href="#计算候选数据集-CK-在数据集-D-中的支持度，并返回支持度大于最小支持度（minSupport）的数据" class="headerlink" title="计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据"></a>计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanD</span>(<span class="params">D, Ck, minSupport</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;scanD（计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度 minSupport 的数据）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        D 数据集</span></span><br><span class="line"><span class="string">        Ck 候选项集列表</span></span><br><span class="line"><span class="string">        minSupport 最小支持度</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retList 支持度大于 minSupport 的集合</span></span><br><span class="line"><span class="string">        supportData 候选项集支持度数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ssCnt 临时存放选数据集 Ck 的频率. 例如: a-&gt;10, b-&gt;5, c-&gt;8</span></span><br><span class="line">    ssCnt = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> tid <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">for</span> can <span class="keyword">in</span> Ck:</span><br><span class="line">            <span class="comment"># s.issubset(t)  测试是否 s 中的每一个元素都在 t 中</span></span><br><span class="line">            <span class="keyword">if</span> can.issubset(tid):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ssCnt.has_key(can):</span><br><span class="line">                    ssCnt[can] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ssCnt[can] += <span class="number">1</span></span><br><span class="line">    numItems = <span class="built_in">float</span>(<span class="built_in">len</span>(D)) <span class="comment"># 数据集 D 的数量</span></span><br><span class="line">    retList = []</span><br><span class="line">    supportData = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> ssCnt:</span><br><span class="line">        <span class="comment"># 支持度 = 候选项（key）出现的次数 / 所有数据集的数量</span></span><br><span class="line">        support = ssCnt[key]/numItems</span><br><span class="line">        <span class="keyword">if</span> support &gt;= minSupport:</span><br><span class="line">            <span class="comment"># 在 retList 的首位插入元素，只存储支持度满足频繁项集的值</span></span><br><span class="line">            retList.insert(<span class="number">0</span>, key)</span><br><span class="line">        <span class="comment"># 存储所有的候选项（key）和对应的支持度（support）</span></span><br><span class="line">        supportData[key] = support</span><br><span class="line">    <span class="keyword">return</span> retList, supportData</span><br></pre></td></tr></table></div></figure>

<p>完整代码地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="组织完整的-Apriori-算法"   >
          <a href="#组织完整的-Apriori-算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#组织完整的-Apriori-算法" class="headerlink" title="组织完整的 Apriori 算法"></a>组织完整的 Apriori 算法</h3>
      
        <h4 id="输入频繁项集列表-Lk-与返回的元素个数-k，然后输出所有可能的候选项集-Ck"   >
          <a href="#输入频繁项集列表-Lk-与返回的元素个数-k，然后输出所有可能的候选项集-Ck" class="heading-link"><i class="fas fa-link"></i></a><a href="#输入频繁项集列表-Lk-与返回的元素个数-k，然后输出所有可能的候选项集-Ck" class="headerlink" title="输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck"></a>输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aprioriGen</span>(<span class="params">Lk, k</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;aprioriGen（输入频繁项集列表 Lk 与返回的元素个数 k，然后输出候选项集 Ck。</span></span><br><span class="line"><span class="string">       例如: 以 &#123;0&#125;,&#123;1&#125;,&#123;2&#125; 为输入且 k = 2 则输出 &#123;0,1&#125;, &#123;0,2&#125;, &#123;1,2&#125;. 以 &#123;0,1&#125;,&#123;0,2&#125;,&#123;1,2&#125; 为输入且 k = 3 则输出 &#123;0,1,2&#125;</span></span><br><span class="line"><span class="string">       仅需要计算一次，不需要将所有的结果计算出来，然后进行去重操作</span></span><br><span class="line"><span class="string">       这是一个更高效的算法）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Lk 频繁项集列表</span></span><br><span class="line"><span class="string">        k 返回的项集元素个数（若元素的前 k-2 相同，就进行合并）</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retList 元素两两合并的数据集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = <span class="built_in">len</span>(Lk)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, lenLk):</span><br><span class="line">            L1 = <span class="built_in">list</span>(Lk[i])[: k-<span class="number">2</span>]</span><br><span class="line">            L2 = <span class="built_in">list</span>(Lk[j])[: k-<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># print &#x27;-----i=&#x27;, i, k-2, Lk, Lk[i], list(Lk[i])[: k-2]</span></span><br><span class="line">            <span class="comment"># print &#x27;-----j=&#x27;, j, k-2, Lk, Lk[j], list(Lk[j])[: k-2]</span></span><br><span class="line">            L1.sort()</span><br><span class="line">            L2.sort()</span><br><span class="line">            <span class="comment"># 第一次 L1,L2 为空，元素直接进行合并，返回元素两两合并的数据集</span></span><br><span class="line">            <span class="comment"># if first k-2 elements are equal</span></span><br><span class="line">            <span class="keyword">if</span> L1 == L2:</span><br><span class="line">                <span class="comment"># set union</span></span><br><span class="line">                <span class="comment"># print &#x27;union=&#x27;, Lk[i] | Lk[j], Lk[i], Lk[j]</span></span><br><span class="line">                retList.append(Lk[i] | Lk[j])</span><br><span class="line">    <span class="keyword">return</span> retList</span><br></pre></td></tr></table></div></figure>


        <h4 id="找出数据集-dataSet-中支持度-gt-最小支持度的候选项集以及它们的支持度。即我们的频繁项集。"   >
          <a href="#找出数据集-dataSet-中支持度-gt-最小支持度的候选项集以及它们的支持度。即我们的频繁项集。" class="heading-link"><i class="fas fa-link"></i></a><a href="#找出数据集-dataSet-中支持度-gt-最小支持度的候选项集以及它们的支持度。即我们的频繁项集。" class="headerlink" title="找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。"></a>找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apriori</span>(<span class="params">dataSet, minSupport=<span class="number">0.5</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;apriori（首先构建集合 C1，然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度的要求。那么满足最小支持度要求的项集构成集合 L1。然后 L1 中的元素相互组合成 C2，C2 再进一步过滤变成 L2，然后以此类推，知道 CN 的长度为 0 时结束，即可找出所有频繁项集的支持度。）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 原始数据集</span></span><br><span class="line"><span class="string">        minSupport 支持度的阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        L 频繁项集的全集</span></span><br><span class="line"><span class="string">        supportData 所有元素和支持度的全集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># C1 即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset</span></span><br><span class="line">    C1 = createC1(dataSet)</span><br><span class="line">    <span class="comment"># 对每一行进行 set 转换，然后存放到集合中</span></span><br><span class="line">    D = <span class="built_in">map</span>(<span class="built_in">set</span>, dataSet)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;D=&#x27;</span>, D</span><br><span class="line">    <span class="comment"># 计算候选数据集 C1 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据</span></span><br><span class="line">    L1, supportData = scanD(D, C1, minSupport)</span><br><span class="line">    <span class="comment"># print &quot;L1=&quot;, L1, &quot;\n&quot;, &quot;outcome: &quot;, supportData</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># L 加了一层 list, L 一共 2 层 list</span></span><br><span class="line">    L = [L1]</span><br><span class="line">    k = <span class="number">2</span></span><br><span class="line">    <span class="comment"># 判断 L 的第 k-2 项的数据长度是否 &gt; 0。第一次执行时 L 为 [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]。L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]，最后面 k += 1</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">len</span>(L[k-<span class="number">2</span>]) &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;k=&#x27;</span>, k, L, L[k-<span class="number">2</span>]</span><br><span class="line">        Ck = aprioriGen(L[k-<span class="number">2</span>], k) <span class="comment"># 例如: 以 &#123;0&#125;,&#123;1&#125;,&#123;2&#125; 为输入且 k = 2 则输出 &#123;0,1&#125;, &#123;0,2&#125;, &#123;1,2&#125;. 以 &#123;0,1&#125;,&#123;0,2&#125;,&#123;1,2&#125; 为输入且 k = 3 则输出 &#123;0,1,2&#125;</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;Ck&#x27;</span>, Ck</span><br><span class="line"></span><br><span class="line">        Lk, supK = scanD(D, Ck, minSupport) <span class="comment"># 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据</span></span><br><span class="line">        <span class="comment"># 保存所有候选项集的支持度，如果字典没有，就追加元素，如果有，就更新元素</span></span><br><span class="line">        supportData.update(supK)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(Lk) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># Lk 表示满足频繁子项的集合，L 元素在增加，例如: </span></span><br><span class="line">        <span class="comment"># l=[[set(1), set(2), set(3)]]</span></span><br><span class="line">        <span class="comment"># l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]]</span></span><br><span class="line">        L.append(Lk)</span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">        <span class="comment"># print &#x27;k=&#x27;, k, len(L[k-2])</span></span><br><span class="line">    <span class="keyword">return</span> L, supportData</span><br></pre></td></tr></table></div></figure>

<p>到这一步，我们就找出我们所需要的 <code>频繁项集</code> 和他们的 <code>支持度</code> 了，接下来再找出关联规则即可！</p>
<p>完整代码地址: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="从频繁项集中挖掘关联规则"   >
          <a href="#从频繁项集中挖掘关联规则" class="heading-link"><i class="fas fa-link"></i></a><a href="#从频繁项集中挖掘关联规则" class="headerlink" title="从频繁项集中挖掘关联规则"></a>从频繁项集中挖掘关联规则</h2>
      <p>前面我们介绍了用于发现 <code>频繁项集</code> 的 Apriori 算法，现在要解决的问题是如何找出 <code>关联规则</code>。</p>
<p>要找到 <code>关联规则</code>，我们首先从一个 <code>频繁项集</code> 开始。<br>我们知道集合中的元素是不重复的，但我们想知道基于这些元素能否获得其它内容。<br>某个元素或某个元素集合可能会推导出另一个元素。<br>从先前 <code>杂货店</code> 的例子可以得到，如果有一个频繁项集 {豆奶,莴苣}，那么就可能有一条关联规则 “豆奶 -&gt; 莴苣”。<br>这意味着如果有人买了豆奶，那么在统计上他会购买莴苣的概率比较大。<br>但是，这一条件反过来并不总是成立。<br>也就是说 “豆奶 -&gt; 莴苣” 统计上显著，那么 “莴苣 -&gt; 豆奶” 也不一定成立。</p>
<p>前面我们给出了 <code>频繁项集</code> 的量化定义，即它满足最小支持度要求。<br>对于 <code>关联规则</code>，我们也有类似的量化方法，这种量化指标称之为 <code>可信度</code>。<br>一条规则 A -&gt; B 的可信度定义为 support(A | B) / support(A)。（注意: 在 python 中 | 表示集合的并操作，而数学书集合并的符号是 U）。<br><code>A | B</code> 是指所有出现在集合 A 或者集合 B 中的元素。<br>由于我们先前已经计算出所有 <code>频繁项集</code> 的支持度了，现在我们要做的只不过是提取这些数据做一次除法运算即可。</p>

        <h3 id="一个频繁项集可以产生多少条关联规则呢？"   >
          <a href="#一个频繁项集可以产生多少条关联规则呢？" class="heading-link"><i class="fas fa-link"></i></a><a href="#一个频繁项集可以产生多少条关联规则呢？" class="headerlink" title="一个频繁项集可以产生多少条关联规则呢？"></a>一个频繁项集可以产生多少条关联规则呢？</h3>
      <p>如下图所示，给出的是项集 {0,1,2,3} 产生的所有关联规则:<br><img src="img/apachecn_association_rule_demo_1.jpg" alt="关联规则网格示意图"><br>与我们前面的 <code>频繁项集</code> 生成一样，我们可以为每个频繁项集产生许多关联规则。<br>如果能减少规则的数目来确保问题的可解析，那么计算起来就会好很多。<br>通过观察，我们可以知道，如果某条规则并不满足 <code>最小可信度</code> 要求，那么该规则的所有子集也不会满足 <code>最小可信度</code> 的要求。<br>如上图所示，假设 <code>123 -&gt; 3</code>  并不满足最小可信度要求，那么就知道任何左部为 {0,1,2} 子集的规则也不会满足 <code>最小可信度</code> 的要求。<br>即 <code>12 -&gt; 03</code> , <code>02 -&gt; 13</code> , <code>01 -&gt; 23</code> , <code>2 -&gt; 013</code>, <code> 1 -&gt; 023</code>, <code>0 -&gt; 123</code> 都不满足 <code>最小可信度</code> 要求。  </p>
<p>可以利用关联规则的上述性质属性来减少需要测试的规则数目，跟先前 Apriori 算法的套路一样。<br>以下是一些辅助函数: </p>

        <h4 id="计算可信度"   >
          <a href="#计算可信度" class="heading-link"><i class="fas fa-link"></i></a><a href="#计算可信度" class="headerlink" title="计算可信度"></a>计算可信度</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算可信度（confidence）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConf</span>(<span class="params">freqSet, H</span></span></span><br><span class="line"><span class="params"><span class="function">, supportData, brl, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;calcConf（对两个元素的频繁项，计算可信度，例如:  &#123;1,2&#125;/&#123;1&#125; 或者 &#123;1,2&#125;/&#123;2&#125; 看是否满足条件）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        freqSet 频繁项集中的元素，例如: frozenset([1, 3])    </span></span><br><span class="line"><span class="string">        H 频繁项集中的元素的集合，例如: [frozenset([1]), frozenset([3])]</span></span><br><span class="line"><span class="string">        supportData 所有元素的支持度的字典</span></span><br><span class="line"><span class="string">        brl 关联规则列表的空数组</span></span><br><span class="line"><span class="string">        minConf 最小可信度</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        prunedH 记录 可信度大于阈值的集合</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 记录可信度大于最小可信度（minConf）的集合</span></span><br><span class="line">    prunedH = []</span><br><span class="line">    <span class="keyword">for</span> conseq <span class="keyword">in</span> H: <span class="comment"># 假设 freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]，那么现在需要求出 frozenset([1]) -&gt; frozenset([3]) 的可信度和 frozenset([3]) -&gt; frozenset([1]) 的可信度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print &#x27;confData=&#x27;, freqSet, H, conseq, freqSet-conseq</span></span><br><span class="line">        conf = supportData[freqSet]/supportData[freqSet-conseq] <span class="comment"># 支持度定义: a -&gt; b = support(a | b) / support(a). 假设  freqSet = frozenset([1, 3]), conseq = [frozenset([1])]，那么 frozenset([1]) 至 frozenset([3]) 的可信度为 = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])]</span></span><br><span class="line">        <span class="keyword">if</span> conf &gt;= minConf:</span><br><span class="line">            <span class="comment"># 只要买了 freqSet-conseq 集合，一定会买 conseq 集合（freqSet-conseq 集合和 conseq 集合是全集）</span></span><br><span class="line">            <span class="built_in">print</span> freqSet-conseq, <span class="string">&#x27;--&gt;&#x27;</span>, conseq, <span class="string">&#x27;conf:&#x27;</span>, conf</span><br><span class="line">            brl.append((freqSet-conseq, conseq, conf))</span><br><span class="line">            prunedH.append(conseq)</span><br><span class="line">    <span class="keyword">return</span> prunedH</span><br><span class="line">​````</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 递归计算频繁项集的规则</span></span><br><span class="line"></span><br><span class="line">​```python</span><br><span class="line"><span class="comment"># 递归计算频繁项集的规则</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rulesFromConseq</span>(<span class="params">freqSet, H, supportData, brl, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;rulesFromConseq</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        freqSet 频繁项集中的元素，例如: frozenset([2, 3, 5])    </span></span><br><span class="line"><span class="string">        H 频繁项集中的元素的集合，例如: [frozenset([2]), frozenset([3]), frozenset([5])]</span></span><br><span class="line"><span class="string">        supportData 所有元素的支持度的字典</span></span><br><span class="line"><span class="string">        brl 关联规则列表的数组</span></span><br><span class="line"><span class="string">        minConf 最小可信度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># H[0] 是 freqSet 的元素组合的第一个元素，并且 H 中所有元素的长度都一样，长度由 aprioriGen(H, m+1) 这里的 m + 1 来控制</span></span><br><span class="line">    <span class="comment"># 该函数递归时，H[0] 的长度从 1 开始增长 1 2 3 ...</span></span><br><span class="line">    <span class="comment"># 假设 freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])]</span></span><br><span class="line">    <span class="comment"># 那么 m = len(H[0]) 的递归的值依次为 1 2</span></span><br><span class="line">    <span class="comment"># 在 m = 2 时, 跳出该递归。假设再递归一次，那么 H[0] = frozenset([2, 3, 5])，freqSet = frozenset([2, 3, 5]) ，没必要再计算 freqSet 与 H[0] 的关联规则了。</span></span><br><span class="line">    m = <span class="built_in">len</span>(H[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(freqSet) &gt; (m + <span class="number">1</span>)):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;freqSet******************&#x27;</span>, <span class="built_in">len</span>(freqSet), m + <span class="number">1</span>, freqSet, H, H[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 生成 m+1 个长度的所有可能的 H 中的组合，假设 H = [frozenset([2]), frozenset([3]), frozenset([5])]</span></span><br><span class="line">        <span class="comment"># 第一次递归调用时生成 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])]</span></span><br><span class="line">        <span class="comment"># 第二次 。。。没有第二次，递归条件判断时已经退出了</span></span><br><span class="line">        Hmp1 = aprioriGen(H, m+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 返回可信度大于最小可信度的集合</span></span><br><span class="line">        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;Hmp1=&#x27;</span>, Hmp1</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;len(Hmp1)=&#x27;</span>, <span class="built_in">len</span>(Hmp1), <span class="string">&#x27;len(freqSet)=&#x27;</span>, <span class="built_in">len</span>(freqSet)</span><br><span class="line">        <span class="comment"># 计算可信度后，还有数据大于最小可信度的话，那么继续递归调用，否则跳出递归</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">len</span>(Hmp1) &gt; <span class="number">1</span>):</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&#x27;----------------------&#x27;</span>, Hmp1</span><br><span class="line">            <span class="comment"># print len(freqSet),  len(Hmp1[0]) + 1</span></span><br><span class="line">            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></div></figure>


        <h4 id="生成关联规则"   >
          <a href="#生成关联规则" class="heading-link"><i class="fas fa-link"></i></a><a href="#生成关联规则" class="headerlink" title="生成关联规则"></a>生成关联规则</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成关联规则</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRules</span>(<span class="params">L, supportData, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;generateRules</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        L 频繁项集列表</span></span><br><span class="line"><span class="string">        supportData 频繁项集支持度的字典</span></span><br><span class="line"><span class="string">        minConf 最小置信度</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bigRuleList 可信度规则列表（关于 (A-&gt;B+置信度) 3个字段的组合）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bigRuleList = []</span><br><span class="line">    <span class="comment"># 假设 L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]]</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(L)):</span><br><span class="line">        <span class="comment"># 获取频繁项集中每个组合的所有元素</span></span><br><span class="line">        <span class="keyword">for</span> freqSet <span class="keyword">in</span> L[i]:</span><br><span class="line">            <span class="comment"># 假设: freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])]</span></span><br><span class="line">            <span class="comment"># 组合总的元素并遍历子元素，并转化为 frozenset 集合，再存放到 list 列表中</span></span><br><span class="line">            H1 = [<span class="built_in">frozenset</span>([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freqSet]</span><br><span class="line">            <span class="comment"># 2 个的组合，走 else, 2 个以上的组合，走 if</span></span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">1</span>):</span><br><span class="line">                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                calcConf(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">    <span class="keyword">return</span> bigRuleList</span><br></pre></td></tr></table></div></figure>

<p>到这里为止，通过调用 generateRules 函数即可得出我们所需的 <code>关联规则</code>。</p>
<ul>
<li>分级法:  频繁项集-&gt;关联规则<ul>
<li>1.首先从一个频繁项集开始，接着创建一个规则列表，其中规则右部分只包含一个元素，然后对这个规则进行测试。</li>
<li>2.接下来合并所有剩余规则来创建一个新的规则列表，其中规则右部包含两个元素。</li>
<li>如下图: </li>
<li><img src="img/%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%E9%A1%B9%E9%9B%86%E7%BB%84%E5%90%88.png" alt="所有可能的项集组合"></li>
</ul>
</li>
<li>最后:  每次增加频繁项集的大小，Apriori 算法都会重新扫描整个数据集，是否有优化空间呢？ 下一章: FP-growth算法等着你的到来</li>
</ul>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/10/ml_10/">第10章 K-Means（K-均值）聚类算法</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-10</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="img/K-Means_%E9%A6%96%E9%A1%B5.jpg" alt="K-Means（K-均值）聚类算法_首页"></p>

        <h2 id="聚类"   >
          <a href="#聚类" class="heading-link"><i class="fas fa-link"></i></a><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2>
      <p>聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，你对你的目标是未知的，同样以动物为例，对于一个动物集来说，你并不清楚这个数据集内部有多少种类的动物，你能做的只是利用聚类方法将它自动按照特征分为多类，然后人为给出这个聚类结果的定义（即簇识别）。例如，你将一个动物集分为了三簇（类），然后通过观察这三类动物的特征，你为每一个簇起一个名字，如大象、狗、猫等，这就是聚类的基本思想。     </p>
<p>至于“相似”这一概念，是利用距离这个评价标准来衡量的，我们通过计算对象与对象之间的距离远近来判断它们是否属于同一类别，即是否是同一个簇。至于距离如何计算，科学家们提出了许多种距离的计算方法，其中欧式距离是最为简单和常用的，除此之外还有曼哈顿距离和余弦相似性距离等。</p>
<p>欧式距离，我想大家再熟悉不过了，但为免有一些基础薄弱的同学，在此再说明一下，它的定义为:<br>对于x点坐标为(x1,x2,x3,…,xn)和 y点坐标为(y1,y2,y3,…,yn)，两者的欧式距离为:</p>
<p>$$<br>d(x,y)<br>    ={\sqrt{<br>            (x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2} + \cdots +(x_{n}-y_{n})^{2}<br>        }}<br>    ={\sqrt{<br>            \sum_{ {i=1} }^{n}(x_{i}-y_{i})^{2}<br>        }}<br>$$</p>
<p>在二维平面，它就是我们初中时就学过的两点距离公式</p>

        <h2 id="K-Means-算法"   >
          <a href="#K-Means-算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-算法" class="headerlink" title="K-Means 算法"></a>K-Means 算法</h2>
      <p>K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 <code>K-均值</code> 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.<br>簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.<br>聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.  </p>
<p><strong>优点</strong>:</p>
<ul>
<li>属于无监督学习，无须准备训练集</li>
<li>原理简单，实现起来较为容易</li>
<li>结果可解释性较好</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>需手动设置k值</strong>。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性</li>
<li>可能收敛到局部最小值, 在大规模数据集上收敛较慢</li>
<li>对于异常点、离群点敏感</li>
</ul>
<p>使用数据类型 : 数值型数据</p>

        <h3 id="K-Means-场景"   >
          <a href="#K-Means-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-场景" class="headerlink" title="K-Means 场景"></a>K-Means 场景</h3>
      <p>kmeans，如前所述，用于数据集内种类属性不明晰，希望能够通过数据挖掘出或自动归类出有相似特点的对象的场景。其商业界的应用场景一般为挖掘出具有相似特点的潜在客户群体以便公司能够重点研究、对症下药。  </p>
<p>例如，在2000年和2004年的美国总统大选中，候选人的得票数比较接近或者说非常接近。任一候选人得到的普选票数的最大百分比为50.7%而最小百分比为47.9% 如果1%的选民将手中的选票投向另外的候选人，那么选举结果就会截然不同。 实际上，如果妥善加以引导与吸引，少部分选民就会转换立场。尽管这类选举者占的比例较低，但当候选人的选票接近时，这些人的立场无疑会对选举结果产生非常大的影响。如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们？ 答案就是聚类（Clustering)。</p>
<p>那么，具体如何实施呢？首先，收集用户的信息，可以同时收集用户满意或不满意的信息，这是因为任何对用户重要的内容都可能影响用户的投票结果。然后，将这些信息输入到某个聚类算法中。接着，对聚类结果中的每一个簇（最好选择最大簇 ）， 精心构造能够吸引该簇选民的消息。最后， 开展竞选活动并观察上述做法是否有效。</p>
<p>另一个例子就是产品部门的市场调研了。为了更好的了解自己的用户，产品部门可以采用聚类的方法得到不同特征的用户群体，然后针对不同的用户群体可以对症下药，为他们提供更加精准有效的服务。</p>

        <h3 id="K-Means-术语"   >
          <a href="#K-Means-术语" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-术语" class="headerlink" title="K-Means 术语"></a>K-Means 术语</h3>
      <ul>
<li>簇: 所有数据的点集合，簇中的对象是相似的。</li>
<li>质心: 簇中所有点的中心（计算所有点的均值而来）.</li>
<li>SSE: Sum of Sqared Error（误差平方和）, 它被用来评估模型的好坏，SSE 值越小，表示越接近它们的质心. 聚类效果越好。由于对误差取了平方，因此更加注重那些远离中心的点（一般为边界点或离群点）。详情见kmeans的评价标准。</li>
</ul>
<p>有关 <code>簇</code> 和 <code>质心</code> 术语更形象的介绍, 请参考下图:</p>
<p><img src="img/apachecn-k-means-term-1.jpg" alt="K-Means 术语图"></p>

        <h3 id="K-Means-工作流程"   >
          <a href="#K-Means-工作流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-工作流程" class="headerlink" title="K-Means 工作流程"></a>K-Means 工作流程</h3>
      <ol>
<li>首先, 随机确定 K 个初始点作为质心（<strong>不必是数据中的点</strong>）。</li>
<li>然后将数据集中的每个点分配到一个簇中, 具体来讲, 就是为每个点找到距其最近的质心, 并将其分配该质心所对应的簇. 这一步完成之后, 每个簇的质心更新为该簇所有点的平均值.</li>
<li>重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束。</li>
</ol>
<p>上述过程的 <code>伪代码</code> 如下:</p>
<ul>
<li>创建 k 个点作为起始质心（通常是随机选择）</li>
<li>当任意一个点的簇分配结果发生改变时（不改变时算法结束）<ul>
<li>对数据集中的每个数据点<ul>
<li>对每个质心<ul>
<li>计算质心与数据点之间的距离</li>
</ul>
</li>
<li>将数据点分配到距其最近的簇</li>
</ul>
</li>
<li>对每一个簇, 计算簇中所有点的均值并将均值作为质心</li>
</ul>
</li>
</ul>

        <h3 id="K-Means-开发流程"   >
          <a href="#K-Means-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-开发流程" class="headerlink" title="K-Means 开发流程"></a>K-Means 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 使用任意方法</span><br><span class="line">准备数据: 需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算</span><br><span class="line">分析数据: 使用任意方法</span><br><span class="line">训练算法: 不适用于无监督学习，即无监督学习不需要训练步骤</span><br><span class="line">测试算法: 应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.</span><br><span class="line">使用算法: 可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.</span><br></pre></td></tr></table></div></figure>

        <h3 id="K-Means-的评价标准"   >
          <a href="#K-Means-的评价标准" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-的评价标准" class="headerlink" title="K-Means 的评价标准"></a>K-Means 的评价标准</h3>
      <p>k-means算法因为手动选取k值和初始化随机质心的缘故，每一次的结果不会完全一样，而且由于手动选取k值，我们需要知道我们选取的k值是否合理，聚类效果好不好，那么如何来评价某一次的聚类效果呢？也许将它们画在图上直接观察是最好的办法，但现实是，我们的数据不会仅仅只有两个特征，一般来说都有十几个特征，而观察十几维的空间对我们来说是一个无法完成的任务。因此，我们需要一个公式来帮助我们判断聚类的性能，这个公式就是<strong>SSE</strong> (Sum of Squared Error, 误差平方和 ），它其实就是每一个点到其簇内质心的距离的平方值的总和，这个数值对应kmeans函数中<strong>clusterAssment</strong>矩阵的第一列之和。 <strong>SSE</strong>值越小表示数据点越接近于它们的质心，聚类效果也越好。 因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低<strong>SSE</strong>值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。</p>

        <h3 id="K-Means-聚类算法函数"   >
          <a href="#K-Means-聚类算法函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-聚类算法函数" class="headerlink" title="K-Means 聚类算法函数"></a>K-Means 聚类算法函数</h3>
      
        <h4 id="从文件加载数据集"   >
          <a href="#从文件加载数据集" class="heading-link"><i class="fas fa-link"></i></a><a href="#从文件加载数据集" class="headerlink" title="从文件加载数据集"></a>从文件加载数据集</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文本中构建矩阵，加载文本文件，然后处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span>    <span class="comment"># 通用函数，用来解析以 tab 键分隔的 floats（浮点数），例如: 1.658985	4.285136</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        fltLine = <span class="built_in">map</span>(<span class="built_in">float</span>,curLine)    <span class="comment"># 映射所有的元素为 float（浮点数）类型</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br></pre></td></tr></table></div></figure>


        <h4 id="计算两个向量的欧氏距离"   >
          <a href="#计算两个向量的欧氏距离" class="heading-link"><i class="fas fa-link"></i></a><a href="#计算两个向量的欧氏距离" class="headerlink" title="计算两个向量的欧氏距离"></a>计算两个向量的欧氏距离</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个向量的欧式距离（可根据场景选择其他距离公式）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span>(<span class="params">vecA, vecB</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(<span class="built_in">sum</span>(power(vecA - vecB, <span class="number">2</span>))) <span class="comment"># la.norm(vecA-vecB)</span></span><br></pre></td></tr></table></div></figure>


        <h4 id="构建一个包含-K-个随机质心的集合"   >
          <a href="#构建一个包含-K-个随机质心的集合" class="heading-link"><i class="fas fa-link"></i></a><a href="#构建一个包含-K-个随机质心的集合" class="headerlink" title="构建一个包含 K 个随机质心的集合"></a>构建一个包含 K 个随机质心的集合</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为给定数据集构建一个包含 k 个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成。然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span>(<span class="params">dataSet, k</span>):</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>] <span class="comment"># 列的数量，即数据的特征个数</span></span><br><span class="line">    centroids = mat(zeros((k,n))) <span class="comment"># 创建k个质心矩阵</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n): <span class="comment"># 创建随机簇质心，并且在每一维的边界内</span></span><br><span class="line">        minJ = <span class="built_in">min</span>(dataSet[:,j])    <span class="comment"># 最小值</span></span><br><span class="line">        rangeJ = <span class="built_in">float</span>(<span class="built_in">max</span>(dataSet[:,j]) - minJ)    <span class="comment"># 范围 = 最大值 - 最小值</span></span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))    <span class="comment"># 随机生成，mat为numpy函数，需要在最开始写上 from numpy import *</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></div></figure>


        <h4 id="K-Means-聚类算法"   >
          <a href="#K-Means-聚类算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-聚类算法" class="headerlink" title="K-Means 聚类算法"></a>K-Means 聚类算法</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># k-means 聚类算法</span></span><br><span class="line"><span class="comment"># 该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。</span></span><br><span class="line"><span class="comment"># 这个过程重复数次，直到数据点的簇分配结果不再改变位置。</span></span><br><span class="line"><span class="comment"># 运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span>(<span class="params">dataSet, k, distMeas=distEclud, createCent=randCent</span>):</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]    <span class="comment"># 行数，即数据个数</span></span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))    <span class="comment"># 创建一个与 dataSet 行数一样，但是有两列的矩阵，用来保存簇分配结果</span></span><br><span class="line">    centroids = createCent(dataSet, k)    <span class="comment"># 创建质心，随机k个质心</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):    <span class="comment"># 循环每一个数据点并分配到最近的质心中去</span></span><br><span class="line">            minDist = inf; minIndex = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])    <span class="comment"># 计算数据点到质心的距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:    <span class="comment"># 如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）</span></span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:    <span class="comment"># 簇分配结果改变</span></span><br><span class="line">                clusterChanged = <span class="literal">True</span>    <span class="comment"># 簇改变</span></span><br><span class="line">                clusterAssment[i, :] = minIndex,minDist**<span class="number">2</span>    <span class="comment"># 更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方</span></span><br><span class="line">        <span class="built_in">print</span> centroids</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> <span class="built_in">range</span>(k): <span class="comment"># 更新质心</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A==cent)[<span class="number">0</span>]] <span class="comment"># 获取该簇中的所有点</span></span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) <span class="comment"># 将质心修改为簇中所有点的平均值，mean 就是求平均值的</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></div></figure>


        <h4 id="测试函数"   >
          <a href="#测试函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#测试函数" class="headerlink" title="测试函数"></a>测试函数</h4>
      <ol>
<li>测试一下以上的基础函数是否可以如预期运行, 请看: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>测试一下 kMeans 函数是否可以如预期运行, 请看: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> </li>
</ol>
<p>参考运行结果如下:<br><img src="img/apachecn-k-means-run-result-1.jpg" alt="K-Means 运行结果1"></p>

        <h3 id="K-Means-聚类算法的缺陷"   >
          <a href="#K-Means-聚类算法的缺陷" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-Means-聚类算法的缺陷" class="headerlink" title="K-Means 聚类算法的缺陷"></a>K-Means 聚类算法的缺陷</h3>
      <blockquote>
<p>在 kMeans 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）.</p>
</blockquote>
<p>局部最小值的的情况如下:<br><img src="img/apachecn-kmeans-partial-best-result-1.jpg" alt="K-Means 局部最小值1"><br>出现这个问题有很多原因，可能是k值取的不合适，可能是距离函数不合适，可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。</p>
<p>为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大<strong>SSE</strong>值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法，令k设为2。</p>
<p>为了保持簇总数不变，可以将某两个簇进行合并。从上图中很明显就可以看出，应该将上图下部两个出错的簇质心进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？</p>
<p>有两种可以量化的办法: 合并最近的质心，或者合并两个使得<strong>SSE</strong>增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总<strong>SSE</strong>值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。</p>
<p>因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法.   </p>

        <h3 id="二分-K-Means-聚类算法"   >
          <a href="#二分-K-Means-聚类算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分-K-Means-聚类算法" class="headerlink" title="二分 K-Means 聚类算法"></a>二分 K-Means 聚类算法</h3>
      <p>该算法首先将所有点作为一个簇，然后将该簇一分为二。<br>之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。<br>上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。  </p>

        <h4 id="二分-K-Means-聚类算法伪代码"   >
          <a href="#二分-K-Means-聚类算法伪代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分-K-Means-聚类算法伪代码" class="headerlink" title="二分 K-Means 聚类算法伪代码"></a>二分 K-Means 聚类算法伪代码</h4>
      <ul>
<li>将所有点看成一个簇</li>
<li>当簇数目小于 k 时</li>
<li>对于每一个簇<ul>
<li>计算总误差</li>
<li>在给定的簇上面进行 KMeans 聚类（k=2）</li>
<li>计算将该簇一分为二之后的总误差</li>
</ul>
</li>
<li>选择使得误差最小的那个簇进行划分操作</li>
</ul>
<p>另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。<br>接下来主要介绍该做法的python2代码实现</p>

        <h4 id="二分-K-Means-聚类算法代码"   >
          <a href="#二分-K-Means-聚类算法代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分-K-Means-聚类算法代码" class="headerlink" title="二分 K-Means 聚类算法代码"></a>二分 K-Means 聚类算法代码</h4>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分 KMeans 聚类算法, 基于 kMeans 基础之上的优化，以避免陷入局部最小值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKMeans</span>(<span class="params">dataSet, k, distMeas=distEclud</span>):</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment"># 保存每个数据点的簇分配结果和平方误差</span></span><br><span class="line">    centroid0 = mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>] <span class="comment"># 质心初始化为所有数据点的均值</span></span><br><span class="line">    centList =[centroid0] <span class="comment"># 初始化只有 1 个质心的 list</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m): <span class="comment"># 计算所有数据点到初始质心的距离平方误差</span></span><br><span class="line">        clusterAssment[j,<span class="number">1</span>] = distMeas(mat(centroid0), dataSet[j,:])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">len</span>(centList) &lt; k): <span class="comment"># 当质心数量小于 k 时</span></span><br><span class="line">        lowestSSE = inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(centList)): <span class="comment"># 对每一个质心</span></span><br><span class="line">            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:] <span class="comment"># 获取当前簇 i 下的所有数据点</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas) <span class="comment"># 将当前簇 i 进行二分 kMeans 处理</span></span><br><span class="line">            sseSplit = <span class="built_in">sum</span>(splitClustAss[:,<span class="number">1</span>]) <span class="comment"># 将二分 kMeans 结果中的平方和的距离进行求和</span></span><br><span class="line">            sseNotSplit = <span class="built_in">sum</span>(clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>]) <span class="comment"># 将未参与二分 kMeans 分配结果中的平方和的距离进行求和</span></span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;sseSplit, and notSplit: &quot;</span>,sseSplit,sseNotSplit</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE: <span class="comment"># 总的（未拆分和已拆分）误差和越小，越相似，效果越优化，划分的结果更好（注意: 这里的理解很重要，不明白的地方可以和我们一起讨论）</span></span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        <span class="comment"># 找出最好的簇分配结果    </span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>] = <span class="built_in">len</span>(centList) <span class="comment"># 调用二分 kMeans 的结果，默认簇是 0,1. 当然也可以改成其它的数字</span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>] = bestCentToSplit <span class="comment"># 更新为最佳质心</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;the bestCentToSplit is: &#x27;</span>,bestCentToSplit</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;the len of bestClustAss is: &#x27;</span>, <span class="built_in">len</span>(bestClustAss)</span><br><span class="line">        <span class="comment"># 更新质心列表</span></span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>,:].tolist()[<span class="number">0</span>] <span class="comment"># 更新原质心 list 中的第 i 个质心为使用二分 kMeans 后 bestNewCents 的第一个质心</span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>,:].tolist()[<span class="number">0</span>]) <span class="comment"># 添加 bestNewCents 的第二个质心</span></span><br><span class="line">        clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>],:]= bestClustAss <span class="comment"># 重新分配最好簇下的数据（质心）以及SSE</span></span><br><span class="line">    <span class="keyword">return</span> mat(centList), clusterAssment</span><br></pre></td></tr></table></div></figure>


        <h4 id="测试二分-KMeans-聚类算法"   >
          <a href="#测试二分-KMeans-聚类算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#测试二分-KMeans-聚类算法" class="headerlink" title="测试二分 KMeans 聚类算法"></a>测试二分 KMeans 聚类算法</h4>
      <ul>
<li>测试一下二分 KMeans 聚类算法，请看: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
<p>上述函数可以运行多次，聚类会收敛到全局最小值，而原始的 kMeans() 函数偶尔会陷入局部最小值。<br>运行参考结果如下:<br><img src="img/apachecn-bikmeans-run-result-1.jpg" alt="二分 K-Means 运行结果1"></p>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuxin" >那伊抹微笑</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>,  <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~xuzhaoqing" >清都江水郎</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong> </li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/09/ml_9/">第9章 树回归</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-09</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/TreeRegression_headPage_xy.png" alt="预测数值型数据回归首页" title="树回归首页"></p>

        <h2 id="树回归-概述"   >
          <a href="#树回归-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#树回归-概述" class="headerlink" title="树回归 概述"></a>树回归 概述</h2>
      <p><code>我们本章介绍 CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。</code></p>

        <h2 id="树回归-场景"   >
          <a href="#树回归-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#树回归-场景" class="headerlink" title="树回归 场景"></a>树回归 场景</h2>
      <p>我们在第 8 章中介绍了线性回归的一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。</p>
<p>一种可行的方法是将数据集切分成很多份易建模的数据，然后利用我们的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树回归和回归法就相当有用。</p>
<p>除了我们在 第3章 中介绍的 决策树算法，我们介绍一个新的叫做 CART(Classification And Regression Trees, 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。</p>

        <h2 id="1、树回归-原理"   >
          <a href="#1、树回归-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、树回归-原理" class="headerlink" title="1、树回归 原理"></a>1、树回归 原理</h2>
      
        <h3 id="1-1、树回归-原理概述"   >
          <a href="#1-1、树回归-原理概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1、树回归-原理概述" class="headerlink" title="1.1、树回归 原理概述"></a>1.1、树回归 原理概述</h3>
      <p>为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？</p>
<p>在这里，计算连续型数值的混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。</p>
<p>上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。总方差可以通过均方差乘以数据集中样本点的个数来得到。</p>

        <h3 id="1-2、树构建算法-比较"   >
          <a href="#1-2、树构建算法-比较" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2、树构建算法-比较" class="headerlink" title="1.2、树构建算法 比较"></a>1.2、树构建算法 比较</h3>
      <p>我们在 第3章 中使用的树构建算法是 ID3 。ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。</p>
<p>除了切分过于迅速外， ID3 算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。</p>
<p>CART 是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对 CART 稍作修改就可以处理回归问题。第 3 章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。</p>
<p>回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型。</p>

        <h4 id="1-2-1、附加-各常见树构造算法的划分分支方式"   >
          <a href="#1-2-1、附加-各常见树构造算法的划分分支方式" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2-1、附加-各常见树构造算法的划分分支方式" class="headerlink" title="1.2.1、附加 各常见树构造算法的划分分支方式"></a>1.2.1、附加 各常见树构造算法的划分分支方式</h4>
      <p>还有一点要说明，构建决策树算法，常用到的是三个方法: ID3, C4.5, CART.<br>三种方法区别是划分树的分支的方式:</p>
<ol>
<li>ID3 是信息增益分支</li>
<li>C4.5 是信息增益率分支</li>
<li>CART 做分类工作时，采用 GINI 值作为节点分裂的依据；回归时，采用样本的最小方差作为节点的分裂依据。</li>
</ol>
<p>工程上总的来说: </p>
<p>CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；</p>
<p>以此拓展出以 CART 为基础的 “树群” Random forest ， 以 回归树 为基础的 “树群” GBDT 。</p>

        <h3 id="1-3、树回归-工作原理"   >
          <a href="#1-3、树回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3、树回归-工作原理" class="headerlink" title="1.3、树回归 工作原理"></a>1.3、树回归 工作原理</h3>
      <p>1、找到数据集切分的最佳位置，函数 chooseBestSplit() 伪代码大致如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对每个特征:</span><br><span class="line">    对每个特征值: </span><br><span class="line">        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）</span><br><span class="line">        计算切分的误差</span><br><span class="line">        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差</span><br><span class="line">返回最佳切分的特征和阈值</span><br></pre></td></tr></table></div></figure>
<p>2、树构建算法，函数 createTree() 伪代码大致如下:   </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">找到最佳的待切分特征:</span><br><span class="line">    如果该节点不能再分，将该节点存为叶节点</span><br><span class="line">    执行二元切分</span><br><span class="line">    在右子树调用 createTree() 方法</span><br><span class="line">    在左子树调用 createTree() 方法</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-4、树回归-开发流程"   >
          <a href="#1-4、树回归-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-4、树回归-开发流程" class="headerlink" title="1.4、树回归 开发流程"></a>1.4、树回归 开发流程</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(1) 收集数据: 采用任意方法收集数据。</span><br><span class="line">(2) 准备数据: 需要数值型数据，标称型数据应该映射成二值型数据。</span><br><span class="line">(3) 分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树。</span><br><span class="line">(4) 训练算法: 大部分时间都花费在叶节点树模型的构建上。</span><br><span class="line">(5) 测试算法: 使用测试数据上的R^2值来分析模型的效果。</span><br><span class="line">(6) 使用算法: 使用训练处的树做预测，预测结果还可以用来做很多事情。</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-5、树回归-算法特点"   >
          <a href="#1-5、树回归-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5、树回归-算法特点" class="headerlink" title="1.5、树回归 算法特点"></a>1.5、树回归 算法特点</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 可以对复杂和非线性的数据建模。</span><br><span class="line">缺点: 结果不易理解。</span><br><span class="line">适用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-6、回归树-项目案例"   >
          <a href="#1-6、回归树-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-6、回归树-项目案例" class="headerlink" title="1.6、回归树 项目案例"></a>1.6、回归树 项目案例</h3>
      
        <h4 id="1-6-1、项目概述"   >
          <a href="#1-6-1、项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-6-1、项目概述" class="headerlink" title="1.6.1、项目概述"></a>1.6.1、项目概述</h4>
      <p>在简单数据集上生成一棵回归树。</p>

        <h4 id="1-6-2、开发流程"   >
          <a href="#1-6-2、开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-6-2、开发流程" class="headerlink" title="1.6.2、开发流程"></a>1.6.2、开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</span><br><span class="line">分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</span><br><span class="line">训练算法: 大部分时间都花费在叶节点树模型的构建上</span><br><span class="line">测试算法: 使用测试数据上的R^2值来分析模型的效果</span><br><span class="line">使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<p>data1.txt 文件中存储的数据格式如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.036098	0.155096</span><br><span class="line">0.993349	1.077553</span><br><span class="line">0.530897	0.893462</span><br><span class="line">0.712386	0.564858</span><br><span class="line">0.343554	-0.371700</span><br><span class="line">0.098016	-0.332760</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</p>
</blockquote>
<blockquote>
<p>分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</p>
</blockquote>
<p>基于 CART 算法构建回归树的简单数据集<br><img src="img/RegTree_1.png" alt="基于 CART 算法构建回归树的简单数据集">  </p>
<p>用于测试回归树的分段常数数据集<br><img src="img/RegTree_2.png" alt="用于测试回归树的分段常数数据集">  </p>
<blockquote>
<p>训练算法: 构造树的数据结构</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binSplitDataSet</span>(<span class="params">dataSet, feature, value</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;binSplitDataSet(将数据集，按照feature列的value进行 二元切分)</span></span><br><span class="line"><span class="string">        Description: 在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat 数据集</span></span><br><span class="line"><span class="string">        feature 待切分的特征列</span></span><br><span class="line"><span class="string">        value 特征列要比较的值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        mat0 小于等于 value 的数据集在左边</span></span><br><span class="line"><span class="string">        mat1 大于 value 的数据集在右边</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># # 测试案例</span></span><br><span class="line">    <span class="comment"># print &#x27;dataSet[:, feature]=&#x27;, dataSet[:, feature]</span></span><br><span class="line">    <span class="comment"># print &#x27;nonzero(dataSet[:, feature] &gt; value)[0]=&#x27;, nonzero(dataSet[:, feature] &gt; value)[0]</span></span><br><span class="line">    <span class="comment"># print &#x27;nonzero(dataSet[:, feature] &lt;= value)[0]=&#x27;, nonzero(dataSet[:, feature] &lt;= value)[0]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)</span></span><br><span class="line">    <span class="comment"># nonzero(dataSet[:, feature] &gt; value)  返回结果为true行的index下标</span></span><br><span class="line">    mat0 = dataSet[nonzero(dataSet[:, feature] &lt;= value)[<span class="number">0</span>], :]</span><br><span class="line">    mat1 = dataSet[nonzero(dataSet[:, feature] &gt; value)[<span class="number">0</span>], :]</span><br><span class="line">    <span class="keyword">return</span> mat0, mat1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.用最佳方式切分数据集</span></span><br><span class="line"><span class="comment"># 2.生成相应的叶节点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestSplit</span>(<span class="params">dataSet, leafType=regLeaf, errType=regErr, ops=(<span class="params"><span class="number">1</span>, <span class="number">4</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet   加载的原始数据集</span></span><br><span class="line"><span class="string">        leafType  建立叶子点的函数</span></span><br><span class="line"><span class="string">        errType   误差计算函数(求总方差)</span></span><br><span class="line"><span class="string">        ops       [容许误差下降值，切分的最少样本数]。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestIndex feature的index坐标</span></span><br><span class="line"><span class="string">        bestValue 切分的最优值</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。</span></span><br><span class="line">    <span class="comment"># 之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。</span></span><br><span class="line">    <span class="comment"># 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分</span></span><br><span class="line">    tolS = ops[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 划分最小 size 小于，就不继续划分了</span></span><br><span class="line">    tolN = ops[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 如果结果集(最后一列为1个变量)，就返回退出</span></span><br><span class="line">    <span class="comment"># .T 对数据集进行转置</span></span><br><span class="line">    <span class="comment"># .tolist()[0] 转化为数组并取第0列</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">set</span>(dataSet[:, -<span class="number">1</span>].T.tolist()[<span class="number">0</span>])) == <span class="number">1</span>: <span class="comment"># 如果集合size为1，不用继续划分。</span></span><br><span class="line">        <span class="comment">#  exit cond 1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    <span class="comment"># 计算行列值</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 无分类误差的总方差和</span></span><br><span class="line">    <span class="comment"># the choice of the best feature is driven by Reduction in RSS error from mean</span></span><br><span class="line">    S = errType(dataSet)</span><br><span class="line">    <span class="comment"># inf 正无穷大</span></span><br><span class="line">    bestS, bestIndex, bestValue = inf, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环处理每一列对应的feature值</span></span><br><span class="line">    <span class="keyword">for</span> featIndex <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>): <span class="comment"># 对于每个特征</span></span><br><span class="line">        <span class="comment"># [0]表示这一列的[所有行]，不要[0]就是一个array[[所有行]]</span></span><br><span class="line">        <span class="keyword">for</span> splitVal <span class="keyword">in</span> <span class="built_in">set</span>(dataSet[:, featIndex].T.tolist()[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># 对该列进行分组，然后组内的成员的val值进行 二元切分</span></span><br><span class="line">            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)</span><br><span class="line">            <span class="comment"># 判断二元切分的方式的元素数量是否符合预期</span></span><br><span class="line">            <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            newS = errType(mat0) + errType(mat1)</span><br><span class="line">            <span class="comment"># 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差</span></span><br><span class="line">            <span class="comment"># 如果划分后误差小于 bestS，则说明找到了新的bestS</span></span><br><span class="line">            <span class="keyword">if</span> newS &lt; bestS:</span><br><span class="line">                bestIndex = featIndex</span><br><span class="line">                bestValue = splitVal</span><br><span class="line">                bestS = newS</span><br><span class="line">    <span class="comment"># 判断二元切分的方式的元素误差是否符合预期</span></span><br><span class="line">    <span class="comment"># if the decrease (S-bestS) is less than a threshold don&#x27;t do the split</span></span><br><span class="line">    <span class="keyword">if</span> (S - bestS) &lt; tolS:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)</span><br><span class="line">    <span class="comment"># 对整体的成员进行判断，是否符合预期</span></span><br><span class="line">    <span class="comment"># 如果集合的 size 小于 tolN </span></span><br><span class="line">    <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN): <span class="comment"># 当最佳划分后，集合过小，也不划分，产生叶节点</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    <span class="keyword">return</span> bestIndex, bestValue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># assume dataSet is NumPy Mat so we can array filtering</span></span><br><span class="line"><span class="comment"># 假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span>(<span class="params">dataSet, leafType=regLeaf, errType=regErr, ops=(<span class="params"><span class="number">1</span>, <span class="number">4</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;createTree(获取回归树)</span></span><br><span class="line"><span class="string">        Description: 递归函数: 如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet      加载的原始数据集</span></span><br><span class="line"><span class="string">        leafType     建立叶子点的函数</span></span><br><span class="line"><span class="string">        errType      误差计算函数</span></span><br><span class="line"><span class="string">        ops=(1, 4)   [容许误差下降值，切分的最少样本数]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retTree    决策树最后的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 选择最好的切分方式:  feature索引值，最优切分值</span></span><br><span class="line">    <span class="comment"># choose the best split</span></span><br><span class="line">    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)</span><br><span class="line">    <span class="comment"># if the splitting hit a stop condition return val</span></span><br><span class="line">    <span class="comment"># 如果 splitting 达到一个停止条件，那么返回 val</span></span><br><span class="line">    <span class="keyword">if</span> feat <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    retTree = &#123;&#125;</span><br><span class="line">    retTree[<span class="string">&#x27;spInd&#x27;</span>] = feat</span><br><span class="line">    retTree[<span class="string">&#x27;spVal&#x27;</span>] = val</span><br><span class="line">    <span class="comment"># 大于在右边，小于在左边，分为2个数据集</span></span><br><span class="line">    lSet, rSet = binSplitDataSet(dataSet, feat, val)</span><br><span class="line">    <span class="comment"># 递归的进行调用，在左右子树中继续递归生成树</span></span><br><span class="line">    retTree[<span class="string">&#x27;left&#x27;</span>] = createTree(lSet, leafType, errType, ops)</span><br><span class="line">    retTree[<span class="string">&#x27;right&#x27;</span>] = createTree(rSet, leafType, errType, ops)</span><br><span class="line">    <span class="keyword">return</span> retTree</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<blockquote>
<p>测试算法: 使用测试数据上的R^2值来分析模型的效果</p>
</blockquote>
<blockquote>
<p>使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</p>
</blockquote>

        <h2 id="2、树剪枝"   >
          <a href="#2、树剪枝" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、树剪枝" class="headerlink" title="2、树剪枝"></a>2、树剪枝</h2>
      <p>一棵树如果节点过多，表明该模型可能对数据进行了 “过拟合”。</p>
<p>通过降低决策树的复杂度来避免过拟合的过程称为 <code>剪枝（pruning）</code>。在函数 chooseBestSplit() 中提前终止条件，实际上是在进行一种所谓的 <code>预剪枝（prepruning）</code>操作。另一个形式的剪枝需要使用测试集和训练集，称作 <code>后剪枝（postpruning）</code>。</p>

        <h3 id="2-1、预剪枝-prepruning"   >
          <a href="#2-1、预剪枝-prepruning" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1、预剪枝-prepruning" class="headerlink" title="2.1、预剪枝(prepruning)"></a>2.1、预剪枝(prepruning)</h3>
      <p>顾名思义，预剪枝就是及早的停止树增长，在构造决策树的同时进行剪枝。</p>
<p>所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。</p>

        <h3 id="2-2、后剪枝-postpruning"   >
          <a href="#2-2、后剪枝-postpruning" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2、后剪枝-postpruning" class="headerlink" title="2.2、后剪枝(postpruning)"></a>2.2、后剪枝(postpruning)</h3>
      <p>决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。合并也被称作 <code>塌陷处理</code> ，在回归树中一般采用取需要合并的所有子树的平均值。后剪枝是目前最普遍的做法。</p>
<p>后剪枝 prune() 的伪代码如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于已有的树切分测试数据:</span><br><span class="line">    如果存在任一子集是一棵树，则在该子集递归剪枝过程</span><br><span class="line">    计算将当前两个叶节点合并后的误差</span><br><span class="line">    计算不合并的误差</span><br><span class="line">    如果合并会降低误差的话，就将叶节点合并</span><br></pre></td></tr></table></div></figure>


        <h3 id="2-3、剪枝-代码"   >
          <a href="#2-3、剪枝-代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3、剪枝-代码" class="headerlink" title="2.3、剪枝 代码"></a>2.3、剪枝 代码</h3>
      <p>回归树剪枝函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断节点是否是一个字典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isTree</span>(<span class="params">obj</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        测试输入变量是否是一棵树,即是否是一个字典</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        obj -- 输入变量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回布尔类型的结果。如果 obj 是一个字典，返回true，否则返回 false</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="built_in">type</span>(obj).__name__ == <span class="string">&#x27;dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算左右枝丫的均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMean</span>(<span class="params">tree</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        从上往下遍历树直到叶节点为止，如果找到两个叶节点则计算它们的平均值。</span></span><br><span class="line"><span class="string">        对 tree 进行塌陷处理，即返回树平均值。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 输入的树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回 tree 节点的平均值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;right&#x27;</span>] = getMean(tree[<span class="string">&#x27;right&#x27;</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;left&#x27;</span>] = getMean(tree[<span class="string">&#x27;left&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> (tree[<span class="string">&#x27;left&#x27;</span>]+tree[<span class="string">&#x27;right&#x27;</span>])/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否适合合并分枝</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span>(<span class="params">tree, testData</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        从上而下找到叶节点，用测试数据集来判断将这些叶节点合并是否能降低测试误差</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 待剪枝的树</span></span><br><span class="line"><span class="string">        testData -- 剪枝所需要的测试数据 testData </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tree -- 剪枝完成的树</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值</span></span><br><span class="line">    <span class="keyword">if</span> shape(testData)[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> getMean(tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断分枝是否是dict字典，如果是就将测试数据集进行切分</span></span><br><span class="line">    <span class="keyword">if</span> (isTree(tree[<span class="string">&#x27;right&#x27;</span>]) <span class="keyword">or</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>])):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">&#x27;spInd&#x27;</span>], tree[<span class="string">&#x27;spVal&#x27;</span>])</span><br><span class="line">    <span class="comment"># 如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;left&#x27;</span>] = prune(tree[<span class="string">&#x27;left&#x27;</span>], lSet)</span><br><span class="line">    <span class="comment"># 如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        tree[<span class="string">&#x27;right&#x27;</span>] = prune(tree[<span class="string">&#x27;right&#x27;</span>], rSet)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。</span></span><br><span class="line">    <span class="comment"># 1. 如果正确 </span></span><br><span class="line">    <span class="comment">#   * 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较</span></span><br><span class="line">    <span class="comment">#   * 如果 合并的总方差 &lt; 不合并的总方差，那么就进行合并</span></span><br><span class="line">    <span class="comment"># 注意返回的结果:  如果可以合并，原来的dict就变为了 数值</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]) <span class="keyword">and</span> <span class="keyword">not</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">&#x27;spInd&#x27;</span>], tree[<span class="string">&#x27;spVal&#x27;</span>])</span><br><span class="line">        <span class="comment"># power(x, y)表示x的y次方</span></span><br><span class="line">        errorNoMerge = <span class="built_in">sum</span>(power(lSet[:, -<span class="number">1</span>] - tree[<span class="string">&#x27;left&#x27;</span>], <span class="number">2</span>)) + <span class="built_in">sum</span>(power(rSet[:, -<span class="number">1</span>] - tree[<span class="string">&#x27;right&#x27;</span>], <span class="number">2</span>))</span><br><span class="line">        treeMean = (tree[<span class="string">&#x27;left&#x27;</span>] + tree[<span class="string">&#x27;right&#x27;</span>])/<span class="number">2.0</span></span><br><span class="line">        errorMerge = <span class="built_in">sum</span>(power(testData[:, -<span class="number">1</span>] - treeMean, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果 合并的总方差 &lt; 不合并的总方差，那么就进行合并</span></span><br><span class="line">        <span class="keyword">if</span> errorMerge &lt; errorNoMerge:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;merging&quot;</span></span><br><span class="line">            <span class="keyword">return</span> treeMean</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> tree</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tree</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="3、模型树"   >
          <a href="#3、模型树" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、模型树" class="headerlink" title="3、模型树"></a>3、模型树</h2>
      
        <h3 id="3-1、模型树-简介"   >
          <a href="#3-1、模型树-简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1、模型树-简介" class="headerlink" title="3.1、模型树 简介"></a>3.1、模型树 简介</h3>
      <p>用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的 <code>分段线性（piecewise linear）</code> 是指模型由多个线性片段组成。</p>
<p>我们看一下图 9-4 中的数据，如果使用两条直线拟合是否比使用一组常数来建模好呢？答案显而易见。可以设计两条分别从 0.0<del>0.3、从 0.3</del>1.0 的直线，于是就可以得到两个线性模型。因为数据集里的一部分数据（0.0<del>0.3）以某个线性模型建模，而另一部分数据（0.3</del>1.0）则以另一个线性模型建模，因此我们说采用了所谓的分段线性模型。</p>
<p>决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。</p>
<p><img src="img/RegTree_3.png" alt="分段线性数据"></p>
<p>将之前的回归树的代码稍作修改，就可以在叶节点生成线性模型而不是常数值。下面将利用树生成算法对数据进行划分，且每份切分数据都能很容易被线性模型所表示。这个算法的关键在于误差的计算。</p>
<p>那么为了找到最佳切分，应该怎样计算误差呢？前面用于回归树的误差计算方法这里不能再用。稍加变化，对于给定的数据集，应该先用模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到了所需的误差。</p>

        <h3 id="3-2、模型树-代码"   >
          <a href="#3-2、模型树-代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2、模型树-代码" class="headerlink" title="3.2、模型树 代码"></a>3.2、模型树 代码</h3>
      <p>模型树的叶节点生成函数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到模型的ws系数: f(x) = x0 + x1*featrue1+ x3*featrue2 ...</span></span><br><span class="line"><span class="comment"># create linear model and return coeficients</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelLeaf</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        当数据不再需要切分的时候，生成叶节点的模型。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        调用 linearSolve 函数，返回得到的 回归系数ws</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算线性模型的误差值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelErr</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        在给定数据集上计算误差。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        调用 linearSolve 函数，返回 yHat 和 Y 之间的平方误差。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    yHat = X * ws</span><br><span class="line">    <span class="comment"># print corrcoef(yHat, Y, rowvar=0)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(power(Y - yHat, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment"># helper function used in two places</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linearSolve</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        将数据集格式化成目标变量Y和自变量X，执行简单的线性回归，得到ws</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ws -- 执行线性回归的回归系数 </span></span><br><span class="line"><span class="string">        X -- 格式化自变量X</span></span><br><span class="line"><span class="string">        Y -- 格式化目标变量Y</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 产生一个关于1的矩阵</span></span><br><span class="line">    X = mat(ones((m, n)))</span><br><span class="line">    Y = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># X的0列为1，常数项，用于计算平衡误差</span></span><br><span class="line">    X[:, <span class="number">1</span>: n] = dataSet[:, <span class="number">0</span>: n-<span class="number">1</span>]</span><br><span class="line">    Y = dataSet[:, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转置矩阵*矩阵</span></span><br><span class="line">    xTx = X.T * X</span><br><span class="line">    <span class="comment"># 如果矩阵的逆不存在，会造成程序异常</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">&#x27;This matrix is singular, cannot do inverse,\ntry increasing the second value of ops&#x27;</span>)</span><br><span class="line">    <span class="comment"># 最小二乘法求最优解:  w0*1+w1*x1=y</span></span><br><span class="line">    ws = xTx.I * (X.T * Y)</span><br><span class="line">    <span class="keyword">return</span> ws, X, Y</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="3-3、模型树-运行结果"   >
          <a href="#3-3、模型树-运行结果" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-3、模型树-运行结果" class="headerlink" title="3.3、模型树 运行结果"></a>3.3、模型树 运行结果</h3>
      <p><img src="img/RegTree_4.png" alt="模型树运行结果"></p>

        <h2 id="4、树回归-项目案例"   >
          <a href="#4、树回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、树回归-项目案例" class="headerlink" title="4、树回归 项目案例"></a>4、树回归 项目案例</h2>
      
        <h3 id="4-1、项目案例1-树回归与标准回归的比较"   >
          <a href="#4-1、项目案例1-树回归与标准回归的比较" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1、项目案例1-树回归与标准回归的比较" class="headerlink" title="4.1、项目案例1: 树回归与标准回归的比较"></a>4.1、项目案例1: 树回归与标准回归的比较</h3>
      
        <h4 id="4-1-1、项目概述"   >
          <a href="#4-1-1、项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-1、项目概述" class="headerlink" title="4.1.1、项目概述"></a>4.1.1、项目概述</h4>
      <p>前面介绍了模型树、回归树和一般的回归方法，下面测试一下哪个模型最好。</p>
<p>这些模型将在某个数据上进行测试，该数据涉及人的智力水平和自行车的速度的关系。当然，数据是假的。</p>

        <h4 id="4-1-2、开发流程"   >
          <a href="#4-1-2、开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-2、开发流程" class="headerlink" title="4.1.2、开发流程"></a>4.1.2、开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</span><br><span class="line">分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</span><br><span class="line">训练算法: 模型树的构建</span><br><span class="line">测试算法: 使用测试数据上的R^2值来分析模型的效果</span><br><span class="line">使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<blockquote>
<p>准备数据: 需要数值型数据，标称型数据应该映射成二值型数据</p>
</blockquote>
<p>数据存储格式:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3.000000	46.852122</span><br><span class="line">23.000000	178.676107</span><br><span class="line">0.000000	86.154024</span><br><span class="line">6.000000	68.707614</span><br><span class="line">15.000000	139.737693</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树</p>
</blockquote>
<p><img src="img/RegTree_5.png" alt="骑自行车速度和智商之间的关系"></p>
<blockquote>
<p>训练算法: 模型树的构建</p>
</blockquote>
<p>用树回归进行预测的代码</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回归树测试案例</span></span><br><span class="line"><span class="comment"># 为了和 modelTreeEval() 保持一致，保留两个输入参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regTreeEval</span>(<span class="params">model, inDat</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对 回归树 进行预测</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- 指定模型，可选值为 回归树模型 或者 模型树模型，这里为回归树</span></span><br><span class="line"><span class="string">        inDat -- 输入的测试数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float(model) -- 将输入的模型数据转换为 浮点数 返回</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型树测试案例</span></span><br><span class="line"><span class="comment"># 对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，</span></span><br><span class="line"><span class="comment"># 也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelTreeEval</span>(<span class="params">model, inDat</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对 模型树 进行预测</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- 输入模型，可选值为 回归树模型 或者 模型树模型，这里为模型树模型</span></span><br><span class="line"><span class="string">        inDat -- 输入的测试数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float(X * model) -- 将测试数据乘以 回归系数 得到一个预测值 ，转化为 浮点数 返回</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = shape(inDat)[<span class="number">1</span>]</span><br><span class="line">    X = mat(ones((<span class="number">1</span>, n+<span class="number">1</span>)))</span><br><span class="line">    X[:, <span class="number">1</span>: n+<span class="number">1</span>] = inDat</span><br><span class="line">    <span class="comment"># print X, model</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(X * model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测的结果</span></span><br><span class="line"><span class="comment"># 在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。</span></span><br><span class="line"><span class="comment"># modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。</span></span><br><span class="line"><span class="comment"># 此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上</span></span><br><span class="line"><span class="comment"># 调用modelEval()函数，该函数的默认值为regTreeEval()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeForeCast</span>(<span class="params">tree, inData, modelEval=regTreeEval</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对特定模型的树进行预测，可以是 回归树 也可以是 模型树</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 已经训练好的树的模型</span></span><br><span class="line"><span class="string">        inData -- 输入的测试数据</span></span><br><span class="line"><span class="string">        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回预测值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree):</span><br><span class="line">        <span class="keyword">return</span> modelEval(tree, inData)</span><br><span class="line">    <span class="keyword">if</span> inData[tree[<span class="string">&#x27;spInd&#x27;</span>]] &lt;= tree[<span class="string">&#x27;spVal&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;left&#x27;</span>]):</span><br><span class="line">            <span class="keyword">return</span> treeForeCast(tree[<span class="string">&#x27;left&#x27;</span>], inData, modelEval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> modelEval(tree[<span class="string">&#x27;left&#x27;</span>], inData)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> isTree(tree[<span class="string">&#x27;right&#x27;</span>]):</span><br><span class="line">            <span class="keyword">return</span> treeForeCast(tree[<span class="string">&#x27;right&#x27;</span>], inData, modelEval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> modelEval(tree[<span class="string">&#x27;right&#x27;</span>], inData)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createForeCast</span>(<span class="params">tree, testData, modelEval=regTreeEval</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 已经训练好的树的模型</span></span><br><span class="line"><span class="string">        inData -- 输入的测试数据</span></span><br><span class="line"><span class="string">        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回预测值矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="built_in">len</span>(testData)</span><br><span class="line">    yHat = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># print yHat</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        yHat[i, <span class="number">0</span>] = treeForeCast(tree, mat(testData[i]), modelEval)</span><br><span class="line">        <span class="comment"># print &quot;yHat==&gt;&quot;, yHat[i, 0]</span></span><br><span class="line">    <span class="keyword">return</span> yHat</span><br></pre></td></tr></table></div></figure>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<blockquote>
<p>测试算法: 使用测试数据上的R^2值来分析模型的效果</p>
</blockquote>
<p>R^2 判定系数就是拟合优度判定系数，它体现了回归模型中自变量的变异在因变量的变异中所占的比例。如 R^2=0.99999 表示在因变量 y 的变异中有 99.999% 是由于变量 x 引起。当 R^2=1 时表示，所有观测点都落在拟合的直线或曲线上；当 R^2=0 时，表示自变量与因变量不存在直线或曲线关系。</p>
<p>所以我们看出， R^2 的值越接近 1.0 越好。</p>
<blockquote>
<p>使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情</p>
</blockquote>

        <h2 id="5、附加-Python-中-GUI-的使用"   >
          <a href="#5、附加-Python-中-GUI-的使用" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、附加-Python-中-GUI-的使用" class="headerlink" title="5、附加 Python 中 GUI 的使用"></a>5、附加 Python 中 GUI 的使用</h2>
      
        <h3 id="5-1、使用-Python-的-Tkinter-库创建-GUI"   >
          <a href="#5-1、使用-Python-的-Tkinter-库创建-GUI" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-1、使用-Python-的-Tkinter-库创建-GUI" class="headerlink" title="5.1、使用 Python 的 Tkinter 库创建 GUI"></a>5.1、使用 Python 的 Tkinter 库创建 GUI</h3>
      <p>如果能让用户不需要任何指令就可以按照他们自己的方式来分析数据，就不需要对数据做出过多解释。其中一个能同时支持数据呈现和用户交互的方式就是构建一个图形用户界面(GUI，Graphical User Interface)，如图9-7所示。</p>
<p><img src="img/GUI%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="GUI示例图" title="GUI示例图"></p>

        <h3 id="5-2、用-Tkinter-创建-GUI"   >
          <a href="#5-2、用-Tkinter-创建-GUI" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-2、用-Tkinter-创建-GUI" class="headerlink" title="5.2、用 Tkinter 创建 GUI"></a>5.2、用 Tkinter 创建 GUI</h3>
      <p>Python 有很多 GUI 框架，其中一个易于使用的 Tkinter，是随 Python 的标准版编译版本发布的。Tkinter 可以在 Windows、Mac OS和大多数的 Linux 平台上使用。</p>

        <h3 id="5-3、集成-Matplotlib-和-Tkinter"   >
          <a href="#5-3、集成-Matplotlib-和-Tkinter" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-3、集成-Matplotlib-和-Tkinter" class="headerlink" title="5.3、集成 Matplotlib 和 Tkinter"></a>5.3、集成 Matplotlib 和 Tkinter</h3>
      <p>MatPlotlib 的构建程序包含一个前端，也就是面向用户的一些代码，如 plot() 和 scatter() 方法等。事实上，它同时创建了一个后端，用于实现绘图和不同应用之间接口。</p>
<p>通过改变后端可以将图像绘制在PNG、PDF、SVG等格式的文件上。下面将设置后端为 TkAgg (Agg 是一个 C++ 的库，可以从图像创建光栅图)。TkAgg可以在所选GUI框架上调用Agg，把 Agg 呈现在画布上。我们可以在Tk的GUI上放置一个画布，并用 .grid()来调整布局。</p>

        <h3 id="5-4、用treeExplore-的GUI构建的模型树示例图"   >
          <a href="#5-4、用treeExplore-的GUI构建的模型树示例图" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-4、用treeExplore-的GUI构建的模型树示例图" class="headerlink" title="5.4、用treeExplore 的GUI构建的模型树示例图"></a>5.4、用treeExplore 的GUI构建的模型树示例图</h3>
      <p><img src="img/GUI%E6%9B%B4%E5%A5%BD%E7%9A%84%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="取得更好预测效果的GUI示例图" title="取得更好预测效果的GUI示例图"></p>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="6、树回归-小结"   >
          <a href="#6、树回归-小结" class="heading-link"><i class="fas fa-link"></i></a><a href="#6、树回归-小结" class="headerlink" title="6、树回归 小结"></a>6、树回归 小结</h2>
      <p>数据集中经常包含一些复杂的相关关系，使得输入数据和目标变量之间呈现非线性关系。对这些复杂的关系建模，一种可行的方式是使用树来对预测值分段，包括分段常数或分段直线。一般采用树结构来对这种数据建模。相应地，若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型师线性回归方程则称为模型树。</p>
<p>CART 算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART 算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。一棵过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝（在树的构建过程中就进行剪枝）和后剪枝（当树构建完毕再进行剪枝），预剪枝更有效但需要用户定义一些参数。</p>
<p>Tkinter 是 Python 的一个 GUI 工具包。虽然并不是唯一的包，但它最常用。利用 Tkinter ，我们可以轻轻松松绘制各种部件并安排它们的位置。另外，可以为 Tkinter 构造一个特殊的部件来显示 Matplotlib 绘出的图。所以，Matplotlib 和 Tkinter 的集成可以构建出更强大的 GUI ，用户可以以更自然的方式来探索机器学习算法的奥妙。</p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/08/ml_8/">第8章 预测数值型数据-回归</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-08</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><img src="img/%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9B%9E%E5%BD%92%E9%A6%96%E9%A1%B5.png" alt="预测数值型数据回归首页" title="回归Regression首页"></p>

        <h2 id="回归（Regression）-概述"   >
          <a href="#回归（Regression）-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归（Regression）-概述" class="headerlink" title="回归（Regression） 概述"></a>回归（Regression） 概述</h2>
      <p><code>我们前边提到的分类的目标变量是标称型数据，而回归则是对连续型的数据做出处理，回归的目的是预测数值型数据的目标值。</code></p>

        <h2 id="回归-场景"   >
          <a href="#回归-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归-场景" class="headerlink" title="回归 场景"></a>回归 场景</h2>
      <p>回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。</p>
<p>假如你想要预测兰博基尼跑车的功率大小，可能会这样计算:</p>
<p>HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio</p>
<p>这就是所谓的 <code>回归方程(regression equation)</code>，其中的 0.0015 和 -0.99 称作 <code>回归系数（regression weights）</code>，求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。我们这里所说的，回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。</p>
<p>说到回归，一般都是指 <code>线性回归(linear regression)</code>。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。</p>
<p>补充:<br>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。</p>

        <h2 id="回归-原理"   >
          <a href="#回归-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#回归-原理" class="headerlink" title="回归 原理"></a>回归 原理</h2>
      
        <h3 id="1、线性回归"   >
          <a href="#1、线性回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、线性回归" class="headerlink" title="1、线性回归"></a>1、线性回归</h3>
      <p>我们应该怎样从一大堆数据里求出回归方程呢？ 假定输入数据存放在矩阵 x 中，而回归系数存放在向量 w 中。那么对于给定的数据 X1，预测结果将会通过 Y = X1^T w 给出。现在的问题是，手里有一些 X 和对应的 y，怎样才能找到 w 呢？一个常用的方法就是找出使误差最小的 w 。这里的误差是指预测 y 值和真实 y 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用平方误差（实际上就是我们通常所说的最小二乘法）。</p>
<p>平方误差可以写做（其实我们是使用这个函数作为 loss function）: </p>
<p><img src="img/LinearR_18.png" alt="平方误差"></p>
<p>用矩阵表示还可以写做 <img src="img/LinearR_19.png" alt="平方误差_2"> 。如果对 w 求导，得到 <img src="img/LinearR_20.png" alt="平方误差_3"> ，令其等于零，解出 w 如下（具体求导过程为: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://blog.csdn.net/nomadlx53/article/details/50849941" >http://blog.csdn.net/nomadlx53/article/details/50849941</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> ）:</p>
<p><img src="img/LinearR_1.png" alt="回归系数的最佳估计计算公式"></p>

        <h4 id="1-1、线性回归-须知概念"   >
          <a href="#1-1、线性回归-须知概念" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1、线性回归-须知概念" class="headerlink" title="1.1、线性回归 须知概念"></a>1.1、线性回归 须知概念</h4>
      
        <h5 id="1-1-1、矩阵求逆"   >
          <a href="#1-1-1、矩阵求逆" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-1、矩阵求逆" class="headerlink" title="1.1.1、矩阵求逆"></a>1.1.1、矩阵求逆</h5>
      <p>因为我们在计算回归方程的回归系数时，用到的计算公式如下: </p>
<p><img src="img/LinearR_1.png" alt="回归系数的最佳估计计算公式"></p>
<p>需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用，我们在程序代码中对此作出判断。<br>判断矩阵是否可逆的一个可选方案是: </p>
<p>判断矩阵的行列式是否为 0，若为 0 ，矩阵就不存在逆矩阵，不为 0 的话，矩阵才存在逆矩阵。</p>

        <h5 id="1-1-2、最小二乘法"   >
          <a href="#1-1-2、最小二乘法" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-1-2、最小二乘法" class="headerlink" title="1.1.2、最小二乘法"></a>1.1.2、最小二乘法</h5>
      <p>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。</p>

        <h4 id="1-2、线性回归-工作原理"   >
          <a href="#1-2、线性回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-2、线性回归-工作原理" class="headerlink" title="1.2、线性回归 工作原理"></a>1.2、线性回归 工作原理</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">读入数据，将数据特征x、特征标签y存储在矩阵x、y中</span><br><span class="line">验证 x^Tx 矩阵是否可逆</span><br><span class="line">使用最小二乘法求得 回归系数 w 的最佳估计</span><br></pre></td></tr></table></div></figure>


        <h4 id="1-3、线性回归-开发流程"   >
          <a href="#1-3、线性回归-开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-3、线性回归-开发流程" class="headerlink" title="1.3、线性回归 开发流程"></a>1.3、线性回归 开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</span><br><span class="line">分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</span><br><span class="line">训练算法: 找到回归系数</span><br><span class="line">测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果</span><br><span class="line">使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</span><br></pre></td></tr></table></div></figure>


        <h4 id="1-4、线性回归-算法特点"   >
          <a href="#1-4、线性回归-算法特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-4、线性回归-算法特点" class="headerlink" title="1.4、线性回归 算法特点"></a>1.4、线性回归 算法特点</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 结果易于理解，计算上不复杂。</span><br><span class="line">缺点: 对非线性的数据拟合不好。</span><br><span class="line">适用于数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h4 id="1-5、线性回归-项目案例"   >
          <a href="#1-5、线性回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5、线性回归-项目案例" class="headerlink" title="1.5、线性回归 项目案例"></a>1.5、线性回归 项目案例</h4>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="1-5-1、线性回归-项目概述"   >
          <a href="#1-5-1、线性回归-项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5-1、线性回归-项目概述" class="headerlink" title="1.5.1、线性回归 项目概述"></a>1.5.1、线性回归 项目概述</h5>
      <p>根据下图中的点，找出该数据的最佳拟合直线。</p>
<p><img src="img/LinearR_2.png" alt="线性回归数据示例图" title="线性回归数据示例图"></p>
<p>数据格式为: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x0          x1          y </span><br><span class="line">1.000000	0.067732	3.176513</span><br><span class="line">1.000000	0.427810	3.816464</span><br><span class="line">1.000000	0.995731	4.550095</span><br><span class="line">1.000000	0.738336	4.256571</span><br></pre></td></tr></table></div></figure>


        <h5 id="1-5-2、线性回归-编写代码"   >
          <a href="#1-5-2、线性回归-编写代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5-2、线性回归-编写代码" class="headerlink" title="1.5.2、线性回归 编写代码"></a>1.5.2、线性回归 编写代码</h5>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span>                 </span><br><span class="line">    <span class="string">&quot;&quot;&quot; 加载数据</span></span><br><span class="line"><span class="string">        解析以tab键分隔的文件中的浮点数</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        dataMat :   feature 对应的数据集</span></span><br><span class="line"><span class="string">        labelMat :  feature 对应的分类标签，即类别标签</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取样本特征的总数，不算最后的目标变量 </span></span><br><span class="line">    numFeat = <span class="built_in">len</span>(<span class="built_in">open</span>(fileName).readline().split(<span class="string">&#x27;\t&#x27;</span>)) - <span class="number">1</span> </span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        lineArr =[]</span><br><span class="line">        <span class="comment"># 删除一行中以tab分隔的数据前后的空白符号</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># i 从0到2，不包括2 </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeat):</span><br><span class="line">            <span class="comment"># 将数据添加到lineArr List中，每一行数据测试数据组成一个行向量           </span></span><br><span class="line">            lineArr.append(<span class="built_in">float</span>(curLine[i]))</span><br><span class="line">            <span class="comment"># 将测试数据的输入数据部分存储到dataMat 的List中</span></span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        <span class="comment"># 将每一行的最后一个数据，即类别，或者叫目标变量存储到labelMat List中</span></span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(curLine[-<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegres</span>(<span class="params">xArr,yArr</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Description: </span></span><br><span class="line"><span class="string">        线性回归</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        xArr : 输入的样本数据，包含每个样本数据的 feature</span></span><br><span class="line"><span class="string">        yArr : 对应于输入数据的类别标签，也就是每个样本对应的目标变量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ws: 回归系数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># mat()函数将xArr，yArr转换为矩阵 mat().T 代表的是对矩阵进行转置操作</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 矩阵乘法的条件是左矩阵的列数等于右矩阵的行数</span></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="comment"># 因为要用到xTx的逆矩阵，所以事先需要确定计算得到的xTx是否可逆，条件是矩阵的行列式不为0</span></span><br><span class="line">    <span class="comment"># linalg.det() 函数是用来求得矩阵的行列式的，如果矩阵的行列式为0，则这个矩阵是不可逆的，就无法进行接下来的运算                   </span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span> </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 最小二乘法</span></span><br><span class="line">    <span class="comment"># http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133</span></span><br><span class="line">    <span class="comment"># 书中的公式，求得w的最优解</span></span><br><span class="line">    ws = xTx.I * (xMat.T*yMat)            </span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression1</span>():</span></span><br><span class="line">    xArr, yArr = loadDataSet(<span class="string">&quot;data/8.Regression/data.txt&quot;</span>)</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr)</span><br><span class="line">    ws = standRegres(xArr, yArr)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)               <span class="comment">#add_subplot(349)函数的参数的意思是，将画布分成3行4列图像画在从左到右从上到下第9块</span></span><br><span class="line">    ax.scatter(xMat[:, <span class="number">1</span>].flatten(), yMat.T[:, <span class="number">0</span>].flatten().A[<span class="number">0</span>]) <span class="comment">#scatter 的x是xMat中的第二列，y是yMat的第一列</span></span><br><span class="line">    xCopy = xMat.copy() </span><br><span class="line">    xCopy.sort(<span class="number">0</span>)</span><br><span class="line">    yHat = xCopy * ws</span><br><span class="line">    ax.plot(xCopy[:, <span class="number">1</span>], yHat)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>



        <h5 id="1-5-3、线性回归-拟合效果"   >
          <a href="#1-5-3、线性回归-拟合效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-5-3、线性回归-拟合效果" class="headerlink" title="1.5.3、线性回归 拟合效果"></a>1.5.3、线性回归 拟合效果</h5>
      <p><img src="img/LinearR_3.png" alt="线性回归数据效果图" title="线性回归数据效果图"></p>

        <h3 id="2、局部加权线性回归"   >
          <a href="#2、局部加权线性回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、局部加权线性回归" class="headerlink" title="2、局部加权线性回归"></a>2、局部加权线性回归</h3>
      <p>线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。</p>
<p>一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数大致为:</p>
<p><img src="img/LinearR_21.png" alt="局部加权线性回归回归系数公式"></p>
<p>目标函数中 w 为权重，不是回归系数。与 kNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数 w 的形式如下: </p>
<p><img src="img/LinearR_4.png" alt="局部加权线性回归回归系数公式"></p>
<p>其中 W 是一个矩阵，用来给每个数据点赋予权重。$\hat{w}$ 则为回归系数。 这两个是不同的概念，请勿混用。</p>
<p>LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下: </p>
<p><img src="img/LinearR_23.png" alt="局部加权线性回归高斯核"></p>
<p>这样就构建了一个只含对角元素的权重矩阵 <strong>w</strong>，并且点 x 与 x(i) 越近，w(i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系。</p>
<p><img src="img/LinearR_6.png" alt="参数k与权重的关系"></p>
<p>上面的图是 每个点的权重图（假定我们正预测的点是 x = 0.5），最上面的图是原始数据集，第二个图显示了当 k = 0.5 时，大部分的数据都用于训练回归模型；而最下面的图显示当 k=0.01 时，仅有很少的局部点被用于训练回归模型。</p>

        <h4 id="2-1、局部加权线性回归-工作原理"   >
          <a href="#2-1、局部加权线性回归-工作原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1、局部加权线性回归-工作原理" class="headerlink" title="2.1、局部加权线性回归 工作原理"></a>2.1、局部加权线性回归 工作原理</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">读入数据，将数据特征x、特征标签y存储在矩阵x、y中</span><br><span class="line">利用高斯核构造一个权重矩阵 W，对预测点附近的点施加权重</span><br><span class="line">验证 X^TWX 矩阵是否可逆</span><br><span class="line">使用最小二乘法求得 回归系数 w 的最佳估计</span><br></pre></td></tr></table></div></figure>


        <h4 id="2-2、局部加权线性回归-项目案例"   >
          <a href="#2-2、局部加权线性回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2、局部加权线性回归-项目案例" class="headerlink" title="2.2、局部加权线性回归 项目案例"></a>2.2、局部加权线性回归 项目案例</h4>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="2-2-1、局部加权线性回归-项目概述"   >
          <a href="#2-2-1、局部加权线性回归-项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-1、局部加权线性回归-项目概述" class="headerlink" title="2.2.1、局部加权线性回归 项目概述"></a>2.2.1、局部加权线性回归 项目概述</h5>
      <p>我们仍然使用上面 线性回归 的数据集，对这些点进行一个 局部加权线性回归 的拟合。</p>
<p><img src="img/LinearR_2.png" alt="局部加权线性回归数据示例图"></p>
<p>数据格式为: </p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.000000	0.067732	3.176513</span><br><span class="line">1.000000	0.427810	3.816464</span><br><span class="line">1.000000	0.995731	4.550095</span><br><span class="line">1.000000	0.738336	4.256571</span><br></pre></td></tr></table></div></figure>


        <h5 id="2-2-2、局部加权线性回归-编写代码"   >
          <a href="#2-2-2、局部加权线性回归-编写代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-2、局部加权线性回归-编写代码" class="headerlink" title="2.2.2、局部加权线性回归 编写代码"></a>2.2.2、局部加权线性回归 编写代码</h5>
      <figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 局部加权线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span>(<span class="params">testPoint,xArr,yArr,k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Description: </span></span><br><span class="line"><span class="string">            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            testPoint: 样本点</span></span><br><span class="line"><span class="string">            xArr: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr: 每个样本对应的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            testPoint * ws: 数据点与具有权重的系数相乘得到的预测点</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)</span></span><br><span class="line"><span class="string">            理解: x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。</span></span><br><span class="line"><span class="string">            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。</span></span><br><span class="line"><span class="string">            算法思路: 假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，</span></span><br><span class="line"><span class="string">            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># mat() 函数是将array转换为矩阵的函数， mat().T 是转换为矩阵之后，再进行转置操作</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 获得xMat矩阵的行数</span></span><br><span class="line">    m = shape(xMat)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重                   </span></span><br><span class="line">    weights = mat(eye((m)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="comment"># testPoint 的形式是 一个行向量的形式</span></span><br><span class="line">        <span class="comment"># 计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值</span></span><br><span class="line">        diffMat = testPoint - xMat[j,:]</span><br><span class="line">        <span class="comment"># k控制衰减的速度</span></span><br><span class="line">        weights[j,j] = exp(diffMat*diffMat.T/(-<span class="number">2.0</span>*k**<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 根据矩阵乘法计算 xTx ，其中的 weights 矩阵是样本点对应的权重矩阵</span></span><br><span class="line">    xTx = xMat.T * (weights * xMat)</span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 计算出回归系数的一个估计</span></span><br><span class="line">    ws = xTx.I * (xMat.T * (weights * yMat))</span><br><span class="line">    <span class="keyword">return</span> testPoint * ws</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTest</span>(<span class="params">testArr,xArr,yArr,k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Description: </span></span><br><span class="line"><span class="string">            测试局部加权线性回归，对数据集中每个点调用 lwlr() 函数</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            testArr: 测试所用的所有样本点</span></span><br><span class="line"><span class="string">            xArr: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr: 每个样本对应的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            k: 控制核函数的衰减速率</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            yHat: 预测点的估计值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 得到样本点的总数</span></span><br><span class="line">    m = shape(testArr)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 构建一个全部都是 0 的 1 * m 的矩阵</span></span><br><span class="line">    yHat = zeros(m)</span><br><span class="line">    <span class="comment"># 循环所有的数据点，并将lwlr运用于所有的数据点 </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        yHat[i] = lwlr(testArr[i],xArr,yArr,k)</span><br><span class="line">    <span class="comment"># 返回估计值</span></span><br><span class="line">    <span class="keyword">return</span> yHat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTestPlot</span>(<span class="params">xArr,yArr,k=<span class="number">1.0</span></span>):</span>  </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Description:</span></span><br><span class="line"><span class="string">            首先将 X 排序，其余的都与lwlrTest相同，这样更容易绘图</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            xArr: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr: 每个样本对应的类别标签，即目标变量，实际值</span></span><br><span class="line"><span class="string">            k: 控制核函数的衰减速率的有关参数，这里设定的是常量值 1</span></span><br><span class="line"><span class="string">        Return: </span></span><br><span class="line"><span class="string">            yHat: 样本点的估计值</span></span><br><span class="line"><span class="string">            xCopy: xArr的复制</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 生成一个与目标变量数目相同的 0 向量</span></span><br><span class="line">    yHat = zeros(shape(yArr))</span><br><span class="line">    <span class="comment"># 将 xArr 转换为 矩阵形式</span></span><br><span class="line">    xCopy = mat(xArr)</span><br><span class="line">    <span class="comment"># 排序</span></span><br><span class="line">    xCopy.sort(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 开始循环，为每个样本点进行局部加权线性回归，得到最终的目标变量估计值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(shape(xArr)[<span class="number">0</span>]):</span><br><span class="line">        yHat[i] = lwlr(xCopy[i],xArr,yArr,k)</span><br><span class="line">    <span class="keyword">return</span> yHat,xCopy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for LWLR</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression2</span>():</span></span><br><span class="line">    xArr, yArr = loadDataSet(<span class="string">&quot;data/8.Regression/data.txt&quot;</span>)</span><br><span class="line">    yHat = lwlrTest(xArr, xArr, yArr, <span class="number">0.003</span>)</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    srtInd = xMat[:,<span class="number">1</span>].argsort(<span class="number">0</span>)           <span class="comment"># argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出</span></span><br><span class="line">    xSort=xMat[srtInd][:,<span class="number">0</span>,:]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(xSort[:,<span class="number">1</span>], yHat[srtInd])</span><br><span class="line">    ax.scatter(xMat[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], mat(yArr).T.flatten().A[<span class="number">0</span>] , s=<span class="number">2</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>



        <h5 id="2-2-3、局部加权线性回归-拟合效果"   >
          <a href="#2-2-3、局部加权线性回归-拟合效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-3、局部加权线性回归-拟合效果" class="headerlink" title="2.2.3、局部加权线性回归 拟合效果"></a>2.2.3、局部加权线性回归 拟合效果</h5>
      <p><img src="img/LinearR_7.png" alt="局部加权线性回归数据效果图"></p>
<p>上图使用了 3 种不同平滑值绘出的局部加权线性回归的结果。上图中的平滑系数 k =1.0，中图 k = 0.01，下图 k = 0.003 。可以看到，k = 1.0 时的使所有数据等比重，其模型效果与基本的线性回归相同，k=0.01时该模型可以挖出数据的潜在规律，而 k=0.003时则考虑了太多的噪声，进而导致了过拟合现象。</p>

        <h4 id="2-3、局部加权线性回归-注意事项"   >
          <a href="#2-3、局部加权线性回归-注意事项" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3、局部加权线性回归-注意事项" class="headerlink" title="2.3、局部加权线性回归 注意事项"></a>2.3、局部加权线性回归 注意事项</h4>
      <p>局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集。</p>

        <h3 id="3、线性回归-amp-局部加权线性回归-项目案例"   >
          <a href="#3、线性回归-amp-局部加权线性回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、线性回归-amp-局部加权线性回归-项目案例" class="headerlink" title="3、线性回归 &amp; 局部加权线性回归 项目案例"></a>3、线性回归 &amp; 局部加权线性回归 项目案例</h3>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>到此为止，我们已经介绍了找出最佳拟合直线的两种方法，下面我们用这些技术来预测鲍鱼的年龄。</p>

        <h4 id="3-1、项目概述"   >
          <a href="#3-1、项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-1、项目概述" class="headerlink" title="3.1、项目概述"></a>3.1、项目概述</h4>
      <p>我们有一份来自 UCI 的数据集合的数据，记录了鲍鱼（一种介壳类水生动物）的年龄。鲍鱼年龄可以从鲍鱼壳的层数推算得到。</p>

        <h4 id="3-2、开发流程"   >
          <a href="#3-2、开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-2、开发流程" class="headerlink" title="3.2、开发流程"></a>3.2、开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 采用任意方法收集数据</span><br><span class="line">准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</span><br><span class="line">分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</span><br><span class="line">训练算法: 找到回归系数</span><br><span class="line">测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果</span><br><span class="line">使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<blockquote>
<p>准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</p>
</blockquote>
<p>数据存储格式:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1	0.455	0.365	0.095	0.514	0.2245	0.101	0.15	15</span><br><span class="line">1	0.35	0.265	0.09	0.2255	0.0995	0.0485	0.07	7</span><br><span class="line">-1	0.53	0.42	0.135	0.677	0.2565	0.1415	0.21	9</span><br><span class="line">1	0.44	0.365	0.125	0.516	0.2155	0.114	0.155	10</span><br><span class="line">0	0.33	0.255	0.08	0.205	0.0895	0.0395	0.055	7</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</p>
</blockquote>
<blockquote>
<p>训练算法: 找到回归系数</p>
</blockquote>
<p>使用上面我们讲到的 局部加权线性回归 训练算法，求出回归系数</p>
<blockquote>
<p>测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssError</span>(<span class="params">yArr,yHatArr</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        返回真实值与预测值误差大小</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        yArr: 样本的真实值</span></span><br><span class="line"><span class="string">        yHatArr: 样本的预测值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        一个数字，代表误差</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">   <span class="keyword">return</span> ((yArr-yHatArr)**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test for abloneDataSet</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abaloneTest</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        预测鲍鱼的年龄</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    abX, abY = loadDataSet(<span class="string">&quot;data/8.Regression/abalone.txt&quot;</span>)</span><br><span class="line">    <span class="comment"># 使用不同的核进行预测</span></span><br><span class="line">    oldyHat01 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">    oldyHat1 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">    oldyHat10 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">10</span>)   </span><br><span class="line">    <span class="comment"># 打印出不同的核预测值与训练数据集上的真实值之间的误差大小</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;old yHat01 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat01.T)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;old yHat1 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat1.T)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;old yHat10 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat10.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印出 不同的核预测值 与 新数据集（测试数据集）上的真实值之间的误差大小</span></span><br><span class="line">    newyHat01 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;new yHat01 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat01.T)</span><br><span class="line">    newyHat1 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;new yHat1 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat1.T)</span><br><span class="line">    newyHat10 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;new yHat10 error Size is :&quot;</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat10.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用简单的 线性回归 进行预测，与上面的计算进行比较</span></span><br><span class="line">    standWs = standRegres(abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>])</span><br><span class="line">    standyHat = mat(abX[<span class="number">100</span>:<span class="number">199</span>]) * standWs</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;standRegress error Size is:&quot;</span>, rssError(abY[<span class="number">100</span>:<span class="number">199</span>], standyHat.T.A)</span><br></pre></td></tr></table></div></figure>


<p>根据我们上边的测试，可以看出: </p>
<p>简单线性回归达到了与局部加权现行回归类似的效果。这也说明了一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是 10 吗？或许是，但如果想得到更好的效果，可以尝试用 10 个不同的样本集做 10 次测试来比较结果。</p>
<blockquote>
<p>使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</p>
</blockquote>

        <h3 id="4、缩减系数来-“理解”-数据"   >
          <a href="#4、缩减系数来-“理解”-数据" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、缩减系数来-“理解”-数据" class="headerlink" title="4、缩减系数来 “理解” 数据"></a>4、缩减系数来 “理解” 数据</h3>
      <p>如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即我们不能再使用前面介绍的方法。这是因为在计算 <img src="img/LinearR_8.png" alt="矩阵求逆"> 的时候会出错。</p>
<p>如果特征比样本点还多(n &gt; m)，也就是说输入数据的矩阵 x 不是满秩矩阵。非满秩矩阵求逆时会出现问题。</p>
<p>为了解决这个问题，我们引入了 <code>岭回归（ridge regression）</code> 这种缩减方法。接着是 <code>lasso法</code>，最后介绍 <code>前向逐步回归</code>。</p>

        <h4 id="4-1、岭回归"   >
          <a href="#4-1、岭回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1、岭回归" class="headerlink" title="4.1、岭回归"></a>4.1、岭回归</h4>
      <p>简单来说，岭回归就是在矩阵 <img src="img/LinearR_9.png" alt="矩阵_1"> 上加一个 λI 从而使得矩阵非奇异，进而能对 <img src="img/LinearR_10.png" alt="矩阵_2"> 求逆。其中矩阵I是一个 n * n （等于列数） 的单位矩阵，<br>对角线上元素全为1，其他元素全为0。而λ是一个用户定义的数值，后面会做介绍。在这种情况下，回归系数的计算公式将变成: </p>
<p><img src="img/LinearR_11.png" alt="岭回归的回归系数计算"></p>
<p>岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入 λ 来限制了所有 w 之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作 <code>缩减(shrinkage)</code>。</p>
<p><img src="img/LinearR_22.png" alt="岭回归"></p>
<p>缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。</p>
<p>这里通过预测误差最小化得到 λ: 数据获取之后，首先抽一部分数据用于测试，剩余的作为训练集用于训练参数 w。训练完毕后在测试集上测试预测性能。通过选取不同的 λ 来重复上述测试过程，最终得到一个使预测误差最小的 λ 。</p>

        <h5 id="4-1-1、岭回归-原始代码"   >
          <a href="#4-1-1、岭回归-原始代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-1、岭回归-原始代码" class="headerlink" title="4.1.1、岭回归 原始代码"></a>4.1.1、岭回归 原始代码</h5>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeRegres</span>(<span class="params">xMat,yMat,lam=<span class="number">0.2</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Desc: </span></span><br><span class="line"><span class="string">            这个函数实现了给定 lambda 下的岭回归求解。</span></span><br><span class="line"><span class="string">            如果数据的特征比样本点还多，就不能再使用上面介绍的的线性回归和局部线性回归了，因为计算 (xTx)^(-1)会出现错误。</span></span><br><span class="line"><span class="string">            如果特征比样本点还多（n &gt; m），也就是说，输入数据的矩阵x不是满秩矩阵。非满秩矩阵在求逆时会出现问题。</span></span><br><span class="line"><span class="string">            为了解决这个问题，我们下边讲一下: 岭回归，这是我们要讲的第一种缩减方法。</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            xMat: 样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yMat: 每个样本对应的类别标签，即目标变量，实际值</span></span><br><span class="line"><span class="string">            lam: 引入的一个λ值，使得矩阵非奇异</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            经过岭回归公式计算得到的回归系数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="comment"># 岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆</span></span><br><span class="line">    denom = xTx + eye(shape(xMat)[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="comment"># 检查行列式是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆。</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(denom) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;This matrix is singular, cannot do inverse&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = denom.I * (xMat.T*yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span>(<span class="params">xArr,yArr</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Desc: </span></span><br><span class="line"><span class="string">            函数 ridgeTest() 用于在一组 λ 上测试结果</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            xArr: 样本数据的特征，即 feature</span></span><br><span class="line"><span class="string">            yArr: 样本数据的类别标签，即真实数据</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            wMat: 将所有的回归系数输出到一个矩阵并返回</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat=mat(yArr).T</span><br><span class="line">    <span class="comment"># 计算Y的均值</span></span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Y的所有的特征减去均值</span></span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    <span class="comment"># 标准化 x，计算 xMat 平均值</span></span><br><span class="line">    xMeans = mean(xMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 然后计算 X的方差</span></span><br><span class="line">    xVar = var(xMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 所有特征都减去各自的均值并除以方差</span></span><br><span class="line">    xMat = (xMat - xMeans)/xVar</span><br><span class="line">    <span class="comment"># 可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数。</span></span><br><span class="line">    numTestPts = <span class="number">30</span></span><br><span class="line">    <span class="comment"># 创建30 * m 的全部数据为0 的矩阵</span></span><br><span class="line">    wMat = zeros((numTestPts,shape(xMat)[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numTestPts):</span><br><span class="line">        <span class="comment"># exp() 返回 e^x </span></span><br><span class="line">        ws = ridgeRegres(xMat,yMat,exp(i-<span class="number">10</span>))</span><br><span class="line">        wMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> wMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for ridgeRegression</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression3</span>():</span></span><br><span class="line">    abX,abY = loadDataSet(<span class="string">&quot;data/8.Regression/abalone.txt&quot;</span>)</span><br><span class="line">    ridgeWeights = ridgeTest(abX, abY)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(ridgeWeights)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></div></figure>



        <h5 id="4-1-2、岭回归在鲍鱼数据集上的运行效果"   >
          <a href="#4-1-2、岭回归在鲍鱼数据集上的运行效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-1-2、岭回归在鲍鱼数据集上的运行效果" class="headerlink" title="4.1.2、岭回归在鲍鱼数据集上的运行效果"></a>4.1.2、岭回归在鲍鱼数据集上的运行效果</h5>
      <p><img src="img/LinearR_12.png" alt="岭回归的运行效果"></p>
<p>上图绘制出了回归系数与 log(λ) 的关系。在最左边，即 λ 最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减为0；在中间部分的某值将可以取得最好的预测效果。为了定量地找到最佳参数值，还需要进行交叉验证。另外，要判断哪些变量对结果预测最具有影响力，在上图中观察它们对应的系数大小就可以了。</p>

        <h4 id="4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator"   >
          <a href="#4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator" class="headerlink" title="4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)"></a>4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)</h4>
      <p>在增加如下约束时，普通的最小二乘法回归会得到与岭回归一样的公式: </p>
<p><img src="img/LinearR_13.png" alt="lasso_1"></p>
<p>上式限定了所有回归系数的平方和不能大于 λ 。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得到一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。</p>
<p>与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下: </p>
<p><img src="img/LinearR_14.png" alt="lasso_2"></p>
<p>唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭: 在 λ 足够小的时候，一些系数会因此被迫缩减到 0.这个特性可以帮助我们更好地理解数据。</p>

        <h4 id="4-3、前向逐步回归"   >
          <a href="#4-3、前向逐步回归" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-3、前向逐步回归" class="headerlink" title="4.3、前向逐步回归"></a>4.3、前向逐步回归</h4>
      <p>前向逐步回归算法可以得到与 lasso 差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有权重都设置为 0，然后每一步所做的决策是对某个权重增加或减少一个很小的值。</p>
<p>伪代码如下:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">数据标准化，使其分布满足 0 均值 和单位方差</span><br><span class="line">在每轮迭代过程中: </span><br><span class="line">    设置当前最小误差 lowestError 为正无穷</span><br><span class="line">    对每个特征:</span><br><span class="line">        增大或缩小:</span><br><span class="line">            改变一个系数得到一个新的 w</span><br><span class="line">            计算新 w 下的误差</span><br><span class="line">            如果误差 Error 小于当前最小误差 lowestError: 设置 Wbest 等于当前的 W</span><br><span class="line">        将 W 设置为新的 Wbest</span><br></pre></td></tr></table></div></figure>


        <h5 id="4-3-1、前向逐步回归-原始代码"   >
          <a href="#4-3-1、前向逐步回归-原始代码" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-3-1、前向逐步回归-原始代码" class="headerlink" title="4.3.1、前向逐步回归 原始代码"></a>4.3.1、前向逐步回归 原始代码</h5>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stageWise</span>(<span class="params">xArr,yArr,eps=<span class="number">0.01</span>,numIt=<span class="number">100</span></span>):</span></span><br><span class="line">    xMat = mat(xArr); yMat=mat(yArr).T</span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean     <span class="comment"># 也可以规则化ys但会得到更小的coef</span></span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    m,n=shape(xMat)</span><br><span class="line">    <span class="comment">#returnMat = zeros((numIt,n)) # 测试代码删除</span></span><br><span class="line">    ws = zeros((n,<span class="number">1</span>)); wsTest = ws.copy(); wsMax = ws.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numIt):</span><br><span class="line">        <span class="built_in">print</span> (ws.T)</span><br><span class="line">        lowestError = inf; </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> sign <span class="keyword">in</span> [-<span class="number">1</span>,<span class="number">1</span>]:</span><br><span class="line">                wsTest = ws.copy()</span><br><span class="line">                wsTest[j] += eps*sign</span><br><span class="line">                yTest = xMat*wsTest</span><br><span class="line">                rssE = rssError(yMat.A,yTest.A)</span><br><span class="line">                <span class="keyword">if</span> rssE &lt; lowestError:</span><br><span class="line">                    lowestError = rssE</span><br><span class="line">                    wsMax = wsTest</span><br><span class="line">        ws = wsMax.copy()</span><br><span class="line">        returnMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for stageWise</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression4</span>():</span></span><br><span class="line">    xArr,yArr=loadDataSet(<span class="string">&quot;data/8.Regression/abalone.txt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(stageWise(xArr,yArr,<span class="number">0.01</span>,<span class="number">200</span>))</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    yM = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yM</span><br><span class="line">    weights = standRegres(xMat, yMat.T)</span><br><span class="line">    <span class="built_in">print</span> (weights.T)</span><br></pre></td></tr></table></div></figure>



        <h5 id="4-3-2、逐步线性回归在鲍鱼数据集上的运行效果"   >
          <a href="#4-3-2、逐步线性回归在鲍鱼数据集上的运行效果" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-3-2、逐步线性回归在鲍鱼数据集上的运行效果" class="headerlink" title="4.3.2、逐步线性回归在鲍鱼数据集上的运行效果"></a>4.3.2、逐步线性回归在鲍鱼数据集上的运行效果</h5>
      <p><img src="img/LinearR_15.png" alt="逐步线性回归运行效果"></p>
<p>逐步线性回归算法的主要优点在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。</p>

        <h4 id="4-4、小结"   >
          <a href="#4-4、小结" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-4、小结" class="headerlink" title="4.4、小结"></a>4.4、小结</h4>
      <p>当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias），与此同时却减小了模型的方差。</p>

        <h3 id="5、权衡偏差和方差"   >
          <a href="#5、权衡偏差和方差" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、权衡偏差和方差" class="headerlink" title="5、权衡偏差和方差"></a>5、权衡偏差和方差</h3>
      <p>任何时候，一旦发现模型和测量值之间存在差异，就说出现了误差。当考虑模型中的 “噪声” 或者说误差时，必须考虑其来源。你可能会对复杂的过程进行简化，这将导致在模型和测量值之间出现 “噪声” 或误差，若无法理解数据的真实生成过程，也会导致差异的产生。另外，测量过程本身也可能产生 “噪声” 或者问题。下面我们举一个例子，我们使用 <code>线性回归</code> 和 <code>局部加权线性回归</code> 处理过一个从文件导入的二维数据。</p>
<p><img src="img/LinearR_16.png" alt="生成公式"></p>
<p>其中的 N(0, 1) 是一个均值为 0、方差为 1 的正态分布。我们尝试过仅用一条直线来拟合上述数据。不难想到，直线所能得到的最佳拟合应该是 3.0+1.7x 这一部分。这样的话，误差部分就是 0.1sin(30x)+0.06N(0, 1) 。在上面，我们使用了局部加权线性回归来试图捕捉数据背后的结构。该结构拟合起来有一定的难度，因此我们测试了多组不同的局部权重来找到具有最小测试误差的解。</p>
<p>下图给出了训练误差和测试误差的曲线图，上面的曲面就是测试误差，下面的曲线是训练误差。我们根据 预测鲍鱼年龄 的实验知道: 如果降低核的大小，那么训练误差将变小。从下图开看，从左到右就表示了核逐渐减小的过程。</p>
<p><img src="img/LinearR_17.png" alt="偏差方差图"></p>
<p>一般认为，上述两种误差由三个部分组成: 偏差、测量误差和随机噪声。局部加权线性回归 和 预测鲍鱼年龄 中，我们通过引入了三个越来越小的核来不断增大模型的方差。</p>
<p>在缩减系数来“理解”数据这一节中，我们介绍了缩减法，可以将一些系数缩减成很小的值或直接缩减为 0 ，这是一个增大模型偏差的例子。通过把一些特征的回归系数缩减到 0 ，同时也就减小了模型的复杂度。例子中有 8 个特征，消除其中两个后不仅使模型更易理解，同时还降低了预测误差。对照上图，左侧是参数缩减过于严厉的结果，而右侧是无缩减的效果。</p>
<p>方差是可以度量的。如果从鲍鱼数据中取一个随机样本集（例如取其中 100 个数据）并用线性模型拟合，将会得到一组回归系数。同理，再取出另一组随机样本集并拟合，将会得到另一组回归系数。这些系数间的差异大小也就是模型方差的反映。</p>

        <h3 id="6、回归-项目案例"   >
          <a href="#6、回归-项目案例" class="heading-link"><i class="fas fa-link"></i></a><a href="#6、回归-项目案例" class="headerlink" title="6、回归 项目案例"></a>6、回归 项目案例</h3>
      
        <h4 id="项目案例1-预测乐高玩具套装的价格"   >
          <a href="#项目案例1-预测乐高玩具套装的价格" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例1-预测乐高玩具套装的价格" class="headerlink" title="项目案例1: 预测乐高玩具套装的价格"></a>项目案例1: 预测乐高玩具套装的价格</h4>
      <p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h5>
      <p>Dangler 喜欢为乐高套装估价，我们用回归技术来帮助他建立一个预测模型。</p>

        <h5 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h5>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(1) 收集数据: 用 Google Shopping 的API收集数据。</span><br><span class="line">(2) 准备数据: 从返回的JSON数据中抽取价格。</span><br><span class="line">(3) 分析数据: 可视化并观察数据。</span><br><span class="line">(4) 训练算法: 构建不同的模型，采用逐步线性回归和直接的线性回归模型。</span><br><span class="line">(5) 测试算法: 使用交叉验证来测试不同的模型，分析哪个效果最好。</span><br><span class="line">(6) 使用算法: 这次练习的目标就是生成数据模型。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 使用 Google 购物的 API </p>
</blockquote>
<p>由于 Google 提供的 api 失效，我们只能自己下载咯，将数据存储在了 input 文件夹下的 setHtml 文件夹下</p>
<blockquote>
<p>准备数据: 从返回的 JSON 数据中抽取价格</p>
</blockquote>
<p>因为我们这里不是在线的，就不再是 JSON 了，我们直接解析线下的网页，得到我们想要的数据。</p>
<blockquote>
<p>分析数据: 可视化并观察数据</p>
</blockquote>
<p>这里我们将解析得到的数据打印出来，然后观察数据。</p>
<blockquote>
<p>训练算法: 构建不同的模型</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从页面读取数据，生成retX和retY列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrapePage</span>(<span class="params">retX, retY, inFile, yr, numPce, origPrc</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打开并读取HTML文件</span></span><br><span class="line">    fr = <span class="built_in">open</span>(inFile)</span><br><span class="line">    soup = BeautifulSoup(fr.read())</span><br><span class="line">    i=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据HTML页面结构进行解析</span></span><br><span class="line">    currentRow = soup.findAll(<span class="string">&#x27;table&#x27;</span>, r=<span class="string">&quot;%d&quot;</span> % i)</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">len</span>(currentRow)!=<span class="number">0</span>):</span><br><span class="line">        currentRow = soup.findAll(<span class="string">&#x27;table&#x27;</span>, r=<span class="string">&quot;%d&quot;</span> % i)</span><br><span class="line">        title = currentRow[<span class="number">0</span>].findAll(<span class="string">&#x27;a&#x27;</span>)[<span class="number">1</span>].text</span><br><span class="line">        lwrTitle = title.lower()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找是否有全新标签</span></span><br><span class="line">        <span class="keyword">if</span> (lwrTitle.find(<span class="string">&#x27;new&#x27;</span>) &gt; -<span class="number">1</span>) <span class="keyword">or</span> (lwrTitle.find(<span class="string">&#x27;nisb&#x27;</span>) &gt; -<span class="number">1</span>):</span><br><span class="line">            newFlag = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newFlag = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找是否已经标志出售，我们只收集已出售的数据</span></span><br><span class="line">        soldUnicde = currentRow[<span class="number">0</span>].findAll(<span class="string">&#x27;td&#x27;</span>)[<span class="number">3</span>].findAll(<span class="string">&#x27;span&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(soldUnicde)==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;item #%d did not sell&quot;</span> % i</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 解析页面获取当前价格</span></span><br><span class="line">            soldPrice = currentRow[<span class="number">0</span>].findAll(<span class="string">&#x27;td&#x27;</span>)[<span class="number">4</span>]</span><br><span class="line">            priceStr = soldPrice.text</span><br><span class="line">            priceStr = priceStr.replace(<span class="string">&#x27;$&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="comment">#strips out $</span></span><br><span class="line">            priceStr = priceStr.replace(<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="comment">#strips out ,</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(soldPrice)&gt;<span class="number">1</span>:</span><br><span class="line">                priceStr = priceStr.replace(<span class="string">&#x27;Free shipping&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            sellingPrice = <span class="built_in">float</span>(priceStr)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 去掉不完整的套装价格</span></span><br><span class="line">            <span class="keyword">if</span>  sellingPrice &gt; origPrc * <span class="number">0.5</span>:</span><br><span class="line">                    <span class="built_in">print</span> <span class="string">&quot;%d\t%d\t%d\t%f\t%f&quot;</span> % (yr,numPce,newFlag,origPrc, sellingPrice)</span><br><span class="line">                    retX.append([yr, numPce, newFlag, origPrc])</span><br><span class="line">                    retY.append(sellingPrice)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        currentRow = soup.findAll(<span class="string">&#x27;table&#x27;</span>, r=<span class="string">&quot;%d&quot;</span> % i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 依次读取六种乐高套装的数据，并生成数据矩阵        </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setDataCollect</span>(<span class="params">retX, retY</span>):</span></span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego8288.html&#x27;</span>, <span class="number">2006</span>, <span class="number">800</span>, <span class="number">49.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10030.html&#x27;</span>, <span class="number">2002</span>, <span class="number">3096</span>, <span class="number">269.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10179.html&#x27;</span>, <span class="number">2007</span>, <span class="number">5195</span>, <span class="number">499.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10181.html&#x27;</span>, <span class="number">2007</span>, <span class="number">3428</span>, <span class="number">199.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10189.html&#x27;</span>, <span class="number">2008</span>, <span class="number">5922</span>, <span class="number">299.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">&#x27;data/8.Regression/setHtml/lego10196.html&#x27;</span>, <span class="number">2009</span>, <span class="number">3263</span>, <span class="number">249.99</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 使用交叉验证来测试不同的模型，分析哪个效果最好</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉验证测试岭回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossValidation</span>(<span class="params">xArr,yArr,numVal=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="comment"># 获得数据点个数，xArr和yArr具有相同长度</span></span><br><span class="line">    m = <span class="built_in">len</span>(yArr)</span><br><span class="line">    indexList = <span class="built_in">range</span>(m)</span><br><span class="line">    errorMat = zeros((numVal,<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主循环 交叉验证循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numVal):</span><br><span class="line">        <span class="comment"># 随机拆分数据，将数据分为训练集（90%）和测试集（10%）</span></span><br><span class="line">        trainX=[]; trainY=[]</span><br><span class="line">        testX = []; testY = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对数据进行混洗操作</span></span><br><span class="line">        random.shuffle(indexList)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 切分训练集和测试集</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">if</span> j &lt; m*<span class="number">0.9</span>: </span><br><span class="line">                trainX.append(xArr[indexList[j]])</span><br><span class="line">                trainY.append(yArr[indexList[j]])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                testX.append(xArr[indexList[j]])</span><br><span class="line">                testY.append(yArr[indexList[j]])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得回归系数矩阵</span></span><br><span class="line">        wMat = ridgeTest(trainX,trainY)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 循环遍历矩阵中的30组回归系数</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>):</span><br><span class="line">            <span class="comment"># 读取训练集和数据集</span></span><br><span class="line">            matTestX = mat(testX); matTrainX=mat(trainX)</span><br><span class="line">            <span class="comment"># 对数据进行标准化</span></span><br><span class="line">            meanTrain = mean(matTrainX,<span class="number">0</span>)</span><br><span class="line">            varTrain = var(matTrainX,<span class="number">0</span>)</span><br><span class="line">            matTestX = (matTestX-meanTrain)/varTrain</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试回归效果并存储</span></span><br><span class="line">            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算误差</span></span><br><span class="line">            errorMat[i,k] = ((yEst.T.A-array(testY))**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算误差估计值的均值</span></span><br><span class="line">    meanErrors = mean(errorMat,<span class="number">0</span>)</span><br><span class="line">    minMean = <span class="built_in">float</span>(<span class="built_in">min</span>(meanErrors))</span><br><span class="line">    bestWeights = wMat[nonzero(meanErrors==minMean)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不要使用标准化的数据，需要对数据进行还原来得到输出结果</span></span><br><span class="line">    xMat = mat(xArr); yMat=mat(yArr).T</span><br><span class="line">    meanX = mean(xMat,<span class="number">0</span>); varX = var(xMat,<span class="number">0</span>)</span><br><span class="line">    unReg = bestWeights/varX</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出构建的模型</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;the best model from Ridge Regression is:\n&quot;</span>,unReg</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;with constant term: &quot;</span>,-<span class="number">1</span>*<span class="built_in">sum</span>(multiply(meanX,unReg)) + mean(yMat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># predict for lego&#x27;s price</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression5</span>():</span></span><br><span class="line">    lgX = []</span><br><span class="line">    lgY = []</span><br><span class="line"></span><br><span class="line">    setDataCollect(lgX, lgY)</span><br><span class="line">    crossValidation(lgX, lgY, <span class="number">10</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 这次练习的目标就是生成数据模型</p>
</blockquote>

        <h2 id="7、选读内容"   >
          <a href="#7、选读内容" class="heading-link"><i class="fas fa-link"></i></a><a href="#7、选读内容" class="headerlink" title="7、选读内容"></a>7、选读内容</h2>
      <p>求解线性回归可以有很多种方式，除了上述的方法（正规方程 normal equation）解决之外，还有可以对Cost function 求导，其中最简单的方法就是梯度下降法。</p>
<p> 那么正规方程就可以直接得出真实值。而梯度下降法只能给出近似值。</p>
<p>以下是梯度下降法和正规方程的比较:</p>
<div class="table-container"><table>
<thead>
<tr>
<th>梯度下降法</th>
<th align="center">正规方程</th>
</tr>
</thead>
<tbody><tr>
<td>结果为真实值的近似值</td>
<td align="center">结果为真实值</td>
</tr>
<tr>
<td>需要循环多次</td>
<td align="center">无需循环</td>
</tr>
<tr>
<td>样本数量大的时候也ok</td>
<td align="center">样本数量特别大的时候会很慢（n&gt;10000）</td>
</tr>
</tbody></table></div>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://cwiki.apachecn.org/display/~chenyao" >小瑶</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/08/07/ml_7/">第7章 集成方法 ensemble method</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-08-07</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-08-29</span></span></div></header><div class="post-body"><div class="post-excerpt"><p><img src="img/adaboost_headPage.jpg" alt="利用AdaBoost元算法提高分类" title="利用AdaBoost元算法提高分类"></p>

        <h2 id="集成方法-ensemble-method（元算法-meta-algorithm）-概述"   >
          <a href="#集成方法-ensemble-method（元算法-meta-algorithm）-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#集成方法-ensemble-method（元算法-meta-algorithm）-概述" class="headerlink" title="集成方法: ensemble method（元算法: meta algorithm） 概述"></a>集成方法: ensemble method（元算法: meta algorithm） 概述</h2>
      <ul>
<li><p>概念: 是对其他算法进行组合的一种形式。</p>
</li>
<li><p>通俗来说:  当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。<br>  机器学习处理问题时又何尝不是如此？ 这就是集成方法背后的思想。</p>
</li>
<li><p>集成方法:   </p>
<ol>
<li>投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法</li>
<li>再学习(boosting): 是基于所有分类器的加权求和的方法</li>
</ol>
</li>
</ul>

        <h2 id="集成方法-场景"   >
          <a href="#集成方法-场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#集成方法-场景" class="headerlink" title="集成方法 场景"></a>集成方法 场景</h2>
      <p>目前 bagging 方法最流行的版本是: 随机森林(random forest)<br/><br>选男友: 美女选择择偶对象的时候，会问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友</p>
<p>目前 boosting 方法最流行的版本是: AdaBoost<br/><br>追女友: 3个帅哥追同一个美女，第1个帅哥失败-&gt;(传授经验: 姓名、家庭情况) 第2个帅哥失败-&gt;(传授经验: 兴趣爱好、性格特点) 第3个帅哥成功</p>
<blockquote>
<p>bagging 和 boosting 区别是什么？</p>
</blockquote>
<ol>
<li>bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。</li>
<li>bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。</li>
<li>bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</li>
</ol>

        <h2 id="随机森林"   >
          <a href="#随机森林" class="heading-link"><i class="fas fa-link"></i></a><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2>
      
        <h3 id="随机森林-概述"   >
          <a href="#随机森林-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#随机森林-概述" class="headerlink" title="随机森林 概述"></a>随机森林 概述</h3>
      <ul>
<li>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。</li>
<li>决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。</li>
</ul>

        <h3 id="随机森林-原理"   >
          <a href="#随机森林-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#随机森林-原理" class="headerlink" title="随机森林 原理"></a>随机森林 原理</h3>
      <p>那随机森林具体如何构建呢？<br/><br>有两个方面: <br/></p>
<ol>
<li>数据的随机性化<br/> </li>
<li>待选特征的随机化<br/></li>
</ol>
<p>使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p>
<blockquote>
<p>数据的随机化: 使得随机森林中的决策树更普遍化一点，适合更多的场景。</p>
</blockquote>
<p>（有放回的准确率在: 70% 以上， 无放回的准确率在: 60% 以上）</p>
<ol>
<li>采取有放回的抽样方式 构造子数据集，保证不同子集之间的数量级一样（不同子集／同一子集 之间的元素可以重复）</li>
<li>利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。</li>
<li>然后统计子决策树的投票结果，得到最终的分类 就是 随机森林的输出结果。</li>
<li>如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。</li>
</ol>
<p><img src="img/%E6%95%B0%E6%8D%AE%E9%87%8D%E6%8A%BD%E6%A0%B7.jpg" alt="数据重抽样"></p>
<blockquote>
<p>待选特征的随机化</p>
</blockquote>
<ol>
<li>子树从所有的待选特征中随机选取一定的特征。</li>
<li>在选取的特征中选取最优的特征。</li>
</ol>
<p>下图中，蓝色的方块代表所有可以被选择的特征，也就是目前的待选特征；黄色的方块是分裂特征。<br/><br>左边是一棵决策树的特征选取过程，通过在待选特征中选取最优的分裂特征（别忘了前文提到的ID3算法，C4.5算法，CART算法等等），完成分裂。<br/><br>右边是一个随机森林中的子树的特征选取过程。<br/></p>
<p><img src="img/%E7%89%B9%E5%BE%81%E9%87%8D%E6%8A%BD%E6%A0%B7.jpg" alt="特征重抽样"></p>
<blockquote>
<p>随机森林 开发流程</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 任何方法</span><br><span class="line">准备数据: 转换样本集</span><br><span class="line">分析数据: 任何方法</span><br><span class="line">训练算法: 通过数据随机化和特征随机化，进行多实例的分类评估</span><br><span class="line">测试算法: 计算错误率</span><br><span class="line">使用算法: 输入样本数据，然后运行 随机森林 算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>随机森林 算法特点</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点: 几乎不需要输入准备、可实现隐式特征选择、训练速度非常快、其他模型很难超越、很难建立一个糟糕的随机森林模型、大量优秀、免费以及开源的实现。</span><br><span class="line">缺点: 劣势在于模型大小、是个很难去解释的黑盒子。</span><br><span class="line">适用数据范围: 数值型和标称型</span><br></pre></td></tr></table></div></figure>


        <h3 id="项目案例-声纳信号分类"   >
          <a href="#项目案例-声纳信号分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-声纳信号分类" class="headerlink" title="项目案例: 声纳信号分类"></a>项目案例: 声纳信号分类</h3>
      
        <h4 id="项目概述"   >
          <a href="#项目概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>这是 Gorman 和 Sejnowski 在研究使用神经网络的声纳信号分类中使用的数据集。任务是训练一个模型来区分声纳信号。</p>

        <h4 id="开发流程"   >
          <a href="#开发流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 提供的文本文件</span><br><span class="line">准备数据: 转换样本集</span><br><span class="line">分析数据: 手工检查数据</span><br><span class="line">训练算法: 在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果</span><br><span class="line">测试算法: 在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分</span><br><span class="line">使用算法: 若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 提供的文本文件</p>
</blockquote>
<p>样本数据: sonar-all-data.txt</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.02,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.066,0.2273,0.31,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.555,0.6711,0.6415,0.7104,0.808,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.051,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.018,0.0084,0.009,0.0032,R</span><br><span class="line">0.0453,0.0523,0.0843,0.0689,0.1183,0.2583,0.2156,0.3481,0.3337,0.2872,0.4918,0.6552,0.6919,0.7797,0.7464,0.9444,1,0.8874,0.8024,0.7818,0.5212,0.4052,0.3957,0.3914,0.325,0.32,0.3271,0.2767,0.4423,0.2028,0.3788,0.2947,0.1984,0.2341,0.1306,0.4182,0.3835,0.1057,0.184,0.197,0.1674,0.0583,0.1401,0.1628,0.0621,0.0203,0.053,0.0742,0.0409,0.0061,0.0125,0.0084,0.0089,0.0048,0.0094,0.0191,0.014,0.0049,0.0052,0.0044,R</span><br><span class="line">0.0262,0.0582,0.1099,0.1083,0.0974,0.228,0.2431,0.3771,0.5598,0.6194,0.6333,0.706,0.5544,0.532,0.6479,0.6931,0.6759,0.7551,0.8929,0.8619,0.7974,0.6737,0.4293,0.3648,0.5331,0.2413,0.507,0.8533,0.6036,0.8514,0.8512,0.5045,0.1862,0.2709,0.4232,0.3043,0.6116,0.6756,0.5375,0.4719,0.4647,0.2587,0.2129,0.2222,0.2111,0.0176,0.1348,0.0744,0.013,0.0106,0.0033,0.0232,0.0166,0.0095,0.018,0.0244,0.0316,0.0164,0.0095,0.0078,R</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 转换样本集</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入csv文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">filename</span>):</span></span><br><span class="line">    dataset = []</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            lineArr = []</span><br><span class="line">            <span class="keyword">for</span> featrue <span class="keyword">in</span> line.split(<span class="string">&#x27;,&#x27;</span>):</span><br><span class="line">                <span class="comment"># strip()返回移除字符串头尾指定的字符生成的新字符串</span></span><br><span class="line">                str_f = featrue.strip()</span><br><span class="line">                <span class="keyword">if</span> str_f.isdigit(): <span class="comment"># 判断是否是数字</span></span><br><span class="line">                    <span class="comment"># 将数据集的第column列转换成float形式</span></span><br><span class="line">                    lineArr.append(<span class="built_in">float</span>(str_f))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 添加分类标签</span></span><br><span class="line">                    lineArr.append(str_f)</span><br><span class="line">            dataset.append(lineArr)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 手工检查数据</p>
</blockquote>
<blockquote>
<p>训练算法: 在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果</p>
</blockquote>
<ul>
<li>样本数据随机无放回抽样-用于交叉验证</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_validation_split</span>(<span class="params">dataset, n_folds</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;cross_validation_split(将数据集进行抽重抽样 n_folds 份，数据可以重复抽取)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset     原始数据集</span></span><br><span class="line"><span class="string">        n_folds     数据集dataset分成n_flods份</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataset_split    list集合，存放的是: 将数据集进行抽重抽样 n_folds 份，数据可以重复抽取</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataset_split = <span class="built_in">list</span>()</span><br><span class="line">    dataset_copy = <span class="built_in">list</span>(dataset)       <span class="comment"># 复制一份 dataset,防止 dataset 的内容改变</span></span><br><span class="line">    fold_size = <span class="built_in">len</span>(dataset) / n_folds</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_folds):</span><br><span class="line">        fold = <span class="built_in">list</span>()                  <span class="comment"># 每次循环 fold 清零，防止重复导入 dataset_split</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(fold) &lt; fold_size:   <span class="comment"># 这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立</span></span><br><span class="line">            <span class="comment"># 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此为自助采样法。从而保证每棵决策树训练集的差异性            </span></span><br><span class="line">            index = randrange(<span class="built_in">len</span>(dataset_copy))</span><br><span class="line">            <span class="comment"># 将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。</span></span><br><span class="line">            <span class="comment"># pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。</span></span><br><span class="line">            fold.append(dataset_copy.pop(index))  <span class="comment"># 无放回的方式</span></span><br><span class="line">            <span class="comment"># fold.append(dataset_copy[index])  # 有放回的方式</span></span><br><span class="line">        dataset_split.append(fold)</span><br><span class="line">    <span class="comment"># 由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证</span></span><br><span class="line">    <span class="keyword">return</span> dataset_split</span><br></pre></td></tr></table></div></figure>

<ul>
<li>训练数据集随机化</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a random subsample from the dataset with replacement</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsample</span>(<span class="params">dataset, ratio</span>):</span>   <span class="comment"># 创建数据集的随机子样本</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;random_forest(评估算法性能，返回模型得分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset         训练数据集</span></span><br><span class="line"><span class="string">        ratio           训练数据集的样本比例</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sample          随机抽样的训练样本</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    sample = <span class="built_in">list</span>()</span><br><span class="line">    <span class="comment"># 训练样本的按比例抽样。</span></span><br><span class="line">    <span class="comment"># round() 方法返回浮点数x的四舍五入值。</span></span><br><span class="line">    n_sample = <span class="built_in">round</span>(<span class="built_in">len</span>(dataset) * ratio)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(sample) &lt; n_sample:</span><br><span class="line">        <span class="comment"># 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此为自助采样法。从而保证每棵决策树训练集的差异性</span></span><br><span class="line">        index = randrange(<span class="built_in">len</span>(dataset))</span><br><span class="line">        sample.append(dataset[index])</span><br><span class="line">    <span class="keyword">return</span> sample</span><br></pre></td></tr></table></div></figure>

<ul>
<li>特征随机化</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_split</span>(<span class="params">dataset, n_features</span>):</span></span><br><span class="line">    class_values = <span class="built_in">list</span>(<span class="built_in">set</span>(row[-<span class="number">1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> dataset))  <span class="comment"># class_values =[0, 1]</span></span><br><span class="line">    b_index, b_value, b_score, b_groups = <span class="number">999</span>, <span class="number">999</span>, <span class="number">999</span>, <span class="literal">None</span></span><br><span class="line">    features = <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(features) &lt; n_features:</span><br><span class="line">        index = randrange(<span class="built_in">len</span>(dataset[<span class="number">0</span>])-<span class="number">1</span>)  <span class="comment"># 往 features 添加 n_features 个特征（ n_feature 等于特征数的个数），特征索引从 dataset 中随机取</span></span><br><span class="line">        <span class="keyword">if</span> index <span class="keyword">not</span> <span class="keyword">in</span> features:</span><br><span class="line">            features.append(index)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> features:                    <span class="comment"># 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> dataset:</span><br><span class="line">            groups = test_split(index, row[index], dataset)  <span class="comment"># groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值</span></span><br><span class="line">            gini = gini_index(groups, class_values)</span><br><span class="line">            <span class="comment"># 左右两边的数量越一样，说明数据区分度不高，gini系数越大</span></span><br><span class="line">            <span class="keyword">if</span> gini &lt; b_score:</span><br><span class="line">                b_index, b_value, b_score, b_groups = index, row[index], gini, groups  <span class="comment"># 最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本</span></span><br><span class="line">    <span class="comment"># print b_score</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;index&#x27;</span>: b_index, <span class="string">&#x27;value&#x27;</span>: b_value, <span class="string">&#x27;groups&#x27;</span>: b_groups&#125;</span><br></pre></td></tr></table></div></figure>

<ul>
<li>随机森林</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Random Forest Algorithm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_forest</span>(<span class="params">train, test, max_depth, min_size, sample_size, n_trees, n_features</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;random_forest(评估算法性能，返回模型得分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train           训练数据集</span></span><br><span class="line"><span class="string">        test            测试数据集</span></span><br><span class="line"><span class="string">        max_depth       决策树深度不能太深，不然容易导致过拟合</span></span><br><span class="line"><span class="string">        min_size        叶子节点的大小</span></span><br><span class="line"><span class="string">        sample_size     训练数据集的样本比例</span></span><br><span class="line"><span class="string">        n_trees         决策树的个数</span></span><br><span class="line"><span class="string">        n_features      选取的特征的个数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        predictions     每一行的预测结果，bagging 预测最后的分类结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    trees = <span class="built_in">list</span>()</span><br><span class="line">    <span class="comment"># n_trees 表示决策树的数量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_trees):</span><br><span class="line">        <span class="comment"># 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性</span></span><br><span class="line">        sample = subsample(train, sample_size)</span><br><span class="line">        <span class="comment"># 创建一个决策树</span></span><br><span class="line">        tree = build_tree(sample, max_depth, min_size, n_features)</span><br><span class="line">        trees.append(tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每一行的预测结果，bagging 预测最后的分类结果</span></span><br><span class="line">    predictions = [bagging_predict(trees, row) <span class="keyword">for</span> row <span class="keyword">in</span> test]</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>测试算法: 在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分。</p>
</blockquote>
<ul>
<li>计算随机森林的预测结果的正确率</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评估算法性能，返回模型得分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_algorithm</span>(<span class="params">dataset, algorithm, n_folds, *args</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;evaluate_algorithm(评估算法性能，返回模型得分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset     原始数据集</span></span><br><span class="line"><span class="string">        algorithm   使用的算法</span></span><br><span class="line"><span class="string">        n_folds     数据的份数</span></span><br><span class="line"><span class="string">        *args       其他的参数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        scores      模型得分</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集进行随机抽样，分成 n_folds 份，数据无重复的抽取</span></span><br><span class="line">    folds = cross_validation_split(dataset, n_folds)</span><br><span class="line">    scores = <span class="built_in">list</span>()</span><br><span class="line">    <span class="comment"># 每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证</span></span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> folds:</span><br><span class="line">        train_set = <span class="built_in">list</span>(folds)</span><br><span class="line">        train_set.remove(fold)</span><br><span class="line">        <span class="comment"># 将多个 fold 列表组合成一个 train_set 列表, 类似 union all</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In [20]: l1=[[1, 2, &#x27;a&#x27;], [11, 22, &#x27;b&#x27;]]</span></span><br><span class="line"><span class="string">        In [21]: l2=[[3, 4, &#x27;c&#x27;], [33, 44, &#x27;d&#x27;]]</span></span><br><span class="line"><span class="string">        In [22]: l=[]</span></span><br><span class="line"><span class="string">        In [23]: l.append(l1)</span></span><br><span class="line"><span class="string">        In [24]: l.append(l2)</span></span><br><span class="line"><span class="string">        In [25]: l</span></span><br><span class="line"><span class="string">        Out[25]: [[[1, 2, &#x27;a&#x27;], [11, 22, &#x27;b&#x27;]], [[3, 4, &#x27;c&#x27;], [33, 44, &#x27;d&#x27;]]]</span></span><br><span class="line"><span class="string">        In [26]: sum(l, [])</span></span><br><span class="line"><span class="string">        Out[26]: [[1, 2, &#x27;a&#x27;], [11, 22, &#x27;b&#x27;], [3, 4, &#x27;c&#x27;], [33, 44, &#x27;d&#x27;]]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        train_set = <span class="built_in">sum</span>(train_set, [])</span><br><span class="line">        test_set = <span class="built_in">list</span>()</span><br><span class="line">        <span class="comment"># fold 表示从原始数据集 dataset 提取出来的测试集</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> fold:</span><br><span class="line">            row_copy = <span class="built_in">list</span>(row)</span><br><span class="line">            row_copy[-<span class="number">1</span>] = <span class="literal">None</span> </span><br><span class="line">            test_set.append(row_copy)</span><br><span class="line">        predicted = algorithm(train_set, test_set, *args)</span><br><span class="line">        actual = [row[-<span class="number">1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> fold]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算随机森林的预测结果的正确率</span></span><br><span class="line">        accuracy = accuracy_metric(actual, predicted)</span><br><span class="line">        scores.append(accuracy)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码</p>
</blockquote>
<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.RandomForest/randomForest.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.RandomForest/randomForest.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.RandomForest/randomForest.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h2 id="AdaBoost"   >
          <a href="#AdaBoost" class="heading-link"><i class="fas fa-link"></i></a><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2>
      
        <h3 id="AdaBoost-adaptive-boosting-自适应-boosting-概述"   >
          <a href="#AdaBoost-adaptive-boosting-自适应-boosting-概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#AdaBoost-adaptive-boosting-自适应-boosting-概述" class="headerlink" title="AdaBoost (adaptive boosting: 自适应 boosting) 概述"></a>AdaBoost (adaptive boosting: 自适应 boosting) 概述</h3>
      <p><code>能否使用弱分类器和多个实例来构建一个强分类器？ 这是一个非常有趣的理论问题。</code></p>

        <h3 id="AdaBoost-原理"   >
          <a href="#AdaBoost-原理" class="heading-link"><i class="fas fa-link"></i></a><a href="#AdaBoost-原理" class="headerlink" title="AdaBoost 原理"></a>AdaBoost 原理</h3>
      <blockquote>
<p>AdaBoost 工作原理</p>
</blockquote>
<p><img src="img/adaboost_illustration.png" alt="AdaBoost 工作原理" title="AdaBoost 工作原理"></p>
<blockquote>
<p>AdaBoost 开发流程</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 可以使用任意方法</span><br><span class="line">准备数据: 依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。</span><br><span class="line">    当然也可以使用任意分类器作为弱分类器，第2章到第6章中的任一分类器都可以充当弱分类器。</span><br><span class="line">    作为弱分类器，简单分类器的效果更好。</span><br><span class="line">分析数据: 可以使用任意方法。</span><br><span class="line">训练算法: AdaBoost 的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。</span><br><span class="line">测试算法: 计算分类的错误率。</span><br><span class="line">使用算法: 通SVM一样，AdaBoost 预测两个类别中的一个。如果想把它应用到多个类别的场景，那么就要像多类 SVM 中的做法一样对 AdaBoost 进行修改。</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>AdaBoost 算法特点</p>
</blockquote>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* 优点: 泛化（由具体的、个别的扩大为一般的）错误率低，易编码，可以应用在大部分分类器上，无参数调节。</span><br><span class="line">* 缺点: 对离群点敏感。</span><br><span class="line">* 适用数据类型: 数值型和标称型数据。</span><br></pre></td></tr></table></div></figure>


        <h3 id="项目案例-马疝病的预测"   >
          <a href="#项目案例-马疝病的预测" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目案例-马疝病的预测" class="headerlink" title="项目案例: 马疝病的预测"></a>项目案例: 马疝病的预测</h3>
      <blockquote>
<p>项目流程图</p>
</blockquote>
<p><img src="img/adaboost_code-flow-chart.jpg" alt="AdaBoost代码流程图" title="AdaBoost代码流程图"></p>
<p>基于单层决策树构建弱分类器</p>
<ul>
<li>单层决策树(decision stump, 也称决策树桩)是一种简单的决策树。</li>
</ul>

        <h4 id="项目概述-1"   >
          <a href="#项目概述-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4>
      <p>预测患有疝气病的马的存活问题，这里的数据包括368个样本和28个特征，疝气病是描述马胃肠痛的术语，然而，这种病并不一定源自马的胃肠问题，其他问题也可能引发疝气病，该数据集中包含了医院检测马疝气病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。另外，除了部分指标主观和难以测量之外，该数据还存在一个问题，数据集中有30%的值是缺失的。</p>

        <h4 id="开发流程-1"   >
          <a href="#开发流程-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据: 提供的文本文件</span><br><span class="line">准备数据: 确保类别标签是+1和-1，而非1和0</span><br><span class="line">分析数据: 统计分析</span><br><span class="line">训练算法: 在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器</span><br><span class="line">测试算法: 我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较</span><br><span class="line">使用算法: 观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>收集数据: 提供的文本文件</p>
</blockquote>
<p>训练数据: horseColicTraining.txt<br/><br>测试数据: horseColicTest.txt</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2.000000	1.000000	38.500000	66.000000	28.000000	3.000000	3.000000	0.000000	2.000000	5.000000	4.000000	4.000000	0.000000	0.000000	0.000000	3.000000	5.000000	45.000000	8.400000	0.000000	0.000000	-1.000000</span><br><span class="line">1.000000	1.000000	39.200000	88.000000	20.000000	0.000000	0.000000	4.000000	1.000000	3.000000	4.000000	2.000000	0.000000	0.000000	0.000000	4.000000	2.000000	50.000000	85.000000	2.000000	2.000000	-1.000000</span><br><span class="line">2.000000	1.000000	38.300000	40.000000	24.000000	1.000000	1.000000	3.000000	1.000000	3.000000	3.000000	1.000000	0.000000	0.000000	0.000000	1.000000	1.000000	33.000000	6.700000	0.000000	0.000000	1.000000</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>准备数据: 确保类别标签是+1和-1，而非1和0</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    <span class="comment"># 获取 feature 的数量, 便于获取</span></span><br><span class="line">    numFeat = <span class="built_in">len</span>(<span class="built_in">open</span>(fileName).readline().split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    dataArr = []</span><br><span class="line">    labelArr = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeat-<span class="number">1</span>):</span><br><span class="line">            lineArr.append(<span class="built_in">float</span>(curLine[i]))</span><br><span class="line">        dataArr.append(lineArr)</span><br><span class="line">        labelArr.append(<span class="built_in">float</span>(curLine[-<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>分析数据: 统计分析</p>
</blockquote>
<p>过拟合(overfitting, 也称为过学习)</p>
<ul>
<li>发现测试错误率在达到一个最小值之后有开始上升，这种现象称为过拟合。</li>
</ul>
<p><img src="img/%E8%BF%87%E6%8B%9F%E5%90%88.png" alt="过拟合"></p>
<ul>
<li>通俗来说: 就是把一些噪音数据也拟合进去的，如下图。</li>
</ul>
<p><img src="img/%E8%BF%87%E6%8B%9F%E5%90%88%E5%9B%BE%E8%A7%A3.png" alt="过拟合"></p>
<blockquote>
<p>训练算法: 在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span>(<span class="params">dataArr, labelArr, numIt=<span class="number">40</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;adaBoostTrainDS(adaBoost训练过程放大)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataArr   特征标签集合</span></span><br><span class="line"><span class="string">        labelArr  分类标签集合</span></span><br><span class="line"><span class="string">        numIt     实例数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        weakClassArr  弱分类器的集合</span></span><br><span class="line"><span class="string">        aggClassEst   预测的分类结果值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    weakClassArr = []</span><br><span class="line">    m = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化 D，设置每个样本的权重值，平均分为m份</span></span><br><span class="line">    D = mat(ones((m, <span class="number">1</span>))/m)</span><br><span class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numIt):</span><br><span class="line">        <span class="comment"># 得到决策树的模型</span></span><br><span class="line">        bestStump, error, classEst = buildStump(dataArr, labelArr, D)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># alpha目的主要是计算每一个分类器实例的权重(组合就是分类结果)</span></span><br><span class="line">        <span class="comment"># 计算每个分类器的alpha权重值</span></span><br><span class="line">        alpha = <span class="built_in">float</span>(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/<span class="built_in">max</span>(error, <span class="number">1e-16</span>)))</span><br><span class="line">        bestStump[<span class="string">&#x27;alpha&#x27;</span>] = alpha</span><br><span class="line">        <span class="comment"># store Stump Params in Array</span></span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;alpha=%s, classEst=%s, bestStump=%s, error=%s &quot;</span> % (alpha, classEst.T, bestStump, error)</span><br><span class="line">        <span class="comment"># 分类正确: 乘积为1，不会影响结果，-1主要是下面求e的-alpha次方</span></span><br><span class="line">        <span class="comment"># 分类错误: 乘积为 -1，结果会受影响，所以也乘以 -1</span></span><br><span class="line">        expon = multiply(-<span class="number">1</span>*alpha*mat(labelArr).T, classEst)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;(-1取反)预测值expon=&#x27;</span>, expon.T</span><br><span class="line">        <span class="comment"># 计算e的expon次方，然后计算得到一个综合的概率的值</span></span><br><span class="line">        <span class="comment"># 结果发现:  判断错误的样本，D中相对应的样本权重值会变大。</span></span><br><span class="line">        D = multiply(D, exp(expon))</span><br><span class="line">        D = D/D.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测的分类结果值，在上一轮结果的基础上，进行加和操作</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;当前的分类结果: &#x27;</span>, alpha*classEst.T</span><br><span class="line">        aggClassEst += alpha*classEst</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;叠加后的分类结果aggClassEst: &quot;</span>, aggClassEst.T</span><br><span class="line">        <span class="comment"># sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。</span></span><br><span class="line">        <span class="comment"># 结果为: 错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负</span></span><br><span class="line">        aggErrors = multiply(sign(aggClassEst) != mat(labelArr).T, ones((m, <span class="number">1</span>)))</span><br><span class="line">        errorRate = aggErrors.<span class="built_in">sum</span>()/m</span><br><span class="line">        <span class="comment"># print &quot;total error=%s &quot; % (errorRate)</span></span><br><span class="line">        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> weakClassArr, aggClassEst</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">发现: </span><br><span class="line">alpha （模型权重）目的主要是计算每一个分类器实例的权重(加和就是分类结果)</span><br><span class="line">  分类的权重值: 最大的值= alpha 的加和，最小值=-最大值</span><br><span class="line">D （样本权重）的目的是为了计算错误概率:  weightedError = D.T*errArr，求最佳分类器</span><br><span class="line">  样本的权重值: 如果一个值误判的几率越小，那么 D 的样本权重越小</span><br></pre></td></tr></table></div></figure>

<p><img src="img/adaboost_alpha.png" alt="AdaBoost算法权重计算公式" title="AdaBoost算法权重计算公式"></p>
<blockquote>
<p>测试算法: 我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较。</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span>(<span class="params">datToClass, classifierArr</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;adaClassify(ada分类测试)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        datToClass     多个待分类的样例</span></span><br><span class="line"><span class="string">        classifierArr  弱分类器的集合</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sign(aggClassEst) 分类结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># do stuff similar to last aggClassEst in adaBoostTrainDS</span></span><br><span class="line">    dataMat = mat(datToClass)</span><br><span class="line">    m = shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环 多个分类器</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classifierArr)):</span><br><span class="line">        <span class="comment"># 前提:  我们已经知道了最佳的分类器的实例</span></span><br><span class="line">        <span class="comment"># 通过分类器来核算每一次的分类结果，然后通过alpha*每一次的结果 得到最后的权重加和的值。</span></span><br><span class="line">        classEst = stumpClassify(dataMat, classifierArr[i][<span class="string">&#x27;dim&#x27;</span>], classifierArr[i][<span class="string">&#x27;thresh&#x27;</span>], classifierArr[i][<span class="string">&#x27;ineq&#x27;</span>])</span><br><span class="line">        aggClassEst += classifierArr[i][<span class="string">&#x27;alpha&#x27;</span>]*classEst</span><br><span class="line">    <span class="keyword">return</span> sign(aggClassEst)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>使用算法: 观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去。</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 马疝病数据集</span></span><br><span class="line"><span class="comment"># 训练集合</span></span><br><span class="line">dataArr, labelArr = loadDataSet(<span class="string">&quot;data/7.AdaBoost/horseColicTraining2.txt&quot;</span>)</span><br><span class="line">weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, labelArr, <span class="number">40</span>)</span><br><span class="line"><span class="built_in">print</span> weakClassArr, <span class="string">&#x27;\n-----\n&#x27;</span>, aggClassEst.T</span><br><span class="line"><span class="comment"># 计算ROC下面的AUC的面积大小</span></span><br><span class="line">plotROC(aggClassEst.T, labelArr)</span><br><span class="line"><span class="comment"># 测试集合</span></span><br><span class="line">dataArrTest, labelArrTest = loadDataSet(<span class="string">&quot;data/7.AdaBoost/horseColicTest2.txt&quot;</span>)</span><br><span class="line">m = shape(dataArrTest)[<span class="number">0</span>]</span><br><span class="line">predicting10 = adaClassify(dataArrTest, weakClassArr)</span><br><span class="line">errArr = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 测试: 计算总样本数，错误样本数，错误率</span></span><br><span class="line"><span class="built_in">print</span> m, errArr[predicting10 != mat(labelArrTest).T].<span class="built_in">sum</span>(), errArr[predicting10 != mat(labelArrTest).T].<span class="built_in">sum</span>()/m</span><br></pre></td></tr></table></div></figure>

<p><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.AdaBoost/adaboost.py" >完整代码地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.AdaBoost/adaboost.py" >https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.AdaBoost/adaboost.py</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="要点补充"   >
          <a href="#要点补充" class="heading-link"><i class="fas fa-link"></i></a><a href="#要点补充" class="headerlink" title="要点补充"></a>要点补充</h4>
      <blockquote>
<p>非均衡现象: </p>
</blockquote>
<p><code>在分类器训练时，正例数目和反例数目不相等（相差很大）。或者发生在正负例分类错误的成本不同的时候。</code></p>
<ul>
<li>判断马是否能继续生存(不可误杀)</li>
<li>过滤垃圾邮件(不可漏判)</li>
<li>不能放过传染病的人</li>
<li>不能随便认为别人犯罪</li>
</ul>
<p>我们有多种方法来处理这个问题:  具体可参考<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" >此链接</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>再结合书中的方法，可以归为八大类: </p>

        <h5 id="1-能否收集到更多的数据？"   >
          <a href="#1-能否收集到更多的数据？" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-能否收集到更多的数据？" class="headerlink" title="1.能否收集到更多的数据？"></a>1.能否收集到更多的数据？</h5>
      <p>这个措施往往被人们所忽略，被认为很蠢。但是更大的数据集更能体现样本的分布，多样性。</p>

        <h5 id="2-尝试使用其他的评价指标"   >
          <a href="#2-尝试使用其他的评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-尝试使用其他的评价指标" class="headerlink" title="2.尝试使用其他的评价指标"></a>2.尝试使用其他的评价指标</h5>
      <p>Accuracy 或者error rate 不能用于非均衡的数据集。这会误导人。这时候可以尝试其他的评价指标。</p>
<p>Confusion Matrix 混淆矩阵: 使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为: TP、FN、FP与TN。</p>
<p>Precision: 精确度</p>
<p>Recall:  召回率</p>
<p>F1 Score (or F-Score):  精确度和召回率的加权平均</p>
<p>或者使用</p>
<p>Kappa (Cohen’s kappa)</p>
<p>ROC Curves</p>
<blockquote>
<p>ROC 评估方法</p>
</blockquote>
<ul>
<li>ROC 曲线: 最佳的分类器应该尽可能地处于左上角</li>
</ul>
<p><img src="img/ROC%E6%9B%B2%E7%BA%BF.png" alt="ROC曲线"></p>
<ul>
<li><p>对不同的 ROC 曲线进行比较的一个指标是曲线下的面积(Area Unser the Curve, AUC). </p>
</li>
<li><p>AUC 给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。</p>
</li>
<li><p>一个完美分类器的 AUC 为1，而随机猜测的 AUC 则为0.5。</p>
</li>
</ul>

        <h5 id="3-尝试对样本重抽样"   >
          <a href="#3-尝试对样本重抽样" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-尝试对样本重抽样" class="headerlink" title="3.尝试对样本重抽样"></a>3.尝试对样本重抽样</h5>
      <p>欠抽样(undersampling)或者过抽样(oversampling)</p>
<pre><code>- 欠抽样: 意味着删除样例
- 过抽样: 意味着复制样例(重复使用)
</code></pre>
<p>对大类进行欠抽样</p>
<p>对小类进行过抽样</p>
<p>或者结合上述两种方法进行抽样</p>
<p>一些经验法则: </p>
<ul>
<li><p>考虑样本（超过1万、十万甚至更多）进行欠采样，即删除部分样本；</p>
</li>
<li><p>考虑样本（不足1为甚至更少）进行过采样，即添加部分样本的副本；</p>
</li>
<li><p>考虑尝试随机采样与非随机采样两种采样方法；</p>
</li>
<li><p>考虑对各类别尝试不同的采样比例，不一定是1:1</p>
</li>
<li><p>考虑同时使用过采样与欠采样</p>
</li>
</ul>

        <h5 id="4-尝试产生人工生成的样本"   >
          <a href="#4-尝试产生人工生成的样本" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-尝试产生人工生成的样本" class="headerlink" title="4.尝试产生人工生成的样本"></a>4.尝试产生人工生成的样本</h5>
      <p>一种简单的方法就是随机抽样小类样本的属性（特征）来组成新的样本即属性值随机采样。你可以根据经验进行抽样，可以使用其他方式比如朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之间的非线性关系。</p>
<p>当然也有系统性的算法。最常用的SMOTE(Synthetic Minority Over-Sampling Technique)。 顾名思义，这是一种over sampling（过抽样）的方式。它是产生人为的样本而不是制造样本副本。这个算法是选取2个或者2个以上相似的样本（根据距离度量 distance measure），然后每次选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声(每次只处理一个属性)。这样就构造了更多的新生数据。具体可以参见<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.jair.org/papers/paper953.html" >原始论文</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。</p>
<p>python实现可以查阅<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/scikit-learn-contrib/imbalanced-learn" >UnbalancedDataset</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="5-尝试不同的算法"   >
          <a href="#5-尝试不同的算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-尝试不同的算法" class="headerlink" title="5.尝试不同的算法"></a>5.尝试不同的算法</h5>
      <p>强烈建议不要在每个问题上使用你最喜欢的算法。虽然这个算法带来较好的效果，但是它也会蒙蔽你观察数据内蕴含的其他的信息。至少你得在同一个问题上试试各种算法。具体可以参阅<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/" >Why you should be Spot-Checking Algorithms on your Machine Learning Problems</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>比如说，决策树经常在非均衡数据集上表现良好。创建分类树时候使用基于类变量的划分规则强制使类别表达出来。如果有疑惑，可以尝试一些流行的决策树，比如, C4.5, C5.0, CART 和 Random Forrest。</p>

        <h5 id="6-尝试使用惩罚的模型"   >
          <a href="#6-尝试使用惩罚的模型" class="heading-link"><i class="fas fa-link"></i></a><a href="#6-尝试使用惩罚的模型" class="headerlink" title="6.尝试使用惩罚的模型"></a>6.尝试使用惩罚的模型</h5>
      <p>你可以使用同种算法但是以不同的角度对待这个问题。</p>
<p>惩罚的模型就是对于不同的分类错误给予不同的代价（惩罚）。比如对于错分的小类给予更高的代价。这种方式会使模型偏差，更加关注小类。</p>
<p>通常来说这种代价/惩罚或者比重在学习中算法是特定的。比如使用代价函数来实现: </p>
<blockquote>
<p>代价函数</p>
</blockquote>
<ul>
<li>基于代价函数的分类器决策控制: <code>TP*(-5)+FN*1+FP*50+TN*0</code></li>
</ul>
<p><img src="img/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png" alt="代价函数"></p>
<p>这种方式叫做 cost sensitive learning，Weka 中相应的框架可以实现叫<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html" >CostSensitiveClassifier</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>如果当你只能使用特定算法而且无法重抽样，或者模型效果不行，这时候使用惩罚（penalization）是可行的方法。这提供另外一种方式来“平衡”类别。但是设定惩罚函数/代价函数是比较复杂的。最好还是尝试不同的代价函数组合来得到最优效果。</p>

        <h5 id="7-尝试使用不同的角度"   >
          <a href="#7-尝试使用不同的角度" class="heading-link"><i class="fas fa-link"></i></a><a href="#7-尝试使用不同的角度" class="headerlink" title="7.尝试使用不同的角度"></a>7.尝试使用不同的角度</h5>
      <p>其实有很多研究关于非均衡数据。他们有自己的算法，度量，术语。</p>
<p>从它们的角度看看你的问题，思考你的问题，说不定会有新的想法。</p>
<p>两个领域您可以考虑:  anomaly detection(异常值检测) 和 change detection（变化趋势检测）。</p>
<p>Anomaly dectection 就是检测稀有事件。 比如通过机器震动来识别机器谷中或者根据一系列系统的调用来检测恶意操作。与常规操作相比，这些事件是罕见的。</p>
<p>把小类想成异常类这种转变可能会帮助你想到新办法来分类数据样本。</p>
<p>change detection 变化趋势检测类似于异常值检测。但是他不是寻找异常值而是寻找变化或区别。比如通过使用模式或者银行交易记录来观察用户行为转变。</p>
<p>这些两种转变可能会给你新的方式去思考你的问题和新的技术去尝试。
    </p>

        <h5 id="8-尝试去创新"   >
          <a href="#8-尝试去创新" class="heading-link"><i class="fas fa-link"></i></a><a href="#8-尝试去创新" class="headerlink" title="8.尝试去创新"></a>8.尝试去创新</h5>
      <p>仔细思考你的问题然后想想看如何将这问题细分为几个更切实际的小问题。</p>
<p>比如: </p>
<p>将你的大类分解成多个较小的类；</p>
<p>使用One Class分类器（看待成异常点检测）；</p>
<p>对数据集进行抽样成多个数据集，使用集成方式，训练多个分类器，然后联合这些分类器进行分类；</p>
<p>这只是一个例子。更多的可以参阅<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set" >In classification, how do you handle an unbalanced training set?</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 和<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/" >Classification when 80% of my training set is of one class</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>​<br>​    </p>
<hr>
<ul>
<li><strong>作者: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/jiangzhonglian" >片刻</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
<li><span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >GitHub地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>: <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://github.com/apachecn/AiLearning" >https://github.com/apachecn/AiLearning</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><strong>版权声明: 欢迎转载学习 =&gt; 请标注信息来源于 <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://www.apachecn.org/" >ApacheCN</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></strong></li>
</ul>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">Categories</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>TainTear</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>